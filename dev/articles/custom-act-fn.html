<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Custom Activation Function • kindling</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<!-- katex math --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><script src="../katex-auto.js"></script><script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Playfair_Display-0.4.10/font.css" rel="stylesheet">
<link href="../deps/Fira_Mono-0.4.10/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Custom Activation Function">
<meta name="robots" content="noindex">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">kindling</a>

    <small class="nav-text text-default me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">0.2.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/kindling.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/kindling.html">Getting Started</a></li>
    <li><a class="dropdown-item" href="../articles/similar-packages.html">Similar packages and comparison</a></li>
    <li><a class="dropdown-item" href="../articles/tuning-capabilities.html">Tuning Capabilities</a></li>
    <li><a class="dropdown-item" href="../articles/special-cases.html">Linear and Logistic Regression (Special Cases)</a></li>
    <li><a class="dropdown-item" href="../articles/custom-act-fn.html">Custom Activation Function</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-learn-more" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Learn more</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-learn-more">
<li><a class="external-link dropdown-item" href="https://www.tidymodels.org/learn/models/parsnip-ranger-glmnet/">Regression modeling</a></li>
    <li><a class="external-link dropdown-item" href="https://www.tidymodels.org/learn/models/parsnip-nnet/">Classification modeling</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/joshuamarie/kindling" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch">
<li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul>
</li>
<li class="nav-item"><a class="external-link nav-link" href="https://joshuamarie.com"><span class="fa fa-blog"></span> Blog</a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Custom Activation Function</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/joshuamarie/kindling/blob/main/vignettes/custom-act-fn.Rmd" class="external-link"><code>vignettes/custom-act-fn.Rmd</code></a></small>
      <div class="d-none name"><code>custom-act-fn.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="rationale">Rationale<a class="anchor" aria-label="anchor" href="#rationale"></a>
</h2>
<p>The biggest strength of <a href="https://kindling.joshuamarie.com">kindling</a> when modelling neural
networks is its versatility — it inherits <a href="https://torch.mlverse.org/docs" class="external-link">torch</a>’s
versatility while being human friendly, including the ability to apply
custom optimizer functions, loss functions, and per-layer activation
functions. Learn more: <a href="https://kindling.joshuamarie.com/articles/special-cases">https://kindling.joshuamarie.com/articles/special-cases</a>.</p>
<p>With <code><a href="../reference/act_funs.html">act_funs()</a></code>, you are not limited to the activation
functions available in <a href="https://torch.mlverse.org/docs" class="external-link">torch</a>’s namespace. Use
<code><a href="../reference/new_act_fn.html">new_act_fn()</a></code> to wrap any compatible function into a
validated custom activation. This feature, however, only available on
version 0.3.0 and above.</p>
</div>
<div class="section level2">
<h2 id="function-to-use">Function to use<a class="anchor" aria-label="anchor" href="#function-to-use"></a>
</h2>
<p>To do this, use <code><a href="../reference/new_act_fn.html">new_act_fn()</a></code> and it takes a
user-supplied function, validates it against a small dummy tensor at
<em>definition time</em> (a dry-run probe), and wraps it in a call-time
type guard. This means errors surface early — before your model ever
starts training.</p>
<p>The function you supply must:</p>
<ul>
<li>Accept at least one argument (the input tensor).</li>
<li>Return a <code>torch_tensor</code>.</li>
</ul>
<div class="section level3">
<h3 id="basic-usage">Basic Usage<a class="anchor" aria-label="anchor" href="#basic-usage"></a>
</h3>
<p>Currently, <code>nnf_tanh</code> doesn’t exist in
<a href="https://torch.mlverse.org/docs" class="external-link">torch</a> namespace, so <code>tanh</code> argument is not
valid. With <code><a href="../reference/new_act_fn.html">new_act_fn()</a></code>, you can wrap
<code><a href="https://torch.mlverse.org/docs/reference/torch_tanh.html" class="external-link">torch::torch_tanh()</a></code> to make it usable.</p>
<p>Here’s a basic example that wraps <code><a href="https://torch.mlverse.org/docs/reference/torch_tanh.html" class="external-link">torch::torch_tanh()</a></code> as
a custom activation:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hyper_tan</span> <span class="op">=</span> <span class="fu"><a href="../reference/new_act_fn.html">new_act_fn</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_tanh.html" class="external-link">torch_tanh</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>You can also pass it directly into <code><a href="../reference/act_funs.html">act_funs()</a></code>, just like
any built-in activation:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/act_funs.html">act_funs</a></span><span class="op">(</span><span class="va">relu</span>, <span class="va">elu</span>, <span class="fu"><a href="../reference/new_act_fn.html">new_act_fn</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_tanh.html" class="external-link">torch_tanh</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="using-custom-activations-in-a-model">Using Custom Activations in a Model<a class="anchor" aria-label="anchor" href="#using-custom-activations-in-a-model"></a>
</h3>
<p>Naturally, functions for modelling like <code><a href="../reference/kindling-basemodels.html">ffnn()</a></code> accepts
<code><a href="../reference/act_funs.html">act_funs()</a></code> into the <code>activations</code> argument.
Again, you can pass a custom activation function within
<code><a href="../reference/new_act_fn.html">new_act_fn()</a></code>, then pass it through
<code><a href="../reference/act_funs.html">act_funs()</a></code>.</p>
<p>Here’s a basic example:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">=</span> <span class="fu"><a href="../reference/kindling-basemodels.html">ffnn</a></span><span class="op">(</span></span>
<span>    <span class="va">Sepal.Length</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>    data <span class="op">=</span> <span class="va">iris</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>,</span>
<span>    hidden_neurons <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">64</span>, <span class="fl">32</span>, <span class="fl">16</span><span class="op">)</span>,</span>
<span>    activations <span class="op">=</span> <span class="fu"><a href="../reference/act_funs.html">act_funs</a></span><span class="op">(</span></span>
<span>        <span class="va">relu</span>,</span>
<span>        <span class="va">silu</span>,</span>
<span>        <span class="fu"><a href="../reference/new_act_fn.html">new_act_fn</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_tanh.html" class="external-link">torch_tanh</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">)</span>,</span>
<span>    epochs <span class="op">=</span> <span class="fl">50</span></span>
<span><span class="op">)</span></span>
<span><span class="va">model</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## <span style="font-weight: bold;">======================= Feedforward Neural Networks (MLP) ======================</span></span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## <span style="font-style: italic;">-- FFNN Model Summary ----------------------------------------------------------</span></span></span></code></pre>
<pre><code><span><span class="co">## Warning in system("tput cols", intern = TRUE): running command 'tput cols' had</span></span>
<span><span class="co">## status 2</span></span></code></pre>
<pre><code><span><span class="co">## -------------------------------------------------------------------</span></span>
<span><span class="co">##   NN Model Type           :         FFNN    n_predictors :      3</span></span>
<span><span class="co">##   Number of Epochs        :           50    n_response   :      1</span></span>
<span><span class="co">##   Hidden Layer Units      :   64, 32, 16    reg.         :   None</span></span>
<span><span class="co">##   Number of Hidden Layers :            3    Device       :    cpu</span></span>
<span><span class="co">##   Pred. Type              :   regression                 :       </span></span>
<span><span class="co">## -------------------------------------------------------------------</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## <span style="font-style: italic;">-- Activation function ---------------------------------------------------------</span></span></span></code></pre>
<pre><code><span><span class="co">## Warning in system("tput cols", intern = TRUE): running command 'tput cols' had</span></span>
<span><span class="co">## status 2</span></span></code></pre>
<pre><code><span><span class="co">## -------------------------------------------------</span></span>
<span><span class="co">##   1st Layer {64}    :                      relu</span></span>
<span><span class="co">##   2nd Layer {32}    :                      silu</span></span>
<span><span class="co">##   3rd Layer {16}    :                  &lt;custom&gt;</span></span>
<span><span class="co">##   Output Activation :   No act function applied</span></span>
<span><span class="co">## -------------------------------------------------</span></span></code></pre>
<p>Each element of <code><a href="../reference/act_funs.html">act_funs()</a></code> corresponds to one hidden
layer, in order. Here, the first hidden layer uses ReLU, the second uses
SiLU (Swish), and the third uses Tanh.</p>
<p>You can also use a single custom activation recycled across all
layers:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/kindling-basemodels.html">ffnn</a></span><span class="op">(</span></span>
<span>    <span class="va">Sepal.Length</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>    data <span class="op">=</span> <span class="va">iris</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>,</span>
<span>    hidden_neurons <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">64</span>, <span class="fl">32</span><span class="op">)</span>,</span>
<span>    activations <span class="op">=</span> <span class="fu"><a href="../reference/act_funs.html">act_funs</a></span><span class="op">(</span><span class="fu"><a href="../reference/new_act_fn.html">new_act_fn</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_tanh.html" class="external-link">torch_tanh</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    epochs <span class="op">=</span> <span class="fl">50</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## <span style="font-weight: bold;">======================= Feedforward Neural Networks (MLP) ======================</span></span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## <span style="font-style: italic;">-- FFNN Model Summary ----------------------------------------------------------</span></span></span></code></pre>
<pre><code><span><span class="co">## Warning in system("tput cols", intern = TRUE): running command 'tput cols' had</span></span>
<span><span class="co">## status 2</span></span></code></pre>
<pre><code><span><span class="co">## -------------------------------------------------------------------</span></span>
<span><span class="co">##   NN Model Type           :         FFNN    n_predictors :      3</span></span>
<span><span class="co">##   Number of Epochs        :           50    n_response   :      1</span></span>
<span><span class="co">##   Hidden Layer Units      :       64, 32    reg.         :   None</span></span>
<span><span class="co">##   Number of Hidden Layers :            2    Device       :    cpu</span></span>
<span><span class="co">##   Pred. Type              :   regression                 :       </span></span>
<span><span class="co">## -------------------------------------------------------------------</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## <span style="font-style: italic;">-- Activation function ---------------------------------------------------------</span></span></span></code></pre>
<pre><code><span><span class="co">## Warning in system("tput cols", intern = TRUE): running command 'tput cols' had</span></span>
<span><span class="co">## status 2</span></span></code></pre>
<pre><code><span><span class="co">## -------------------------------------------------</span></span>
<span><span class="co">##   1st Layer {64}    :                  &lt;custom&gt;</span></span>
<span><span class="co">##   2nd Layer {32}    :                  &lt;custom&gt;</span></span>
<span><span class="co">##   Output Activation :   No act function applied</span></span>
<span><span class="co">## -------------------------------------------------</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="skipping-the-dry-run-probe">Skipping the Dry-Run Probe<a class="anchor" aria-label="anchor" href="#skipping-the-dry-run-probe"></a>
</h3>
<p>By default, <code><a href="../reference/new_act_fn.html">new_act_fn()</a></code> runs a quick dry-run with a
small dummy tensor to validate your function before training. You can
disable this with <code>probe = FALSE</code>, though this is generally
not recommended:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">my_act</span> <span class="op">=</span> <span class="fu"><a href="../reference/new_act_fn.html">new_act_fn</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_tanh.html" class="external-link">torch_tanh</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, probe <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="naming-your-custom-activation">Naming Your Custom Activation<a class="anchor" aria-label="anchor" href="#naming-your-custom-activation"></a>
</h3>
<p>You can provide a human-readable name via <code>.name</code>, which
is used in print output and diagnostics:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">my_act</span> <span class="op">=</span> <span class="fu"><a href="../reference/new_act_fn.html">new_act_fn</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_tanh.html" class="external-link">torch_tanh</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, .name <span class="op">=</span> <span class="st">"my_tanh"</span><span class="op">)</span></span></code></pre></div>
<p>Here’s a simple application:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/kindling-basemodels.html">ffnn</a></span><span class="op">(</span></span>
<span>    <span class="va">Sepal.Length</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>    data <span class="op">=</span> <span class="va">iris</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>,</span>
<span>    hidden_neurons <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">64</span>, <span class="fl">32</span><span class="op">)</span>,</span>
<span>    activations <span class="op">=</span> <span class="fu"><a href="../reference/act_funs.html">act_funs</a></span><span class="op">(</span></span>
<span>        <span class="va">relu</span>, </span>
<span>        <span class="fu"><a href="../reference/new_act_fn.html">new_act_fn</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_tanh.html" class="external-link">torch_tanh</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, .name <span class="op">=</span> <span class="st">"hyper_tanh"</span><span class="op">)</span></span>
<span>    <span class="op">)</span>,</span>
<span>    epochs <span class="op">=</span> <span class="fl">50</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## <span style="font-weight: bold;">======================= Feedforward Neural Networks (MLP) ======================</span></span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## <span style="font-style: italic;">-- FFNN Model Summary ----------------------------------------------------------</span></span></span></code></pre>
<pre><code><span><span class="co">## Warning in system("tput cols", intern = TRUE): running command 'tput cols' had</span></span>
<span><span class="co">## status 2</span></span></code></pre>
<pre><code><span><span class="co">## -------------------------------------------------------------------</span></span>
<span><span class="co">##   NN Model Type           :         FFNN    n_predictors :      3</span></span>
<span><span class="co">##   Number of Epochs        :           50    n_response   :      1</span></span>
<span><span class="co">##   Hidden Layer Units      :       64, 32    reg.         :   None</span></span>
<span><span class="co">##   Number of Hidden Layers :            2    Device       :    cpu</span></span>
<span><span class="co">##   Pred. Type              :   regression                 :       </span></span>
<span><span class="co">## -------------------------------------------------------------------</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## <span style="font-style: italic;">-- Activation function ---------------------------------------------------------</span></span></span></code></pre>
<pre><code><span><span class="co">## Warning in system("tput cols", intern = TRUE): running command 'tput cols' had</span></span>
<span><span class="co">## status 2</span></span></code></pre>
<pre><code><span><span class="co">## -------------------------------------------------</span></span>
<span><span class="co">##   1st Layer {64}    :                      relu</span></span>
<span><span class="co">##   2nd Layer {32}    :                hyper_tanh</span></span>
<span><span class="co">##   Output Activation :   No act function applied</span></span>
<span><span class="co">## -------------------------------------------------</span></span></code></pre>
</div>
</div>
<div class="section level2">
<h2 id="error-handling">Error Handling<a class="anchor" aria-label="anchor" href="#error-handling"></a>
</h2>
<p><code><a href="../reference/new_act_fn.html">new_act_fn()</a></code> is designed to fail loudly and early.
Common errors include:</p>
<ol style="list-style-type: decimal">
<li>
<p>Function returns a non-tensor. This will error at definition
time:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/new_act_fn.html">new_act_fn</a></span><span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #BBBB00; font-weight: bold;">Error</span><span style="font-weight: bold;"> in `.assert_tensor_output()`:</span></span></span>
<span><span class="co">## <span style="color: #BBBB00;">!</span> Dry-run must be a <span style="color: #0000BB;">&lt;torch_tensor&gt;</span>.</span></span>
<span><span class="co">## <span style="color: #BB0000;">✖</span> Got <span style="color: #0000BB;">&lt;numeric&gt;</span>.</span></span>
<span><span class="co">## <span style="color: #00BBBB;">ℹ</span> Ensure your function returns the result of a <span style="color: #0000BB;">torch</span> operation.</span></span></code></pre>
</li>
<li>
<p>Function accepts no arguments. This will error immediately:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/new_act_fn.html">new_act_fn</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_zeros.html" class="external-link">torch_zeros</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #BBBB00; font-weight: bold;">Error</span><span style="font-weight: bold;"> in `new_act_fn()`:</span></span></span>
<span><span class="co">## <span style="color: #BBBB00;">!</span> `fn` must accept at least one argument (the input tensor).</span></span>
<span><span class="co">## <span style="color: #00BBBB;">ℹ</span> Use a lambda like `\(x) torch::torch_tanh(x)`.</span></span></code></pre>
</li>
</ol>
<p>These checks ensure your model’s architecture is valid before any
data ever flows through it.</p>
</div>
<div class="section level2">
<h2 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Details</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Wraps any R function</td>
<td>Must accept a tensor, return a tensor</td>
</tr>
<tr class="even">
<td>Dry-run probe</td>
<td>Validates at definition time (<code>probe = TRUE</code> by
default)</td>
</tr>
<tr class="odd">
<td>Call-time guard</td>
<td>Type-checks output on every forward pass</td>
</tr>
<tr class="even">
<td>Compatible with <code><a href="../reference/act_funs.html">act_funs()</a></code>
</td>
<td>Use alongside built-in activations freely</td>
</tr>
<tr class="odd">
<td>Closures supported</td>
<td>Parametric activations work naturally</td>
</tr>
</tbody>
</table>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Joshua Marie, Antoine Soetewey.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a>. Visit my <a href="https://joshuamarie.com" class="external-link">blog</a> for more content.</p>
</div>

    </footer>
</div>





  </body>
</html>
