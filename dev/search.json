[{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement joshua.marie.k@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to kindling","title":"Contributing to kindling","text":"outlines propose change kindling. detailed discussion contributing tidyverse packages, please see development contributing guide code review principles.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to kindling","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to kindling","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed). See guide create great issue advice.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to kindling","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"joshuamarie/kindling\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to kindling","text":"New code follow tidyverse style guide. can use Air apply style, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to kindling","text":"Please note kindling project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://kindling.joshuamarie.com/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 kindling authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting Started with kindling","text":"kindling bridges gap torch tidymodels, providing streamlined interface building, training, tuning deep learning models. vignette guide basic usage.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Getting Started with kindling","text":"","code":"# Install from GitHub pak::pak(\"joshuamarie/kindling\") library(kindling) #>  #> Attaching package: 'kindling' #> The following object is masked from 'package:base': #>  #>     args"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"four-levels-of-interaction","dir":"Articles","previous_headings":"","what":"Four Levels of Interaction","title":"Getting Started with kindling","text":"kindling offers flexibility four levels abstraction: Code Generation - Generate raw torch::nn_module code Direct Training - Train models simple function calls tidymodels Integration - Use parsnip, recipes, workflows Hyperparameter Tuning - Optimize models tune dials","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"level-1-code-generation","dir":"Articles","previous_headings":"","what":"Level 1: Code Generation","title":"Getting Started with kindling","text":"Generate PyTorch-style module code:","code":"ffnn_generator(     nn_name = \"MyNetwork\",     hd_neurons = c(64, 32),     no_x = 10,     no_y = 1,     activations = 'relu' )"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"level-2-direct-training","dir":"Articles","previous_headings":"","what":"Level 2: Direct Training","title":"Getting Started with kindling","text":"Train model one function call:","code":"model = ffnn(     Species ~ .,     data = iris,     hidden_neurons = c(10, 15, 7),     activations = act_funs(relu, elu),     loss = \"cross_entropy\",     epochs = 100 )  predictions = predict(model, newdata = iris)"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"level-3-tidymodels-integration","dir":"Articles","previous_headings":"","what":"Level 3: tidymodels Integration","title":"Getting Started with kindling","text":"Work neural networks like parsnip model:","code":"box::use(     parsnip[fit, augment],     yardstick[metrics] )  nn_spec = mlp_kindling(     mode = \"classification\",     hidden_neurons = c(10, 7),     activations = act_funs(relu, softshrink = args(lambd = 0.5)),     epochs = 100 )  nn_fit = fit(nn_spec, Species ~ ., data = iris) augment(nn_fit, new_data = iris) |>      metrics(truth = Species, estimate = .pred_class)"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"learn-more","dir":"Articles","previous_headings":"","what":"Learn More","title":"Getting Started with kindling","text":"Visit package website: https://kindling.joshuamarie.com Report issues: https://github.com/joshuamarie/kindling/issues","code":""},{"path":"https://kindling.joshuamarie.com/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Joshua Marie. Author, maintainer.","code":""},{"path":"https://kindling.joshuamarie.com/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Marie, Joshua (2026). kindling: Higher-Level Interface Torch Auto-Train Neural Networks https://kindling.joshuamarie.com","code":"@Manual{,   title = {{kindling}: Higher-Level Interface of Torch to Auto-Train Neural Networks},   author = {Joshua Marie},   year = {2026},   note = {R package version 0.1.0.9000}, }"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"kindling-higher-level-interface-of-torch-package-to-auto-train-neural-networks-","dir":"","previous_headings":"","what":"kindling • Higher-Level Interface for torch","title":"kindling • Higher-Level Interface for torch","text":"Note: package active development. API may change future versions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"kindling • Higher-Level Interface for torch","text":"kindling bridges gap {torch} {tidymodels}, offering streamlined interface building, training, tuning deep learning models within familiar tidymodels ecosystem. Whether ’re prototyping neural architectures deploying production models, kindling minimizes boilerplate code preserving flexibility torch. works seamlessly parsnip, recipes, workflows bring deep learning existing modeling pipeline.","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"key-features","dir":"","previous_headings":"Overview","what":"Key Features","title":"kindling • Higher-Level Interface for torch","text":"Seamless integration parsnip set_engine(\"kindling\") Native support tidymodels workflows pipelines Multiple architectures available: feedforward networks (DNN/FFNN) recurrent variants (RNN, LSTM, GRU) Fine-grained control network depth, layer sizes, activation functions Full GPU acceleration via torch tensors Dramatically less boilerplate raw torch implementations","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"supported-architectures","dir":"","previous_headings":"Overview","what":"Supported Architectures","title":"kindling • Higher-Level Interface for torch","text":"Feedforward Networks (DNN/FFNN): Classic multi-layer perceptrons tabular data general supervised learning Recurrent Neural Networks (RNN): Basic recurrent architecture sequential patterns Long Short-Term Memory (LSTM): Sophisticated recurrent networks gating mechanisms long-range dependencies Gated Recurrent Units (GRU): Streamlined alternative LSTM fewer parameters","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"kindling • Higher-Level Interface for torch","text":"can install kindling CRAN: install development version GitHub:","code":"install.packages('kindling') # install.packages(\"pak\") pak::pak(\"joshuamarie/kindling\")"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"usage-four-levels-of-interaction","dir":"","previous_headings":"","what":"Usage: Four Levels of Interaction","title":"kindling • Higher-Level Interface for torch","text":"kindling leverages R’s metaprogramming capabilities code generation. Generated torch::nn_module expressions power training functions, turn serve engines tidymodels integration. architecture gives flexibility work whatever abstraction level suits task.","code":"library(kindling) #>  #> Attaching package: 'kindling' #> The following object is masked from 'package:base': #>  #>     args"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-1-code-generation-for-torchnn_module","dir":"","previous_headings":"Usage: Four Levels of Interaction","what":"Level 1: Code Generation for torch::nn_module","title":"kindling • Higher-Level Interface for torch","text":"lowest level, can generate raw torch::nn_module code maximum customization. Functions ending _generator return unevaluated expressions can inspect, modify, execute. ’s generate feedforward network specification: creates three-hidden-layer network (64 - 32 - 16 neurons) takes 10 inputs produces 1 output. hidden layer uses ReLU activation, output layer remains “untransformed”.","code":"ffnn_generator(     nn_name = \"MyFFNN\",     hd_neurons = c(64, 32, 16),     no_x = 10,     no_y = 1,     activations = 'relu' ) #> torch::nn_module(\"MyFFNN\", initialize = function ()  #> { #>     self$fc1 = torch::nn_linear(10, 64, bias = TRUE) #>     self$fc2 = torch::nn_linear(64, 32, bias = TRUE) #>     self$fc3 = torch::nn_linear(32, 16, bias = TRUE) #>     self$out = torch::nn_linear(16, 1, bias = TRUE) #> }, forward = function (x)  #> { #>     x = self$fc1(x) #>     x = torch::nnf_relu(x) #>     x = self$fc2(x) #>     x = torch::nnf_relu(x) #>     x = self$fc3(x) #>     x = torch::nnf_relu(x) #>     x = self$out(x) #>     x #> })"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-2-direct-training-interface","dir":"","previous_headings":"Usage: Four Levels of Interaction","what":"Level 2: Direct Training Interface","title":"kindling • Higher-Level Interface for torch","text":"Skip code generation train models directly data. approach handles torch boilerplate internally. Let’s classify iris species: predict() method offers flexible prediction behavior newdata argument: Without new data — predictions default training set: new data — simply pass data frame:","code":"model = ffnn(     Species ~ .,     data = iris,     hidden_neurons = c(10, 15, 7),     activations = act_funs(relu, softshrink = args(lambd = 0.5), elu),     loss = \"cross_entropy\",     epochs = 100 ) model ======================= Feedforward Neural Networks (MLP) ======================   -- FFNN Model Summary ----------------------------------------------------------       ----------------------------------------------------------------------        NN Model Type           :             FFNN    n_predictors :     4        Number of Epochs        :              100    n_response   :     3        Hidden Layer Units      :        10, 15, 7    Device       :   cpu        Number of Hidden Layers :                3                 :              Pred. Type              :   classification                 :            ----------------------------------------------------------------------    -- Activation function ---------------------------------------------------------                 -------------------------------------------------                  1st Layer {10}    :                      relu                  2nd Layer {15}    :   softshrink(lambd = 0.5)                  3rd Layer {7}     :                       elu                  Output Activation :   No act function applied                ------------------------------------------------- predict(model) |>      (\\(x) table(actual = iris$Species, predicted = x))() #>             predicted #> actual       setosa versicolor virginica #>   setosa         50          0         0 #>   versicolor      0         47         3 #>   virginica       0          1        49 sample_iris = dplyr::slice_sample(iris, n = 10, by = Species)  predict(model, newdata = sample_iris) |>      (\\(x) table(actual = sample_iris$Species, predicted = x))() #>             predicted #> actual       setosa versicolor virginica #>   setosa         10          0         0 #>   versicolor      0          9         1 #>   virginica       0          1         9"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-3-full-tidymodels-integration","dir":"","previous_headings":"Usage: Four Levels of Interaction","what":"Level 3: Full tidymodels Integration","title":"kindling • Higher-Level Interface for torch","text":"Work neural networks just like parsnip model. unlocks entire tidymodels toolkit preprocessing, cross-validation, model evaluation.","code":"# library(kindling) # library(parsnip) # library(yardstick) box::use(     kindling[mlp_kindling, rnn_kindling, act_funs, args],     parsnip[fit, augment],     yardstick[metrics],     mlbench[Ionosphere] # data(Ionosphere, package = \"mlbench\") )  ionosphere_data = Ionosphere[, -2]  # Train a feedforward network with parsnip mlp_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, softshrink = args(lambd = 0.5)),     epochs = 100 ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> # A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.989 #> 2 kap      binary         0.975  # Or try a recurrent architecture (demonstrative example with tabular data) rnn_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, elu),     epochs = 100,     rnn_type = \"gru\" ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> # A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.641 #> 2 kap      binary         0"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-4-hyperparameter-tuning--resampling","dir":"","previous_headings":"Usage: Four Levels of Interaction","what":"Level 4: Hyperparameter Tuning & Resampling","title":"kindling • Higher-Level Interface for torch","text":"functionality available, still fully optimized. roadmap includes full support hyperparameter tuning via tune searchable parameters: Network depth (number hidden layers - coming soon) Layer widths (neurons per layer) Activation function combinations Output activation Optimizer (Type optimization algorithm) Bias (choose presence absence bias term) Validation Split Proportion Bidirectional (RNN) ’s example: Resampling strategies rsample enable robust cross-validation workflows, orchestrated tune dials APIs.","code":"box::use(     kindling[         mlp_kindling, hidden_neurons, activations, output_activation, grid_depth     ],     parsnip[fit, augment],     recipes[recipe],     workflows[workflow, add_recipe, add_model],     rsample[vfold_cv],     tune[tune_grid, tune, select_best, finalize_workflow],     dials[grid_random],     yardstick[accuracy, roc_auc, metric_set, metrics] )  mlp_tune_spec = mlp_kindling(     mode = \"classification\",     hidden_neurons = tune(),     activations = tune(),     output_activation = tune() )  iris_folds = vfold_cv(iris, v = 3) nn_wf = workflow() |>     add_recipe(recipe(Species ~ ., data = iris)) |>     add_model(mlp_tune_spec)  nn_grid = grid_random(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\")),     output_activation(c(\"sigmoid\", \"linear\")),     size = 10 )  nn_grid_depth = grid_depth(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\")),     output_activation(c(\"sigmoid\", \"linear\")),     n_hlayer = 2,     size = 10,     type = \"latin_hypercube\" )  nn_tunes = tune::tune_grid(     nn_wf,     iris_folds,     grid = nn_grid_depth     # metrics = metric_set(accuracy, roc_auc) )  best_nn = select_best(nn_tunes) final_nn = finalize_workflow(nn_wf, best_nn) # Last run: 4 - 91 (relu) - 3 (sigmoid) units final_nn_model = fit(final_nn, data = iris)  final_nn_model |>     augment(new_data = iris) |>     metrics(truth = Species, estimate = .pred_class) #> # A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy multiclass     0.667 #> 2 kap      multiclass     0.5"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"variable-importance","dir":"","previous_headings":"","what":"Variable Importance","title":"kindling • Higher-Level Interface for torch","text":"kindling integrates established variable importance methods {NeuralNetTools} vip interpret trained neural networks. Two primary algorithms available: Garson’s Algorithm Olden’s Algorithm","code":"garson(model, bar_plot = FALSE) #>        x_names y_names  rel_imp #> 1  Petal.Width Species 30.38174 #> 2 Petal.Length Species 25.83497 #> 3 Sepal.Length Species 22.78038 #> 4  Sepal.Width Species 21.00291 olden(model, bar_plot = FALSE) #>        x_names y_names      rel_imp #> 1  Petal.Width Species  0.575948477 #> 2  Sepal.Width Species -0.286548868 #> 3 Sepal.Length Species -0.204277142 #> 4 Petal.Length Species  0.006615014"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"integration-with-vip","dir":"","previous_headings":"Variable Importance","what":"Integration with {vip}","title":"kindling • Higher-Level Interface for torch","text":"users working within tidymodels ecosystem, kindling models work seamlessly vip package:  Note: Weight caching increases memory usage proportional network size. enable plan compute variable importance multiple times model.","code":"box::use(     vip[vi, vip] )  vi(model) |>      vip()"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"kindling • Higher-Level Interface for torch","text":"Falbel D, Luraschi J (2023). torch: Tensors Neural Networks ‘GPU’ Acceleration. R package version 0.13.0, https://torch.mlverse.org, https://github.com/mlverse/torch. Wickham H (2019). Advanced R, 2nd edition. Chapman Hall/CRC. ISBN 978-0815384571, https://adv-r.hadley.nz/. Goodfellow , Bengio Y, Courville (2016). Deep Learning. MIT Press. https://www.deeplearningbook.org/.","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"kindling • Higher-Level Interface for torch","text":"MIT + file LICENSE","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"kindling • Higher-Level Interface for torch","text":"Please note kindling project released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":null,"dir":"Reference","previous_headings":"","what":"Activation Functions Specification Helper — act_funs","title":"Activation Functions Specification Helper — act_funs","text":"function DSL function, kind like ggplot2::aes(), helps specify activation functions neural network layers. validates activation functions exist torch parameters match function's formal arguments.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Activation Functions Specification Helper — act_funs","text":"","code":"act_funs(...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Activation Functions Specification Helper — act_funs","text":"... Activation function specifications. Can : Bare symbols: relu, tanh Character strings (simple): \"relu\", \"tanh\" Character strings (params): \"softshrink(lambda = 0.1)\", \"rrelu(lower = 1/5, upper = 1/4)\" Named parameters: softmax = args(dim = 2L)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Activation Functions Specification Helper — act_funs","text":"vctrs vector class \"activation_spec\" containing validated activation specifications.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":null,"dir":"Reference","previous_headings":"","what":"Activation Function Arguments Helper — args","title":"Activation Function Arguments Helper — args","text":"Type-safe helper specify parameters activation functions. parameters must named match formal arguments corresponding torch activation function.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Activation Function Arguments Helper — args","text":"","code":"args(...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Activation Function Arguments Helper — args","text":"... Named arguments activation function.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Activation Function Arguments Helper — args","text":"list class \"activation_args\" containing parameters.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Tunable hyperparameters for kindling models — dials-kindling","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"parameters extend dials framework support hyperparameter tuning neural networks built {kindling} package. control network architecture, activation functions, optimization, training behavior.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"","code":"n_hlayers(range = c(1L, 5L), trans = NULL)  hidden_neurons(range = c(8L, 512L), trans = NULL)  activations(   values = c(\"relu\", \"relu6\", \"elu\", \"selu\", \"celu\", \"leaky_relu\", \"gelu\", \"softplus\",     \"softshrink\", \"softsign\", \"tanhshrink\", \"hardtanh\", \"hardshrink\", \"hardswish\",     \"hardsigmoid\", \"silu\", \"mish\", \"logsigmoid\") )  output_activation(   values = c(\"relu\", \"elu\", \"selu\", \"softplus\", \"softmax\", \"log_softmax\", \"logsigmoid\",     \"hardtanh\", \"hardsigmoid\", \"silu\") )  optimizer(values = c(\"adam\", \"sgd\", \"rmsprop\", \"adamw\"))  bias(values = c(TRUE, FALSE))  validation_split(range = c(0, 0.5), trans = NULL)  bidirectional(values = c(TRUE, FALSE))"},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"range two-element numeric vector default lower upper bounds. trans optional transformation; NULL none. values Logical vector possible values.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"architecture-strategy","dir":"Reference","previous_headings":"","what":"Architecture Strategy","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Since tidymodels tuning works independent parameters, use simplified approach : hidden_neurons specifies single value used layers activations specifies single activation used layers n_hlayers controls depth complex architectures different neurons/activations per layer, users manually specify tuning use custom tuning logic.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"n_hlayers Number hidden layers network. hidden_neurons Number units per hidden layer (applied layers). activation Single activation function applied hidden layers. output_activation Activation function output layer. optimizer Optimizer algorithm. bias Whether include bias terms layers. validation_split Proportion training data held validation. bidirectional Whether RNN layers bidirectional.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"number-of-hidden-layers","dir":"Reference","previous_headings":"","what":"Number of Hidden Layers","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Controls depth network. tuning, determine many layers created, hidden_neurons units activations function.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"hidden-units-per-layer","dir":"Reference","previous_headings":"","what":"Hidden Units per Layer","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Specifies number units per hidden layer.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"activation-function-hidden-layers-","dir":"Reference","previous_headings":"","what":"Activation Function (Hidden Layers)","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Activation functions hidden layers.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"output-activation-function","dir":"Reference","previous_headings":"","what":"Output Activation Function","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Activation function applied output layer. Values must correspond torch::nnf_* functions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"optimizer-type","dir":"Reference","previous_headings":"","what":"Optimizer Type","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"optimization algorithm used training.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"include-bias-terms","dir":"Reference","previous_headings":"","what":"Include Bias Terms","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Whether layers include bias parameters.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"validation-split-proportion","dir":"Reference","previous_headings":"","what":"Validation Split Proportion","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Fraction training data use validation set training.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"bidirectional-rnn","dir":"Reference","previous_headings":"","what":"Bidirectional RNN","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Whether recurrent layers process sequences directions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"","code":"if (FALSE) { # \\dontrun{ library(dials) library(tune)  # Create a tuning grid grid = grid_regular(     n_hlayers(range = c(1L, 4L)),     hidden_neurons(range = c(32L, 256L)),     activations(c('relu', 'elu', 'selu')),     levels = c(4, 5, 3) )  # Use in a model spec mlp_spec = mlp_kindling(     mode = \"classification\",     hidden_neurons = tune(),     activations = tune(),     epochs = tune(),     learn_rate = tune() ) } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":null,"dir":"Reference","previous_headings":"","what":"Depth-Aware Grid Generation for Neural Networks — grid_depth","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"grid_depth() extends standard grid generation support multi-layer neural network architectures. creates heterogeneous layer configurations generating list columns hidden_neurons activations.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"","code":"grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'parameters' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'list' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'workflow' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'model_spec' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'param' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # Default S3 method grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )"},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"x parameters object, list, workflow, model spec. Can also single param object ... contains additional param objects. ... One param objects (e.g., hidden_neurons(), epochs()). x parameters object, ... ignored. None objects can unknown() values. n_hlayer Integer vector specifying number hidden layers generate (e.g., 2:4 2, 3, 4 layers). Default 2. size Integer. Number parameter combinations generate. type Character. Type grid: \"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\". original Logical. original parameter ranges used? levels Integer. Levels per parameter regular grids. variogram_range Numeric. Range audze_eglais design. iter Integer. Iterations max_entropy optimization.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"tibble list columns hidden_neurons activations, element vector length n_hlayer.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"function specifically {kindling} models. n_hlayer parameter determines network depth creates list columns hidden_neurons activations, element vector length matching sampled depth.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"","code":"if (FALSE) { # \\dontrun{ library(dials)  # Method 1: Using parameters() params = parameters(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\", \"selu\")),     epochs(c(50L, 200L)) ) grid = grid_depth(params, n_hlayer = 2:3, type = \"regular\", levels = 3)  # Method 2: Direct param objects grid = grid_depth(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\")),     epochs(c(50L, 200L)),     n_hlayer = 2:3,     type = \"random\",     size = 20 )  # Method 3: From workflow wf = workflow() |>     add_model(mlp_kindling(hidden_neurons = tune(), activations = tune())) |>     add_formula(y ~ .) grid = grid_depth(wf, n_hlayer = 2:4, type = \"latin_hypercube\", size = 15) } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":null,"dir":"Reference","previous_headings":"","what":"Base models for Neural Network Training in kindling — ffnn","title":"Base models for Neural Network Training in kindling — ffnn","text":"Base models Neural Network Training kindling","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base models for Neural Network Training in kindling — ffnn","text":"","code":"ffnn(   formula,   data,   hidden_neurons,   activations = NULL,   output_activation = NULL,   bias = TRUE,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   ... )  rnn(   formula,   data,   hidden_neurons,   rnn_type = \"lstm\",   activations = NULL,   output_activation = NULL,   bias = TRUE,   bidirectional = TRUE,   dropout = 0,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   ... )"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base models for Neural Network Training in kindling — ffnn","text":"formula Formula. Model formula (e.g., y ~ x1 + x2). data Data frame. Training data. hidden_neurons Integer vector. Number neurons hidden layer. activations Activation function specifications. See act_funs(). output_activation Optional. Activation output layer. bias Logical. Use bias weights. Default TRUE. epochs Integer. Number training epochs. Default 100. batch_size Integer. Batch size training. Default 32. penalty Numeric. Regularization penalty (lambda). Default 0 (regularization). Higher values increase regularization strength. mixture Numeric. Elastic net mixing parameter (0-1). Default 0. 0: Pure L2 regularization (Ridge) 1: Pure L1 regularization (Lasso) 0 < mixture < 1: Elastic net (combination L1 L2) relevant penalty > 0. learn_rate Numeric. Learning rate optimizer. Default 0.001. optimizer Character. Optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Default \"adam\". optimizer_args Named list. Additional arguments passed optimizer. Default list(). loss Character. Loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). Default \"mse\". validation_split Numeric. Proportion data validation (0-1). Default 0. device Character. Device use (\"cpu\", \"cuda\", \"mps\"). Default NULL (auto-detect). verbose Logical. Print training progress. Default FALSE. cache_weights Logical. Cache weight matrices faster variable importance computation. Default FALSE. TRUE, weight matrices extracted stored returned object, avoiding repeated extraction importance calculations. enable plan compute variable importance multiple times. ... used. Reserved future extensions. rnn_type Character. Type RNN (\"rnn\", \"lstm\", \"gru\"). Default \"lstm\". bidirectional Logical. Use bidirectional RNN. Default TRUE. dropout Numeric. Dropout rate layers. Default 0.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Base models for Neural Network Training in kindling — ffnn","text":"object class \"ffnn_fit\" containing: model Trained torch module formula Model formula fitted.values Fitted values training data loss_history Training loss per epoch val_loss_history Validation loss per epoch (validation_split > 0) n_epochs Number epochs trained feature_names Names predictor variables response_name Name response variable device Device used training cached_weights Weight matrices (cache_weights = TRUE)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"ffnn","dir":"Reference","previous_headings":"","what":"FFNN","title":"Base models for Neural Network Training in kindling — ffnn","text":"Train feed-forward neural network using torch package.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"rnn","dir":"Reference","previous_headings":"","what":"RNN","title":"Base models for Neural Network Training in kindling — ffnn","text":"Train recurrent neural network using torch package.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base models for Neural Network Training in kindling — ffnn","text":"","code":"if (FALSE) { # \\dontrun{ if (torch::torch_is_installed()) {     # Regression task (auto-detect GPU)     model_reg = ffnn(         Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50,         verbose = FALSE     )      # With weight caching for multiple importance calculations     model_cached = ffnn(         Species ~ .,         data = iris,         hidden_neurons = c(128, 64, 32),         activations = \"relu\",         cache_weights = TRUE,         epochs = 100     ) } else {     message(\"Torch not fully installed – skipping example\") }  } # }  if (FALSE) { # \\dontrun{ # Regression with LSTM on GPU if (torch::torch_is_installed()) {     model_rnn = rnn(         Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         rnn_type = \"lstm\",         activations = \"relu\",         epochs = 50     )      # With weight caching     model_cached = rnn(         Species ~ .,         data = iris,         hidden_neurons = c(128, 64),         rnn_type = \"gru\",         cache_weights = TRUE,         epochs = 100     ) } else {     message(\"Torch not fully installed – skipping example\") } } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":null,"dir":"Reference","previous_headings":"","what":"Basemodels-tidymodels wrappers — ffnn_wrapper","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"Basemodels-tidymodels wrappers","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"","code":"ffnn_wrapper(formula, data, ...)  rnn_wrapper(formula, data, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"formula formula specifying model (e.g., y ~ x1 + x2) data data frame containing training data ... Additional arguments passed underlying training function","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"ffnn-mlp-wrapper-for-tidymodels-interface","dir":"Reference","previous_headings":"","what":"FFNN (MLP) Wrapper for {tidymodels} interface","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"function interface {tidymodels} (use , use kindling::ffnn() instead).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"rnn-wrapper-for-tidymodels-interface","dir":"Reference","previous_headings":"","what":"RNN Wrapper for {tidymodels} interface","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"function interface {tidymodels} (use , use kindling::rnn() instead).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable Importance Methods for kindling Models — kindling-varimp","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"file implements methods variable importance generics NeuralNetTools vip packages.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"","code":"# S3 method for class 'ffnn_fit' garson(mod_in, bar_plot = FALSE, ...)  # S3 method for class 'ffnn_fit' olden(mod_in, bar_plot = TRUE, ...)  # S3 method for class 'ffnn_fit' vi_model(object, type = c(\"olden\", \"garson\"), ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"mod_in fitted model object class \"ffnn_fit\". bar_plot Logical. Whether plot variable importance (default TRUE). ... Additional arguments passed methods. object fitted model object class \"ffnn_fit\". type Type algorithm extract variable importance. must one strings: 'olden' 'garson'","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"data frame variable importance scores. tibble columns \"Variable\" \"Importance\" (via vip::vi() / vip::vi_model() ).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"garson-s-algorithm-for-ffnn-models","dir":"Reference","previous_headings":"","what":"Garson's Algorithm for FFNN Models","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"{kindling} inherits NeuralNetTools::garson extract variable importance fitted ffnn() model.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"olden-s-algorithm-for-ffnn-models","dir":"Reference","previous_headings":"","what":"Olden's Algorithm for FFNN Models","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"{kindling} inherits NeuralNetTools::olden extract variable importance fitted ffnn() model.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"variable-importance-via-vip-package","dir":"Reference","previous_headings":"","what":"Variable Importance via {vip} Package","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"can directly use vip::vi() vip::vi_model() extract variable importance fitted ffnn() model.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"Beck, M.W. 2018. NeuralNetTools: Visualization Analysis Tools Neural Networks. Journal Statistical Software. 85(11):1-20. Garson, G.D. 1991. Interpreting neural network connection weights. Artificial Intelligence Expert. 6(4):46-51. Goh, .T.C. 1995. Back-propagation neural networks modeling complex systems. Artificial Intelligence Engineering. 9(3):143-151. Olden, J.D., Jackson, D.. 2002. Illuminating 'black-box': randomization approach understanding variable contributions artificial neural networks. Ecological Modelling. 154:135-150. Olden, J.D., Joy, M.K., Death, R.G. 2004. accurate comparison methods quantifying variable importance artificial neural networks using simulated data. Ecological Modelling. 178:389-397.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"","code":"# Directly use `NeuralNetTools::garson` if (torch::torch_is_installed()) {     model_mlp = ffnn(         Species ~ .,         data = iris,         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 100,         verbose = FALSE,         cache_weights = TRUE     )      model_mlp |>         garson() } else {     message(\"Torch not fully installed — skipping example\") } #>        x_names y_names  rel_imp #> 1 Petal.Length Species 26.79185 #> 2  Petal.Width Species 25.50513 #> 3  Sepal.Width Species 25.44254 #> 4 Sepal.Length Species 22.26048  # kindling also supports `vip::vi()` / `vip::vi_model()` if (torch::torch_is_installed()) {     model_mlp = ffnn(         Species ~ .,         data = iris,         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 100,         verbose = FALSE,         cache_weights = TRUE     )      model_mlp |>         vip::vi(type = 'garson') |>         vip::vip() } else {     message(\"Torch not fully installed — skipping example\") }"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"{kindling} enables R users build train deep neural networks : Deep Neural Networks / (Deep) Feedforward Neural Networks (DNN / FFNN) Recurrent Neural Networks (RNN) designed reduce boilerplate {torch} code FFNN RNN. also integrate seamlessly {tidymodels} components like {parsnip}, {recipes}, {workflows}, allowing flexibility consistent interface model specification, training, evaluation. Thus, package supports hyperparameter tuning : Number hidden layers Number units per layer Choice activation functions Note: hyperparameter tuning support currently implemented.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"{kindling} package provides unified, high-level interface bridges torch tidymodels ecosystems, making easy define, train, tune deep learning models using familiar tidymodels workflow.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"how-to-use","dir":"Reference","previous_headings":"","what":"How to use","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"following uses package 3 levels: Level 1: Code generation   Level 2: Direct Execution   Level 3: tidymodels interface part 1   Level 4: tidymodels interface part 2 - tuning (yet implemented)","code":"ffnn_generator(    nn_name = \"MyFFNN\",    hd_neurons = c(64, 32, 16),    no_x = 10,    no_y = 1,    activations = 'relu' ) ffnn(    Species ~ .,    data = iris,    hidden_neurons = c(128, 64, 32),    activations = 'relu',    loss = \"cross_entropy\",    epochs = 100 ) # library(parsnip) # library(kindling) box::use(    kindling[mlp_kindling, rnn_kindling, act_funs, args],    parsnip[fit, augment],    yardstick[metrics],    mlbench[Ionosphere] # data(Ionosphere, package = \"mlbench\") )  # Remove V2 as it's all zeros ionosphere_data = Ionosphere[, -2]  # MLP example mlp_kindling(    mode = \"classification\",    hidden_neurons = c(128, 64),    activations = act_funs(relu, softshrink = args(lambd = 0.5)),    epochs = 100 ) |>    fit(Class ~ ., data = ionosphere_data) |>    augment(new_data = ionosphere_data) |>    metrics(truth = Class, estimate = .pred_class) #> A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.989 #> 2 kap      binary         0.975  # RNN example (toy usage on non-sequential data) rnn_kindling(    mode = \"classification\",    hidden_neurons = c(128, 64),    activations = act_funs(relu, elu),    epochs = 100,    rnn_type = \"gru\" ) |>    fit(Class ~ ., data = ionosphere_data) |>    augment(new_data = ionosphere_data) |>    metrics(truth = Class, estimate = .pred_class) #> A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.641 #> 2 kap      binary         0"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"key-features","dir":"Reference","previous_headings":"","what":"Key Features","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"Define neural network models using parsnip::set_engine(\"kindling\") Integrate deep learning {tidymodels} workflows Support multiple architectures (DNN, RNN) Hyperparameter tuning architecture depth, units, activation Compatible {torch} tensors GPU acceleration","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"license","dir":"Reference","previous_headings":"","what":"License","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"MIT + file LICENSE","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"Falbel D, Luraschi J (2023). torch: Tensors Neural Networks 'GPU' Acceleration. R package version 0.13.0, https://torch.mlverse.org, https://github.com/mlverse/torch. Wickham H (2019). Advanced R, 2nd edition. Chapman Hall/CRC. ISBN 978-0815384571, https://adv-r.hadley.nz/. Goodfellow , Bengio Y, Courville (2016). Deep Learning. MIT Press. https://www.deeplearningbook.org/.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"Maintainer: Joshua Marie joshua.marie.k@gmail.com","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/make_kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Register kindling engines with parsnip — make_kindling","title":"Register kindling engines with parsnip — make_kindling","text":"function registers kindling engine MLP RNN models parsnip. called package loaded.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/make_kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register kindling engines with parsnip — make_kindling","text":"","code":"make_kindling()"},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"mlp_kindling() defines feedforward neural network model can used classification regression. integrates tidymodels ecosystem uses torch backend via kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"","code":"mlp_kindling(   mode = \"unknown\",   engine = \"kindling\",   hidden_neurons = NULL,   activations = NULL,   output_activation = NULL,   bias = NULL,   epochs = NULL,   batch_size = NULL,   penalty = NULL,   mixture = NULL,   learn_rate = NULL,   optimizer = NULL,   loss = NULL,   validation_split = NULL,   device = NULL,   verbose = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"mode single character string type model. Possible values \"unknown\", \"regression\", \"classification\". engine single character string specifying computational engine use fitting. Currently \"kindling\" supported. hidden_neurons integer vector number units hidden layer. Can tuned. activations character vector activation function names hidden layer (e.g., \"relu\", \"tanh\", \"sigmoid\"). Can tuned. output_activation character string output activation function. Can tuned. bias Logical whether include bias terms. Can tuned. epochs integer number training iterations. Can tuned. batch_size integer batch size training. Can tuned. penalty number regularization penalty (lambda). Default 0 (regularization). Higher values increase regularization strength. Can tuned. mixture number 0 1 elastic net mixing parameter. Default 0 (pure L2/Ridge regularization). 0: Pure L2 regularization (Ridge) 1: Pure L1 regularization (Lasso) 0 < mixture < 1: Elastic net (combination L1 L2) relevant penalty > 0. Can tuned. learn_rate number learning rate. Can tuned. optimizer character string optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Can tuned. loss character string loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). Can tuned. validation_split number 0 1 proportion data used validation. Can tuned. device character string device use (\"cpu\", \"cuda\", \"mps\"). NULL, auto-detects available GPU. Can tuned. verbose Logical whether print training progress. Default FALSE.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"model specification object class mlp_kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"function creates model specification feedforward neural network can used within tidymodels workflows. model supports: Multiple hidden layers configurable units Various activation functions per layer GPU acceleration (CUDA, MPS, CPU) Hyperparameter tuning integration regression classification tasks hidden_neurons parameter accepts integer vector element represents number neurons hidden layer. example, hidden_neurons = c(128, 64, 32) creates network three hidden layers. device parameter controls computation occurs: NULL (default): Auto-detect best available device (CUDA > MPS > CPU) \"cuda\": Use NVIDIA GPU \"mps\": Use Apple Silicon GPU \"cpu\": Use CPU tuning, can use special tune tokens: hidden_neurons: use tune(\"hidden_neurons\") custom range activation: use tune(\"activation\") values like \"relu\", \"tanh\" device: use tune(\"device\") compare CPU vs GPU performance","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"","code":"if (FALSE) { # \\dontrun{ if (torch::torch_is_installed()) {     box::use(         recipes[recipe],         workflows[workflow, add_recipe, add_model],         tune[tune],         parsnip[fit]     )      # Model specs     mlp_spec = mlp_kindling(         mode = \"classification\",         hidden_neurons = c(128, 64, 32),         activation = c(\"relu\", \"relu\", \"relu\"),         epochs = 100     )      # If you want to tune     mlp_tune_spec = mlp_kindling(         mode = \"classification\",         hidden_neurons = tune(),         activation = tune(),         epochs = tune(),         learn_rate = tune()     )      wf = workflow() |>         add_recipe(recipe(Species ~ ., data = iris)) |>         add_model(mlp_spec)       fit_wf = fit(wf, data = iris) } else {     message(\"Torch not fully installed — skipping example\") } } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":null,"dir":"Reference","previous_headings":"","what":"Functions to generate nn_module (language) expression — ffnn_generator","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"Functions generate nn_module (language) expression","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"","code":"ffnn_generator(   nn_name = \"DeepFFN\",   hd_neurons,   no_x,   no_y,   activations = NULL,   output_activation = NULL,   bias = TRUE )  rnn_generator(   nn_name = \"DeepRNN\",   hd_neurons,   no_x,   no_y,   rnn_type = \"lstm\",   bias = TRUE,   activations = NULL,   output_activation = NULL,   bidirectional = TRUE,   dropout = 0,   ... )"},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"nn_name Character. Name generated RNN module class. Default \"DeepRNN\". hd_neurons Integer vector. Number neurons hidden RNN layer. no_x Integer. Number input features. no_y Integer. Number output features. activations Activation function specifications hidden layer. Can : NULL: activation functions. Character vector: e.g., c(\"relu\", \"sigmoid\"). List: e.g., act_funs(relu, elu, softshrink = args(lambd = 0.5)). activation_spec object act_funs(). length activations 1L, activation throughout architecture. output_activation Optional. Activation function output layer. format activations single activation. bias Logical. Whether use bias weights. Default TRUE rnn_type Character. Type RNN use. Must one \"rnn\", \"lstm\", \"gru\". Default \"lstm\". bidirectional Logical. Whether use bidirectional RNN layers. Default TRUE. dropout Numeric. Dropout rate RNN layers. Default 0. ... Additional arguments (currently unused).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"torch module expression representing FFNN. torch module expression representing RNN.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"generated FFNN module specified number hidden layers, layer containing specified number neurons. Activation functions can applied hidden layer specified. can used classification regression tasks. generated module properly namespaces torch functions avoid polluting global namespace. generated RNN module specified number recurrent layers, layer containing specified number hidden units. Activation functions can applied RNN layer specified. final output taken last time step passed linear layer. generated module properly namespaces torch functions avoid polluting global namespace.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"feed-forward-neural-network-module-generator","dir":"Reference","previous_headings":"","what":"Feed-Forward Neural Network Module Generator","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"ffnn_generator() function generates feed-forward neural network (FFNN) module expression torch package R. allows customization FFNN architecture, including number hidden layers, neurons, activation functions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"recurrent-neural-network-module-generator","dir":"Reference","previous_headings":"","what":"Recurrent Neural Network Module Generator","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"rnn_generator() function generates recurrent neural network (RNN) module expression torch package R. allows customization RNN architecture, including number hidden layers, neurons, RNN type, activation functions, parameters.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"","code":"# FFNN if (torch::torch_is_installed()) {     # Generate an MLP module with 3 hidden layers     ffnn_mod = ffnn_generator(         nn_name = \"MyFFNN\",         hd_neurons = c(64, 32, 16),         no_x = 10,         no_y = 1,         activations = 'relu'     )      # Evaluate and instantiate     model = eval(ffnn_mod)()      # More complex: With different activations     ffnn_mod2 = ffnn_generator(         nn_name = \"MyFFNN2\",         hd_neurons = c(128, 64, 32),         no_x = 20,         no_y = 5,         activations = act_funs(             relu,             selu,             sigmoid         )     )      # Even more complex: Different activations and customized argument     # for the specific activation function     ffnn_mod2 = ffnn_generator(         nn_name = \"MyFFNN2\",         hd_neurons = c(128, 64, 32),         no_x = 20,         no_y = 5,         activations = act_funs(             relu,             selu,             softshrink = args(lambd = 0.5)         )     )      # Customize output activation (softmax is useful for classification tasks)     ffnn_mod3 = ffnn_generator(         hd_neurons = c(64, 32),         no_x = 10,         no_y = 3,         activations = 'relu',         output_activation = act_funs(softmax = args(dim = 2L))     ) } else {     message(\"Torch not fully installed — skipping example\") }  ## RNN if (torch::torch_is_installed()) {     # Basic LSTM with 2 layers     rnn_mod = rnn_generator(         nn_name = \"MyLSTM\",         hd_neurons = c(64, 32),         no_x = 10,         no_y = 1,         rnn_type = \"lstm\",         activations = 'relu'     )      # Evaluate and instantiate     model = eval(rnn_mod)()      # GRU with different activations     rnn_mod2 = rnn_generator(         nn_name = \"MyGRU\",         hd_neurons = c(128, 64, 32),         no_x = 20,         no_y = 5,         rnn_type = \"gru\",         activations = act_funs(relu, elu, relu),         bidirectional = FALSE     )  } else {     message(\"Torch not fully installed — skipping example\") }  if (FALSE) { # \\dontrun{ ## Parameterized activation and dropout # (Will throw an error due to `nnf_tanh()` not being available in `{torch}`) # rnn_mod3 = rnn_generator( #     hd_neurons = c(100, 50, 25), #     no_x = 15, #     no_y = 3, #     rnn_type = \"lstm\", #     activations = act_funs( #         relu, #         leaky_relu = args(negative_slope = 0.01), #         tanh #     ), #     bidirectional = TRUE, #     dropout = 0.3 # ) } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":null,"dir":"Reference","previous_headings":"","what":"Ordinal Suffixes Generator — ordinal_gen","title":"Ordinal Suffixes Generator — ordinal_gen","text":"function originally numform::f_ordinal().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ordinal Suffixes Generator — ordinal_gen","text":"","code":"ordinal_gen(x)"},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ordinal Suffixes Generator — ordinal_gen","text":"x Vector numbers. string equivalent","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"this-is-how-you-use-it","dir":"Reference","previous_headings":"","what":"This is how you use it","title":"Ordinal Suffixes Generator — ordinal_gen","text":"Note: exported public namespace. please, refer numform::f_ordinal() instead.","code":"kindling:::ordinal_gen(1:10)"},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Ordinal Suffixes Generator — ordinal_gen","text":"Rinker, T. W. (2021). numform: publication style number plot formatter version 0.7.0. https://github.com/trinker/numform","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict.ffnn_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for FFNN Fits — predict.ffnn_fit","title":"Predict Method for FFNN Fits — predict.ffnn_fit","text":"Predict Method FFNN Fits","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict.ffnn_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for FFNN Fits — predict.ffnn_fit","text":"","code":"# S3 method for class 'ffnn_fit' predict(object, newdata = NULL, type = \"response\", ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/predict.ffnn_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for FFNN Fits — predict.ffnn_fit","text":"object object class \"ffnn_fit\". newdata Data frame. New data predictions. type Character. Type prediction: \"response\" (default) \"prob\" classification. ... Additional arguments (unused).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict.rnn_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for RNN Fits — predict.rnn_fit","title":"Predict Method for RNN Fits — predict.rnn_fit","text":"Predict Method RNN Fits","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict.rnn_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for RNN Fits — predict.rnn_fit","text":"","code":"# S3 method for class 'rnn_fit' predict(object, newdata = NULL, type = \"response\", ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/predict.rnn_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for RNN Fits — predict.rnn_fit","text":"object object class \"rnn_fit\". newdata Data frame. New data predictions. type Character. Type prediction: \"response\" (default) \"prob\" classification. ... Additional arguments (unused).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/prepare_kindling_args.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare arguments for kindling models — prepare_kindling_args","title":"Prepare arguments for kindling models — prepare_kindling_args","text":"Prepare arguments kindling models","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/prepare_kindling_args.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare arguments for kindling models — prepare_kindling_args","text":"","code":"prepare_kindling_args(args)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for ffnn_fit objects — print.ffnn_fit","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"Print method ffnn_fit objects","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"","code":"# S3 method for class 'ffnn_fit' print(x, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"x object class \"ffnn_fit\" ... Additional arguments (unused)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for rnn_fit objects — print.rnn_fit","title":"Print method for rnn_fit objects — print.rnn_fit","text":"Print method rnn_fit objects","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for rnn_fit objects — print.rnn_fit","text":"","code":"# S3 method for class 'rnn_fit' print(x, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for rnn_fit objects — print.rnn_fit","text":"x object class \"rnn_fit\" ... Additional arguments (unused)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. NeuralNetTools garson, olden vip vi_model","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Recurrent Neural Network via kindling — rnn_kindling","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"rnn_kindling() defines recurrent neural network model can used classification regression sequential data. integrates tidymodels ecosystem uses torch backend via kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"","code":"rnn_kindling(   mode = \"unknown\",   engine = \"kindling\",   hidden_neurons = NULL,   rnn_type = NULL,   activations = NULL,   output_activation = NULL,   bias = NULL,   bidirectional = NULL,   dropout = NULL,   epochs = NULL,   batch_size = NULL,   penalty = NULL,   mixture = NULL,   learn_rate = NULL,   optimizer = NULL,   loss = NULL,   validation_split = NULL,   device = NULL,   verbose = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"mode single character string type model. Possible values \"unknown\", \"regression\", \"classification\". engine single character string specifying computational engine use fitting. Currently \"kindling\" supported. hidden_neurons integer vector number units hidden layer. Can tuned. rnn_type character string type RNN cell (\"rnn\", \"lstm\", \"gru\"). Can tuned. activations character vector activation function names hidden layer (e.g., \"relu\", \"tanh\", \"sigmoid\"). Can tuned. output_activation character string output activation function. Can tuned. bias Logical whether include bias terms. Can tuned. bidirectional logical indicating whether use bidirectional RNN. Can tuned. dropout number 0 1 dropout rate layers. Can tuned. epochs integer number training iterations. Can tuned. batch_size integer batch size training. Can tuned. penalty number regularization penalty (lambda). Default 0 (regularization). Higher values increase regularization strength. Can tuned. mixture number 0 1 elastic net mixing parameter. Default 0 (pure L2/Ridge regularization). 0: Pure L2 regularization (Ridge) 1: Pure L1 regularization (Lasso) 0 < mixture < 1: Elastic net (combination L1 L2) relevant penalty > 0. Can tuned. learn_rate number learning rate. Can tuned. optimizer character string optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Can tuned. loss character string loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). Can tuned. validation_split number 0 1 proportion data used validation. Can tuned. device character string device use (\"cpu\", \"cuda\", \"mps\"). NULL, auto-detects available GPU. Can tuned. verbose Logical whether print training progress. Default FALSE.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"model specification object class rnn_kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"function creates model specification recurrent neural network can used within tidymodels workflows. model supports: Multiple RNN types: basic RNN, LSTM, GRU Bidirectional processing Dropout regularization GPU acceleration (CUDA, MPS, CPU) Hyperparameter tuning integration regression classification tasks device parameter controls computation occurs: NULL (default): Auto-detect best available device (CUDA > MPS > CPU) \"cuda\": Use NVIDIA GPU \"mps\": Use Apple Silicon GPU \"cpu\": Use CPU ","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"","code":"if (FALSE) { # \\dontrun{ if (torch::torch_is_installed()) {     box::use(         recipes[recipe],         workflows[workflow, add_recipe, add_model],         parsnip[fit]     )      # Model specs     rnn_spec = rnn_kindling(         mode = \"classification\",         hidden_neurons = c(64, 32),         rnn_type = \"lstm\",         activation = c(\"relu\", \"elu\"),         epochs = 100,         bidirectional = TRUE     )      wf = workflow() |>         add_recipe(recipe(Species ~ ., data = iris)) |>         add_model(rnn_spec)      fit_wf = fit(wf, data = iris)     fit_wf } else {     message(\"Torch not fully installed — skipping example\") } } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"function takes two-column data frame formats summary-like table. table can optionally split two parts, centered, given title. useful displaying summary information clean, tabular format. function also supports styling ANSI colors text formatting {cli} package column alignment options.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"","code":"table_summary(   data,   title = NULL,   l = NULL,   header = FALSE,   center_table = FALSE,   border_char = \"-\",   style = list(),   align = NULL,   ... )"},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"data data frame exactly two columns. data summarized displayed. title character string. optional title displayed table. l integer. number rows include left part split table. NULL, table split. header logical value. TRUE, column names data displayed header. center_table logical value. TRUE, table centered terminal. border_char Character used borders. Default \"\\u2500\". style list controlling visual styling table elements using ANSI formatting. Can include following components: left_col: Styling left column values. right_col: Styling right column values. border_text: Styling border. title: Styling title. sep: Separator character left right column. style component can either predefined style string (e.g., \"blue\", \"red_italic\", \"bold\") function takes context list /without value element returns styled text. align Controls alignment column values. Can specified three ways: single string: affects left column (e.g., \"left\", \"center\", \"right\"). vector two strings: affects columns order (e.g., c(\"left\", \"right\")). list named components: explicitly specifies alignment column ... Additional arguments (currently unused).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"function return value. prints formatted table console.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"","code":"# Create a sample data frame df = data.frame(     Category = c(\"A\", \"B\", \"C\", \"D\", \"E\"),     Value = c(10, 20, 30, 40, 50) )  # Display the table with a title and header table_summary(df, title = \"Sample Table\", header = TRUE) #>  #>     Sample Table      #> --------------------- #>   Category    Value #> --------------------- #>   A              10 #>   B              20 #>   C              30 #>   D              40 #>   E              50 #> ---------------------  # Split the table after the second row and center it table_summary(df, l = 2, center_table = TRUE) #> Warning: running command 'tput cols' had status 2 #> ------------------------------------------ #>   A              10    C              30 #>   B              20    D              40 #>                        E              50 #> ------------------------------------------  # Use styling and alignment table_summary(     df, header = TRUE,     style = list(         left_col = \"blue_bold\",         right_col = \"red\",         title = \"green\",         border_text = \"yellow\"     ),     align = c(\"center\", \"right\") ) #> --------------------- #>   Category    Value #> --------------------- #>      A           10 #>      B           20 #>      C           30 #>      D           40 #>      E           50 #> ---------------------  # Use custom styling with lambda functions table_summary(     df, header = TRUE,     style = list(         left_col = \\(ctx) cli::col_red(ctx), # ctx$value is another option         right_col = \\(ctx) cli::col_blue(ctx)     ),     align = list(left_col = \"left\", right_col = \"right\") ) #> --------------------- #>   Category    Value #> --------------------- #>   A              10 #>   B              20 #>   C              30 #>   D              40 #>   E              50 #> ---------------------"},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate device and get default device — validate_device","title":"Validate device and get default device — validate_device","text":"Check requested device available. auto-detect available GPU device fallback CPU.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate device and get default device — validate_device","text":"","code":"validate_device(device)"},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate device and get default device — validate_device","text":"device Character. Requested device.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate device and get default device — validate_device","text":"Character string validated device.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"new-features-development-version","dir":"Changelog","previous_headings":"","what":"New features","title":"kindling (development version)","text":"Added regularization support neural network models L1 regularization (Lasso) feature selection via mixture = 1 L2 regularization (Ridge) weight decay via mixture = 0 Elastic Net combining L1 L2 penalties via 0 < mixture < 1 Controlled via penalty (regularization strength) mixture (L1/L2 balance) parameters Follows tidymodels conventions consistency glmnet packages","code":""},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"kindling-010","dir":"Changelog","previous_headings":"","what":"kindling 0.1.0","title":"kindling 0.1.0","text":"Initial CRAN release Higher-level interface torch package define, train, tune neural networks Support feedforward (multi-layer perceptron) recurrent networks (RNN, LSTM, GRU) Integration tidymodels ecosystem (parsnip, workflows, recipes, tuning) Variable importance plots network visualization tools","code":""}]
