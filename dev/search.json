[{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement joshua.marie.k@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to kindling","title":"Contributing to kindling","text":"outlines propose change kindling. detailed discussion contributing tidyverse packages, please see development contributing guide code review principles.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to kindling","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to kindling","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed). See guide create great issue advice.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to kindling","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"joshuamarie/kindling\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to kindling","text":"New code follow tidyverse style guide. can use Air apply style, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to kindling","text":"Please note kindling project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://kindling.joshuamarie.com/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 kindling authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting Started with kindling","text":"kindling bridges gap torch tidymodels, providing streamlined interface building, training, tuning deep learning models. vignette guide basic usage.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Getting Started with kindling","text":"can install kindling CRAN: install development version GitHub:","code":"install.packages('kindling') # install.packages(\"pak\") pak::pak(\"joshuamarie/kindling\") ## devtools::install_github(\"joshuamarie/kindling\") library(kindling) #>  #> Attaching package: 'kindling' #> The following object is masked from 'package:base': #>  #>     args"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"before-using-kindling","dir":"Articles","previous_headings":"","what":"Before using {kindling}","title":"Getting Started with kindling","text":"starting, need install LibTorch, backend PyTorch also backend torch R package:","code":"torch::install_torch()"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"four-levels-of-interaction","dir":"Articles","previous_headings":"","what":"Four Levels of Interaction","title":"Getting Started with kindling","text":"kindling offers flexibility four levels abstraction: Code Generation - Generate raw torch::nn_module code Direct Training - Train models simple function calls tidymodels Integration - Use parsnip, recipes, workflows Hyperparameter Tuning - Optimize models tune dials","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"level-1-code-generation","dir":"Articles","previous_headings":"","what":"Level 1: Code Generation","title":"Getting Started with kindling","text":"Generate PyTorch-style module code:","code":"ffnn_generator(     nn_name = \"MyNetwork\",     hd_neurons = c(64, 32),     no_x = 10,     no_y = 1,     activations = 'relu' )"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"level-2-direct-training","dir":"Articles","previous_headings":"","what":"Level 2: Direct Training","title":"Getting Started with kindling","text":"Train model one function call:","code":"model = ffnn(     Species ~ .,     data = iris,     hidden_neurons = c(10, 15, 7),     activations = act_funs(relu, elu), # c(\"relu\", \"elu\")     loss = \"cross_entropy\",     epochs = 100 )  predictions = predict(model, newdata = iris)"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"level-3-tidymodels-integration","dir":"Articles","previous_headings":"","what":"Level 3: tidymodels Integration","title":"Getting Started with kindling","text":"Work neural networks like parsnip model:","code":"box::use(     parsnip[fit, augment],     yardstick[metrics] )  nn_spec = mlp_kindling(     mode = \"classification\",     hidden_neurons = c(10, 7),     activations = act_funs(relu, softshrink = args(lambd = 0.5)),     epochs = 100 )  nn_fit = fit(nn_spec, Species ~ ., data = iris) augment(nn_fit, new_data = iris) |>      metrics(truth = Species, estimate = .pred_class)"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"learn-more","dir":"Articles","previous_headings":"","what":"Learn More","title":"Getting Started with kindling","text":"Visit package website: https://kindling.joshuamarie.com Report issues: https://github.com/joshuamarie/kindling/issues","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/similar-packages.html","id":"similar-packages","dir":"Articles","previous_headings":"","what":"Similar packages","title":"Similar packages and comparison","text":"packages discussed built top torch, R’s native implementation PyTorch. torch package provides low-level tensor operations neural network building blocks, requires substantial boilerplate code training. Higher-level packages like kindling, brulee, cito, luz simplify process offering different features design philosophies. kindling distinguishes unique code generation approach, versatile neural architecture support (can expanded future), three-level API design. brulee focuses production-ready statistical models, cito emphasizes explainability statistical inference, luz provides adaptable training loops. kindling different mutually exclusive : offers deep architectural control bridges gap torch code tidymodels workflows.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/articles/similar-packages.html","id":"complementary-use","dir":"Articles","previous_headings":"","what":"Complementary Use","title":"Similar packages and comparison","text":"packages aren’t mutually exclusive. can use kindling brulee production MLPs, except kindling provides RNNs. cito package need oriented model interpretation. , luz package ideal want less verbose training loops. Despite difference philosophies main usage, integrate build upon torch ecosystem, allowing switch modeling needs evolve. instance, prototyping kindling explore different network architectures much easier, well deploying models production, just like brulee, cito stakeholders need detailed explanations model predictions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Joshua Marie. Author, maintainer.","code":""},{"path":"https://kindling.joshuamarie.com/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Marie, Joshua (2026). kindling: Higher-Level Interface 'torch' Package Auto-Train Neural Networks https://kindling.joshuamarie.com","code":"@Manual{,   title = {{kindling}: Higher-Level Interface of 'torch' Package to Auto-Train Neural Networks},   author = {Joshua Marie},   year = {2026},   note = {R package version 0.1.0.9000}, }"},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"package-overview","dir":"","previous_headings":"","what":"Package overview","title":"kindling • Higher-Level Interface for torch","text":"Title: Higher-Level Interface ‘torch’ Package Auto-Train Neural Networks Whether ’re generating neural network architectures expressions fitting/training actual models, kindling minimizes boilerplate code preserving torch. Since package uses torch backend, GPU/TPU devices also supported. kindling also bridges gap torch tidymodels. works seamlessly parsnip, recipes, workflows bring deep learning existing tidymodels modeling pipeline. enables streamlined interface building, training, tuning deep learning models within familiar tidymodels ecosystem.","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"main-features","dir":"","previous_headings":"Package overview","what":"Main Features","title":"kindling • Higher-Level Interface for torch","text":"Code generation torch expression Multiple architectures available: feedforward networks (MLP/DNN/FFNN) recurrent variants (RNN, LSTM, GRU) Native support tidymodels workflows pipelines Fine-grained control network depth, layer sizes, activation functions GPU acceleration supports via torch tensors","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"supported-architectures-as-of-now","dir":"","previous_headings":"Package overview","what":"Supported Architectures (As of now)","title":"kindling • Higher-Level Interface for torch","text":"Feedforward Networks (DNN/FFNN): Classic multi-layer perceptrons tabular data general supervised learning Recurrent Neural Networks (RNN): Basic recurrent architecture sequential patterns Long Short-Term Memory (LSTM): Sophisticated recurrent networks gating mechanisms long-range dependencies Gated Recurrent Units (GRU): Streamlined alternative LSTM fewer parameters","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"kindling • Higher-Level Interface for torch","text":"can install kindling CRAN: install development version GitHub:","code":"install.packages('kindling') # install.packages(\"pak\") pak::pak(\"joshuamarie/kindling\") ## devtools::install_github(\"joshuamarie/kindling\")"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"usage-three-levels-of-interaction","dir":"","previous_headings":"","what":"Usage: Three Levels of Interaction","title":"kindling • Higher-Level Interface for torch","text":"kindling powered R’s metaprogramming capabilities code generation. Generated torch::nn_module() expressions power training functions, turn serve engines tidymodels integration. architecture gives flexibility work whatever abstraction level suits task. starting, need install LibTorch, backend PyTorch also backend torch R package:","code":"library(kindling) #>  #> Attaching package: 'kindling' #> The following object is masked from 'package:base': #>  #>     args torch::install_torch()"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-1-code-generation-for-torchnn_module","dir":"","previous_headings":"Usage: Three Levels of Interaction","what":"Level 1: Code Generation for torch::nn_module","title":"kindling • Higher-Level Interface for torch","text":"lowest level, can generate raw torch::nn_module code maximum customization. Functions ending _generator return unevaluated expressions can inspect, modify, execute. ’s generate feedforward network specification: creates three-hidden-layer network (64 - 32 - 16 neurons) takes 10 inputs produces 1 output. hidden layer uses ReLU activation, output layer remains “untransformed”.","code":"ffnn_generator(     nn_name = \"MyFFNN\",     hd_neurons = c(64, 32, 16),     no_x = 10,     no_y = 1,     activations = 'relu' ) #> torch::nn_module(\"MyFFNN\", initialize = function ()  #> { #>     self$fc1 = torch::nn_linear(10, 64, bias = TRUE) #>     self$fc2 = torch::nn_linear(64, 32, bias = TRUE) #>     self$fc3 = torch::nn_linear(32, 16, bias = TRUE) #>     self$out = torch::nn_linear(16, 1, bias = TRUE) #> }, forward = function (x)  #> { #>     x = self$fc1(x) #>     x = torch::nnf_relu(x) #>     x = self$fc2(x) #>     x = torch::nnf_relu(x) #>     x = self$fc3(x) #>     x = torch::nnf_relu(x) #>     x = self$out(x) #>     x #> })"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-2-direct-training-interface","dir":"","previous_headings":"Usage: Three Levels of Interaction","what":"Level 2: Direct Training Interface","title":"kindling • Higher-Level Interface for torch","text":"Skip code generation train models directly data. approach handles torch boilerplate training models internally. Let’s classify iris species: Evaluate prediction predict(). predict() method extended fitted models newdata argument. Two kinds predict() usage: Without newdata predictions default parent data frame. newdata simply pass new data frame new reference.","code":"model = ffnn(     Species ~ .,     data = iris,     hidden_neurons = c(10, 15, 7),     activations = act_funs(relu, softshrink = args(lambd = 0.5), elu),      loss = \"cross_entropy\",     epochs = 100 )  model ======================= Feedforward Neural Networks (MLP) ======================   -- FFNN Model Summary ----------------------------------------------------------      -----------------------------------------------------------------------       NN Model Type           :             FFNN    n_predictors :      4       Number of Epochs        :              100    n_response   :      3       Hidden Layer Units      :        10, 15, 7    reg.         :   None       Number of Hidden Layers :                3    Device       :    cpu       Pred. Type              :   classification                 :            -----------------------------------------------------------------------    -- Activation function ---------------------------------------------------------                 -------------------------------------------------                  1st Layer {10}    :                      relu                  2nd Layer {15}    :   softshrink(lambd = 0.5)                  3rd Layer {7}     :                       elu                  Output Activation :   No act function applied                ------------------------------------------------- predict(model) |>     (\\(x) table(actual = iris$Species, predicted = x))() #>             predicted #> actual       setosa versicolor virginica #>   setosa         50          0         0 #>   versicolor      0         46         4 #>   virginica       0          0        50 sample_iris = dplyr::slice_sample(iris, n = 10, by = Species)  predict(model, newdata = sample_iris) |>     (\\(x) table(actual = sample_iris$Species, predicted = x))() #>             predicted #> actual       setosa versicolor virginica #>   setosa         10          0         0 #>   versicolor      0          9         1 #>   virginica       0          0        10"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-3-conventional-tidymodels-integration","dir":"","previous_headings":"Usage: Three Levels of Interaction","what":"Level 3: Conventional tidymodels Integration","title":"kindling • Higher-Level Interface for torch","text":"Work neural networks just like parsnip model. unlocks entire tidymodels toolkit preprocessing, cross-validation, model evaluation.","code":"# library(kindling) # library(parsnip) # library(yardstick) box::use(     kindling[mlp_kindling, rnn_kindling, act_funs, args],     parsnip[fit, augment],     yardstick[metrics],     mlbench[Ionosphere] # data(Ionosphere, package = \"mlbench\") )  ionosphere_data = Ionosphere[, -2]  # Train a feedforward network with parsnip mlp_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, softshrink = args(lambd = 0.5)),     epochs = 100 ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> # A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.989 #> 2 kap      binary         0.975  # Or try a recurrent architecture (demonstrative example with tabular data) rnn_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, elu),     epochs = 100,     rnn_type = \"gru\" ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> # A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.641 #> 2 kap      binary         0"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"hyperparameter-tuning--resampling","dir":"","previous_headings":"","what":"Hyperparameter Tuning & Resampling","title":"kindling • Higher-Level Interface for torch","text":"package integration tidymodels, supports hyperparameter tuning via tune searchable parameters. current searchable parameters kindling: Layer widths (neurons per layer) Network depth (number hidden layers) Activation function combinations Output activation Optimizer (Type optimization algorithm) Bias (choose presence absence bias term) Validation Split Proportion Bidirectional (boolean; RNN) searchable parameters outside kindling, .e. dials package learn_rate() also supported. ’s example: Resampling strategies rsample enable robust cross-validation workflows, orchestrated tune dials APIs.","code":"# library(tidymodels) box::use(     kindling[         mlp_kindling, hidden_neurons, activations, output_activation, grid_depth     ],     parsnip[fit, augment],     recipes[recipe],     workflows[workflow, add_recipe, add_model],     rsample[vfold_cv],     tune[tune_grid, tune, select_best, finalize_workflow],     dials[grid_random],     yardstick[accuracy, roc_auc, metric_set, metrics] )  mlp_tune_spec = mlp_kindling(     mode = \"classification\",     hidden_neurons = tune(),     activations = tune(),     output_activation = tune() )  iris_folds = vfold_cv(iris, v = 3) nn_wf = workflow() |>     add_recipe(recipe(Species ~ ., data = iris)) |>     add_model(mlp_tune_spec)  nn_grid_depth = grid_depth(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\")),     output_activation(c(\"sigmoid\", \"linear\")),     n_hlayer = 2,     size = 10,     type = \"latin_hypercube\" )  # This is supported but limited to 1 hidden layer only ## nn_grid = grid_random( ##     hidden_neurons(c(32L, 128L)), ##     activations(c(\"relu\", \"elu\")), ##     output_activation(c(\"sigmoid\", \"linear\")), ##     size = 10 ## )  nn_tunes = tune::tune_grid(     nn_wf,     iris_folds,     grid = nn_grid_depth     # metrics = metric_set(accuracy, roc_auc) )  best_nn = select_best(nn_tunes) final_nn = finalize_workflow(nn_wf, best_nn) # Last run: 4 - 91 (relu) - 3 (sigmoid) units final_nn_model = fit(final_nn, data = iris)  final_nn_model |>     augment(new_data = iris) |>     metrics(truth = Species, estimate = .pred_class) #> # A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy multiclass     0.667 #> 2 kap      multiclass     0.5"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"variable-importance","dir":"","previous_headings":"","what":"Variable Importance","title":"kindling • Higher-Level Interface for torch","text":"kindling integrates established variable importance methods {NeuralNetTools} vip interpret trained neural networks. Two primary algorithms available: Garson’s Algorithm Olden’s Algorithm","code":"garson(model, bar_plot = FALSE) #>        x_names y_names  rel_imp #> 1  Petal.Width       y 28.49543 #> 2  Sepal.Width       y 25.82405 #> 3 Sepal.Length       y 24.21862 #> 4 Petal.Length       y 21.46190 olden(model, bar_plot = FALSE) #>        x_names y_names     rel_imp #> 1  Petal.Width       y -0.26225266 #> 2  Sepal.Width       y  0.20693472 #> 3 Petal.Length       y -0.15842190 #> 4 Sepal.Length       y -0.09426378"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"integration-with-vip","dir":"","previous_headings":"Variable Importance","what":"Integration with {vip}","title":"kindling • Higher-Level Interface for torch","text":"users working within tidymodels ecosystem, kindling models work seamlessly vip package:  Note: Weight caching increases memory usage proportional network size. enable plan compute variable importance multiple times model.","code":"box::use(     vip[vi, vip] )  vi(model) |>      vip()"},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"kindling • Higher-Level Interface for torch","text":"Falbel D, Luraschi J (2023). torch: Tensors Neural Networks ‘GPU’ Acceleration. R package version 0.13.0, https://torch.mlverse.org, https://github.com/mlverse/torch. Wickham H (2019). Advanced R, 2nd edition. Chapman Hall/CRC. ISBN 978-0815384571, https://adv-r.hadley.nz/. Goodfellow , Bengio Y, Courville (2016). Deep Learning. MIT Press. https://www.deeplearningbook.org/.","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"kindling • Higher-Level Interface for torch","text":"MIT + file LICENSE","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"kindling • Higher-Level Interface for torch","text":"Please note kindling project released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":null,"dir":"Reference","previous_headings":"","what":"Activation Functions Specification Helper — act_funs","title":"Activation Functions Specification Helper — act_funs","text":"function DSL function, kind like ggplot2::aes(), helps specify activation functions neural network layers. validates activation functions exist torch parameters match function's formal arguments.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Activation Functions Specification Helper — act_funs","text":"","code":"act_funs(...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Activation Functions Specification Helper — act_funs","text":"... Activation function specifications. Can : Bare symbols: relu, tanh Character strings (simple): \"relu\", \"tanh\" Character strings (params): \"softshrink(lambda = 0.1)\", \"rrelu(lower = 1/5, upper = 1/4)\" Named parameters: softmax = args(dim = 2L)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Activation Functions Specification Helper — act_funs","text":"vctrs vector class \"activation_spec\" containing validated activation specifications.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":null,"dir":"Reference","previous_headings":"","what":"Activation Function Arguments Helper — args","title":"Activation Function Arguments Helper — args","text":"Type-safe helper specify parameters activation functions. parameters must named match formal arguments corresponding torch activation function.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Activation Function Arguments Helper — args","text":"","code":"args(...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Activation Function Arguments Helper — args","text":"... Named arguments activation function.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Activation Function Arguments Helper — args","text":"list class \"activation_args\" containing parameters.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Tunable hyperparameters for kindling models — dials-kindling","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"parameters extend dials framework support hyperparameter tuning neural networks built {kindling} package. control network architecture, activation functions, optimization, training behavior.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"","code":"n_hlayers(range = c(1L, 2L), trans = NULL)  hidden_neurons(range = c(8L, 512L), trans = NULL)  activations(   values = c(\"relu\", \"relu6\", \"elu\", \"selu\", \"celu\", \"leaky_relu\", \"gelu\", \"softplus\",     \"softshrink\", \"softsign\", \"tanhshrink\", \"hardtanh\", \"hardshrink\", \"hardswish\",     \"hardsigmoid\", \"silu\", \"mish\", \"logsigmoid\") )  output_activation(   values = c(\"relu\", \"elu\", \"selu\", \"softplus\", \"softmax\", \"log_softmax\", \"logsigmoid\",     \"hardtanh\", \"hardsigmoid\", \"silu\") )  optimizer(values = c(\"adam\", \"sgd\", \"rmsprop\", \"adamw\"))  bias(values = c(TRUE, FALSE))  validation_split(range = c(0, 0.5), trans = NULL)  bidirectional(values = c(TRUE, FALSE))"},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"range two-element numeric vector default lower upper bounds. trans optional transformation; NULL none. values Logical vector possible values.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"function returns dials parameter object: n_hlayers() quantitative parameter number hidden layers hidden_neurons() quantitative parameter hidden units per layer activations() qualitative parameter activation function names output_activation() qualitative parameter output activation optimizer() qualitative parameter optimizer type bias() qualitative parameter bias inclusion validation_split() quantitative parameter validation proportion bidirectional() qualitative parameter bidirectional RNN","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"architecture-strategy","dir":"Reference","previous_headings":"","what":"Architecture Strategy","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Since tidymodels tuning works independent parameters, use simplified approach : hidden_neurons specifies single value used layers activations specifies single activation used layers n_hlayers controls depth complex architectures different neurons/activations per layer, users manually specify tuning use custom tuning logic.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"n_hlayers Number hidden layers network. hidden_neurons Number units per hidden layer (applied layers). activation Single activation function applied hidden layers. output_activation Activation function output layer. optimizer Optimizer algorithm. bias Whether include bias terms layers. validation_split Proportion training data held validation. bidirectional Whether RNN layers bidirectional.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"number-of-hidden-layers","dir":"Reference","previous_headings":"","what":"Number of Hidden Layers","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Controls depth network. tuning, determine many layers created, hidden_neurons units activations function.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"hidden-units-per-layer","dir":"Reference","previous_headings":"","what":"Hidden Units per Layer","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Specifies number units per hidden layer.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"activation-function-hidden-layers-","dir":"Reference","previous_headings":"","what":"Activation Function (Hidden Layers)","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Activation functions hidden layers.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"output-activation-function","dir":"Reference","previous_headings":"","what":"Output Activation Function","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Activation function applied output layer. Values must correspond torch::nnf_* functions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"optimizer-type","dir":"Reference","previous_headings":"","what":"Optimizer Type","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"optimization algorithm used training.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"include-bias-terms","dir":"Reference","previous_headings":"","what":"Include Bias Terms","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Whether layers include bias parameters.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"validation-split-proportion","dir":"Reference","previous_headings":"","what":"Validation Split Proportion","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Fraction training data use validation set training.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"bidirectional-rnn","dir":"Reference","previous_headings":"","what":"Bidirectional RNN","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Whether recurrent layers process sequences directions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"","code":"# \\donttest{ library(dials) #> Loading required package: scales library(tune)  # Create a tuning grid grid = grid_regular(     n_hlayers(range = c(1L, 4L)),     hidden_neurons(range = c(32L, 256L)),     activations(c('relu', 'elu', 'selu')),     levels = c(4, 5, 3) )  # Use in a model spec mlp_spec = mlp_kindling(     mode = \"classification\",     hidden_neurons = tune(),     activations = tune(),     epochs = tune(),     learn_rate = tune() ) # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/extract_depth_param.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract depth parameter values from n_hlayer argument — extract_depth_param","title":"Extract depth parameter values from n_hlayer argument — extract_depth_param","text":"Extract depth parameter values n_hlayer argument","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/extract_depth_param.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract depth parameter values from n_hlayer argument — extract_depth_param","text":"","code":"extract_depth_param(n_hlayer, param_list = list(), levels = 3L)"},{"path":"https://kindling.joshuamarie.com/dev/reference/extract_depth_param.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract depth parameter values from n_hlayer argument — extract_depth_param","text":"n_hlayer Either integer vector param object param_list List parameters (extracting n_hlayers present) levels Number levels regular grids","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/extract_depth_param.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract depth parameter values from n_hlayer argument — extract_depth_param","text":"List values component containing integer vector depths","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ffnn_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"FFNN Implementation — ffnn_impl","title":"FFNN Implementation — ffnn_impl","text":"FFNN Implementation","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ffnn_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"FFNN Implementation — ffnn_impl","text":"","code":"ffnn_impl(   x,   y,   hidden_neurons,   activations = NULL,   output_activation = NULL,   bias = TRUE,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE )"},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":null,"dir":"Reference","previous_headings":"","what":"Depth-Aware Grid Generation for Neural Networks — grid_depth","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"grid_depth() extends standard grid generation support multi-layer neural network architectures. creates heterogeneous layer configurations generating list columns hidden_neurons activations.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"","code":"grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'parameters' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'list' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'workflow' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'model_spec' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'param' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # Default S3 method grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )"},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"x parameters object, list, workflow, model spec. Can also single param object ... contains additional param objects. ... One param objects (e.g., hidden_neurons(), epochs()). x parameters object, ... ignored. None objects can unknown() values. n_hlayer Integer vector specifying number hidden layers generate (e.g., 2:4 2, 3, 4 layers), param object created n_hlayers(). Default 2. size Integer. Number parameter combinations generate. type Character. Type grid: \"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\". original Logical. original parameter ranges used? levels Integer. Levels per parameter regular grids. variogram_range Numeric. Range audze_eglais design. iter Integer. Iterations max_entropy optimization.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"tibble list columns hidden_neurons activations, element vector length n_hlayer.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"function specifically {kindling} models. n_hlayer parameter determines network depth creates list columns hidden_neurons activations, element vector length matching sampled depth. n_hlayer parameter object (created n_hlayers()), treated tunable parameter sampled according defined range.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"","code":"# \\donttest{ if (FALSE) { # \\dontrun{ library(dials) library(workflows) library(tune)  # Method 1: Fixed depth grid = grid_depth(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\")),     epochs(c(50L, 200L)),     n_hlayer = 2:3,     type = \"random\",     size = 20 )  # Method 2: Tunable depth using parameter object grid = grid_depth(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\")),     epochs(c(50L, 200L)),     n_hlayer = n_hlayers(range = c(2L, 4L)),     type = \"random\",     size = 20 )  # Method 3: From workflow wf = workflow() |>     add_model(mlp_kindling(hidden_neurons = tune(), activations = tune())) |>     add_formula(y ~ .) grid = grid_depth(wf, n_hlayer = 2:4, type = \"latin_hypercube\", size = 15) } # } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":null,"dir":"Reference","previous_headings":"","what":"Base models for Neural Network Training in kindling — ffnn","title":"Base models for Neural Network Training in kindling — ffnn","text":"Base models Neural Network Training kindling","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base models for Neural Network Training in kindling — ffnn","text":"","code":"ffnn(   formula = NULL,   data = NULL,   hidden_neurons,   activations = NULL,   output_activation = NULL,   bias = TRUE,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   ...,   x = NULL,   y = NULL )  rnn(   formula = NULL,   data = NULL,   hidden_neurons,   rnn_type = \"lstm\",   activations = NULL,   output_activation = NULL,   bias = TRUE,   bidirectional = TRUE,   dropout = 0,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   ...,   x = NULL,   y = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base models for Neural Network Training in kindling — ffnn","text":"formula Formula. Model formula (e.g., y ~ x1 + x2). data Data frame. Training data. hidden_neurons Integer vector. Number neurons hidden layer. activations Activation function specifications. See act_funs(). output_activation Optional. Activation output layer. bias Logical. Use bias weights. Default TRUE. epochs Integer. Number training epochs. Default 100. batch_size Integer. Batch size training. Default 32. penalty Numeric. Regularization penalty (lambda). Default 0 (regularization). mixture Numeric. Elastic net mixing parameter (0-1). Default 0. learn_rate Numeric. Learning rate optimizer. Default 0.001. optimizer Character. Optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Default \"adam\". optimizer_args Named list. Additional arguments passed optimizer. Default list(). loss Character. Loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). Default \"mse\". validation_split Numeric. Proportion data validation (0-1). Default 0. device Character. Device use (\"cpu\", \"cuda\", \"mps\"). Default NULL (auto-detect). verbose Logical. Print training progress. Default FALSE. cache_weights Logical. Cache weight matrices faster variable importance. Default FALSE. ... Additional arguments. Can used pass x y direct interface. x using formula: predictor data (data.frame matrix). y using formula: outcome data (vector, factor, matrix). rnn_type Character. Type RNN (\"rnn\", \"lstm\", \"gru\"). Default \"lstm\". bidirectional Logical. Use bidirectional RNN. Default TRUE. dropout Numeric. Dropout rate layers. Default 0.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Base models for Neural Network Training in kindling — ffnn","text":"object class \"ffnn_fit\" containing trained model metadata.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"ffnn","dir":"Reference","previous_headings":"","what":"FFNN","title":"Base models for Neural Network Training in kindling — ffnn","text":"Train feed-forward neural network using torch package.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"rnn","dir":"Reference","previous_headings":"","what":"RNN","title":"Base models for Neural Network Training in kindling — ffnn","text":"Train recurrent neural network using torch package.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base models for Neural Network Training in kindling — ffnn","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     # Formula interface (original)     model_reg = ffnn(         Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50     )      # XY interface (new)     model_xy = ffnn(         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50,         x = iris[, 2:4],         y = iris$Sepal.Length     ) } # }  # \\donttest{ if (torch::torch_is_installed()) {     # Formula interface (original)     model_rnn = rnn(         Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         rnn_type = \"lstm\",         activations = \"relu\",         epochs = 50     )      # XY interface (new)     model_xy = rnn(         hidden_neurons = c(64, 32),         rnn_type = \"gru\",         epochs = 50,         x = iris[, 2:4],         y = iris$Sepal.Length     ) } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":null,"dir":"Reference","previous_headings":"","what":"Basemodels-tidymodels wrappers — ffnn_wrapper","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"Basemodels-tidymodels wrappers","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"","code":"ffnn_wrapper(formula, data, ...)  rnn_wrapper(formula, data, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"formula formula specifying model (e.g., y ~ x1 + x2) data data frame containing training data ... Additional arguments passed underlying training function","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"ffnn-mlp-wrapper-for-tidymodels-interface","dir":"Reference","previous_headings":"","what":"FFNN (MLP) Wrapper for {tidymodels} interface","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"function interface {tidymodels} (use , use kindling::ffnn() instead).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"rnn-wrapper-for-tidymodels-interface","dir":"Reference","previous_headings":"","what":"RNN Wrapper for {tidymodels} interface","title":"Basemodels-tidymodels wrappers — ffnn_wrapper","text":"function interface {tidymodels} (use , use kindling::rnn() instead).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable Importance Methods for kindling Models — kindling-varimp","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"file implements methods variable importance generics NeuralNetTools vip packages.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"","code":"# S3 method for class 'ffnn_fit' garson(mod_in, bar_plot = FALSE, ...)  # S3 method for class 'ffnn_fit' olden(mod_in, bar_plot = TRUE, ...)  # S3 method for class 'ffnn_fit' vi_model(object, type = c(\"olden\", \"garson\"), ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"mod_in fitted model object class \"ffnn_fit\". bar_plot Logical. Whether plot variable importance (default TRUE). ... Additional arguments passed methods. object fitted model object class \"ffnn_fit\". type Type algorithm extract variable importance. must one strings: 'olden' 'garson'","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"data frame \"garson\" \"olden\" classes columns: x_names Character vector predictor variable names y_names Character string response variable name rel_imp Numeric vector relative importance scores (percentage) data frame sorted importance descending order. tibble columns \"Variable\" \"Importance\" (via vip::vi() / vip::vi_model() ).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"garson-s-algorithm-for-ffnn-models","dir":"Reference","previous_headings":"","what":"Garson's Algorithm for FFNN Models","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"{kindling} inherits NeuralNetTools::garson extract variable importance fitted ffnn() model.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"olden-s-algorithm-for-ffnn-models","dir":"Reference","previous_headings":"","what":"Olden's Algorithm for FFNN Models","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"{kindling} inherits NeuralNetTools::olden extract variable importance fitted ffnn() model.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"variable-importance-via-vip-package","dir":"Reference","previous_headings":"","what":"Variable Importance via {vip} Package","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"can directly use vip::vi() vip::vi_model() extract variable importance fitted ffnn() model.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"Beck, M.W. 2018. NeuralNetTools: Visualization Analysis Tools Neural Networks. Journal Statistical Software. 85(11):1-20. Garson, G.D. 1991. Interpreting neural network connection weights. Artificial Intelligence Expert. 6(4):46-51. Goh, .T.C. 1995. Back-propagation neural networks modeling complex systems. Artificial Intelligence Engineering. 9(3):143-151. Olden, J.D., Jackson, D.. 2002. Illuminating 'black-box': randomization approach understanding variable contributions artificial neural networks. Ecological Modelling. 154:135-150. Olden, J.D., Joy, M.K., Death, R.G. 2004. accurate comparison methods quantifying variable importance artificial neural networks using simulated data. Ecological Modelling. 178:389-397.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     model_mlp = ffnn(         Species ~ .,         data = iris,         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 100,         verbose = FALSE,         cache_weights = TRUE     )          # Directly use `NeuralNetTools::garson`     model_mlp |>         garson()          # Directly use `NeuralNetTools::olden`         model_mlp |>         olden() } else {     message(\"Torch not fully installed — skipping example\") }  #>        x_names y_names    rel_imp #> 1 Petal.Length       y -0.1609266 #> 2 Sepal.Length       y  0.1041763 #> 3  Petal.Width       y -0.0874111 #> 4  Sepal.Width       y  0.0317807 # }  # \\donttest{ # kindling also supports `vip::vi()` / `vip::vi_model()` if (torch::torch_is_installed()) {     model_mlp = ffnn(         Species ~ .,         data = iris,         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 100,         verbose = FALSE,         cache_weights = TRUE     )      model_mlp |>         vip::vi(type = 'garson') |>         vip::vip() } else {     message(\"Torch not fully installed — skipping example\") }  # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"{kindling} enables R users build train deep neural networks : Deep Neural Networks / (Deep) Feedforward Neural Networks (DNN / FFNN) Recurrent Neural Networks (RNN) designed reduce boilerplate {torch} code FFNN RNN. also integrate seamlessly {tidymodels} components like {parsnip}, {recipes}, {workflows}, allowing flexibility consistent interface model specification, training, evaluation. Thus, package supports hyperparameter tuning : Number hidden layers Number units per layer Choice activation functions Note: hyperparameter tuning support currently implemented.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"{kindling} package provides unified, high-level interface bridges {torch} {tidymodels} ecosystems, making easy define, train, tune deep learning models using familiar tidymodels workflow.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"how-to-use","dir":"Reference","previous_headings":"","what":"How to use","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"following uses package 3 levels: Level 1: Code generation   Level 2: Direct Execution   Level 3: Conventional tidymodels interface","code":"ffnn_generator(     nn_name = \"MyFFNN\",     hd_neurons = c(64, 32, 16),     no_x = 10,     no_y = 1,     activations = 'relu' ) ffnn(     Species ~ .,     data = iris,     hidden_neurons = c(128, 64, 32),     activations = 'relu',     loss = \"cross_entropy\",     epochs = 100 ) # library(parsnip) # library(kindling) box::use(    kindling[mlp_kindling, rnn_kindling, act_funs, args],    parsnip[fit, augment],    yardstick[metrics],    mlbench[Ionosphere] # data(Ionosphere, package = \"mlbench\") )  # Remove V2 as it's all zeros ionosphere_data = Ionosphere[, -2]  # MLP example mlp_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, softshrink = args(lambd = 0.5)),     epochs = 100 ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.989 #> 2 kap      binary         0.975  # RNN example (toy usage on non-sequential data) rnn_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, elu),     epochs = 100,     rnn_type = \"gru\" ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.641 #> 2 kap      binary         0"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"main-features","dir":"Reference","previous_headings":"","what":"Main Features","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"Code generation {torch} expression Multiple architectures available: feedforward networks (MLP/DNN/FFNN) recurrent variants (RNN, LSTM, GRU) Native support {tidymodels} workflows pipelines Fine-grained control network depth, layer sizes, activation functions GPU acceleration supports via {torch} tensors","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"license","dir":"Reference","previous_headings":"","what":"License","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"MIT + file LICENSE","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"Falbel D, Luraschi J (2023). torch: Tensors Neural Networks 'GPU' Acceleration. R package version 0.13.0, https://torch.mlverse.org, https://github.com/mlverse/torch. Wickham H (2019). Advanced R, 2nd edition. Chapman Hall/CRC. ISBN 978-0815384571, https://adv-r.hadley.nz/. Goodfellow , Bengio Y, Courville (2016). Deep Learning. MIT Press. https://www.deeplearningbook.org/.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"Maintainer: Joshua Marie joshua.marie.k@gmail.com","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/make_kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Register kindling engines with parsnip — make_kindling","title":"Register kindling engines with parsnip — make_kindling","text":"function registers kindling engine MLP RNN models parsnip. called package loaded.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/make_kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register kindling engines with parsnip — make_kindling","text":"","code":"make_kindling()"},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"mlp_kindling() defines feedforward neural network model can used classification regression. integrates tidymodels ecosystem uses torch backend via kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"","code":"mlp_kindling(   mode = \"unknown\",   engine = \"kindling\",   hidden_neurons = NULL,   activations = NULL,   output_activation = NULL,   bias = NULL,   epochs = NULL,   batch_size = NULL,   penalty = NULL,   mixture = NULL,   learn_rate = NULL,   optimizer = NULL,   optimizer_args = NULL,   loss = NULL,   validation_split = NULL,   device = NULL,   verbose = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"mode single character string type model. Possible values \"unknown\", \"regression\", \"classification\". engine single character string specifying computational engine use fitting. Currently \"kindling\" supported. hidden_neurons integer vector number units hidden layer. Can tuned. activations character vector activation function names hidden layer (e.g., \"relu\", \"tanh\", \"sigmoid\"). Can tuned. output_activation character string output activation function. Can tuned. bias Logical whether include bias terms. Can tuned. epochs integer number training iterations. Can tuned. batch_size integer batch size training. Can tuned. penalty number regularization penalty (lambda). Default 0 (regularization). Higher values increase regularization strength. Can tuned. mixture number 0 1 elastic net mixing parameter. Default 0 (pure L2/Ridge regularization). 0: Pure L2 regularization (Ridge) 1: Pure L1 regularization (Lasso) 0 < mixture < 1: Elastic net (combination L1 L2) relevant penalty > 0. Can tuned. learn_rate number learning rate. Can tuned. optimizer character string optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Can tuned. optimizer_args named list additional arguments passed optimizer. tuned. loss character string loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). tuned. validation_split number 0 1 proportion data used validation. Can tuned. device character string device use (\"cpu\", \"cuda\", \"mps\"). NULL, auto-detects available GPU. tuned. verbose Logical whether print training progress. Default FALSE. tuned.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"model specification object class mlp_kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"function creates model specification feedforward neural network can used within tidymodels workflows. model supports: Multiple hidden layers configurable units Various activation functions per layer GPU acceleration (CUDA, MPS, CPU) Hyperparameter tuning integration regression classification tasks hidden_neurons parameter accepts integer vector element represents number neurons hidden layer. example, hidden_neurons = c(128, 64, 32) creates network three hidden layers. device parameter controls computation occurs: NULL (default): Auto-detect best available device (CUDA > MPS > CPU) \"cuda\": Use NVIDIA GPU \"mps\": Use Apple Silicon GPU \"cpu\": Use CPU tuning, can use special tune tokens: hidden_neurons: use tune(\"hidden_neurons\") custom range activation: use tune(\"activation\") values like \"relu\", \"tanh\"","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     box::use(         recipes[recipe],         workflows[workflow, add_recipe, add_model],         tune[tune],         parsnip[fit]     )      # Model specs     mlp_spec = mlp_kindling(         mode = \"classification\",         hidden_neurons = c(128, 64, 32),         activation = c(\"relu\", \"relu\", \"relu\"),         epochs = 100     )      # If you want to tune     mlp_tune_spec = mlp_kindling(         mode = \"classification\",         hidden_neurons = tune(),         activation = tune(),         epochs = tune(),         learn_rate = tune()     )      wf = workflow() |>         add_recipe(recipe(Species ~ ., data = iris)) |>         add_model(mlp_spec)       fit_wf = fit(wf, data = iris) } else {     message(\"Torch not fully installed — skipping example\") } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":null,"dir":"Reference","previous_headings":"","what":"Functions to generate nn_module (language) expression — ffnn_generator","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"Functions generate nn_module (language) expression","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"","code":"ffnn_generator(   nn_name = \"DeepFFN\",   hd_neurons,   no_x,   no_y,   activations = NULL,   output_activation = NULL,   bias = TRUE )  rnn_generator(   nn_name = \"DeepRNN\",   hd_neurons,   no_x,   no_y,   rnn_type = \"lstm\",   bias = TRUE,   activations = NULL,   output_activation = NULL,   bidirectional = TRUE,   dropout = 0,   ... )"},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"nn_name Character. Name generated RNN module class. Default \"DeepRNN\". hd_neurons Integer vector. Number neurons hidden RNN layer. no_x Integer. Number input features. no_y Integer. Number output features. activations Activation function specifications hidden layer. Can : NULL: activation functions. Character vector: e.g., c(\"relu\", \"sigmoid\"). List: e.g., act_funs(relu, elu, softshrink = args(lambd = 0.5)). activation_spec object act_funs(). length activations 1L, activation throughout architecture. output_activation Optional. Activation function output layer. format activations single activation. bias Logical. Whether use bias weights. Default TRUE rnn_type Character. Type RNN use. Must one \"rnn\", \"lstm\", \"gru\". Default \"lstm\". bidirectional Logical. Whether use bidirectional RNN layers. Default TRUE. dropout Numeric. Dropout rate RNN layers. Default 0. ... Additional arguments (currently unused).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"torch module expression representing FFNN. torch module expression representing RNN.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"generated FFNN module specified number hidden layers, layer containing specified number neurons. Activation functions can applied hidden layer specified. can used classification regression tasks. generated module properly namespaces torch functions avoid polluting global namespace. generated RNN module specified number recurrent layers, layer containing specified number hidden units. Activation functions can applied RNN layer specified. final output taken last time step passed linear layer. generated module properly namespaces torch functions avoid polluting global namespace.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"feed-forward-neural-network-module-generator","dir":"Reference","previous_headings":"","what":"Feed-Forward Neural Network Module Generator","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"ffnn_generator() function generates feed-forward neural network (FFNN) module expression torch package R. allows customization FFNN architecture, including number hidden layers, neurons, activation functions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"recurrent-neural-network-module-generator","dir":"Reference","previous_headings":"","what":"Recurrent Neural Network Module Generator","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"rnn_generator() function generates recurrent neural network (RNN) module expression torch package R. allows customization RNN architecture, including number hidden layers, neurons, RNN type, activation functions, parameters.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Functions to generate nn_module (language) expression — ffnn_generator","text":"","code":"# \\donttest{ # FFNN if (torch::torch_is_installed()) {     # Generate an MLP module with 3 hidden layers     ffnn_mod = ffnn_generator(         nn_name = \"MyFFNN\",         hd_neurons = c(64, 32, 16),         no_x = 10,         no_y = 1,         activations = 'relu'     )      # Evaluate and instantiate     model = eval(ffnn_mod)()      # More complex: With different activations     ffnn_mod2 = ffnn_generator(         nn_name = \"MyFFNN2\",         hd_neurons = c(128, 64, 32),         no_x = 20,         no_y = 5,         activations = act_funs(             relu,             selu,             sigmoid         )     )      # Even more complex: Different activations and customized argument     # for the specific activation function     ffnn_mod2 = ffnn_generator(         nn_name = \"MyFFNN2\",         hd_neurons = c(128, 64, 32),         no_x = 20,         no_y = 5,         activations = act_funs(             relu,             selu,             softshrink = args(lambd = 0.5)         )     )      # Customize output activation (softmax is useful for classification tasks)     ffnn_mod3 = ffnn_generator(         hd_neurons = c(64, 32),         no_x = 10,         no_y = 3,         activations = 'relu',         output_activation = act_funs(softmax = args(dim = 2L))     ) } else {     message(\"Torch not fully installed — skipping example\") } # }  # \\donttest{ ## RNN if (torch::torch_is_installed()) {     # Basic LSTM with 2 layers     rnn_mod = rnn_generator(         nn_name = \"MyLSTM\",         hd_neurons = c(64, 32),         no_x = 10,         no_y = 1,         rnn_type = \"lstm\",         activations = 'relu'     )      # Evaluate and instantiate     model = eval(rnn_mod)()      # GRU with different activations     rnn_mod2 = rnn_generator(         nn_name = \"MyGRU\",         hd_neurons = c(128, 64, 32),         no_x = 20,         no_y = 5,         rnn_type = \"gru\",         activations = act_funs(relu, elu, relu),         bidirectional = FALSE     )  } else {     message(\"Torch not fully installed — skipping example\") } # }  if (FALSE) { # \\dontrun{ ## Parameterized activation and dropout # (Will throw an error due to `nnf_tanh()` not being available in `{torch}`) # rnn_mod3 = rnn_generator( #     hd_neurons = c(100, 50, 25), #     no_x = 15, #     no_y = 3, #     rnn_type = \"lstm\", #     activations = act_funs( #         relu, #         leaky_relu = args(negative_slope = 0.01), #         tanh #     ), #     bidirectional = TRUE, #     dropout = 0.3 # ) } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":null,"dir":"Reference","previous_headings":"","what":"Ordinal Suffixes Generator — ordinal_gen","title":"Ordinal Suffixes Generator — ordinal_gen","text":"function originally numform::f_ordinal().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ordinal Suffixes Generator — ordinal_gen","text":"","code":"ordinal_gen(x)"},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ordinal Suffixes Generator — ordinal_gen","text":"x Vector numbers. string equivalent","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ordinal Suffixes Generator — ordinal_gen","text":"Returns string vector ordinal suffixes.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"this-is-how-you-use-it","dir":"Reference","previous_headings":"","what":"This is how you use it","title":"Ordinal Suffixes Generator — ordinal_gen","text":"Note: exported public namespace. please, refer numform::f_ordinal() instead.","code":"kindling:::ordinal_gen(1:10)"},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Ordinal Suffixes Generator — ordinal_gen","text":"Rinker, T. W. (2021). numform: publication style number plot formatter version 0.7.0. https://github.com/trinker/numform","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict-basemodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict method for kindling basemodel fits — predict-basemodel","title":"Predict method for kindling basemodel fits — predict-basemodel","text":"Predict method kindling basemodel fits","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict-basemodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict method for kindling basemodel fits — predict-basemodel","text":"","code":"# S3 method for class 'ffnn_fit' predict(object, newdata = NULL, new_data = NULL, type = \"response\", ...)  # S3 method for class 'rnn_fit' predict(object, newdata = NULL, new_data = NULL, type = \"response\", ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/predict-basemodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict method for kindling basemodel fits — predict-basemodel","text":"object object class \"ffnn_fit\" \"rnn_fit\". newdata Data frame. New data predictions. NULL, uses original training data (available). new_data Alternative newdata (consistency hardhat). type Character. Type prediction: \"response\" (default) – predicted values predicted classes \"prob\" – class probabilities (classification models) ... Currently unused.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict-basemodel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict method for kindling basemodel fits — predict-basemodel","text":"regression models: numeric vector (single output) matrix (multiple outputs) predicted values. classification models: type = \"response\": factor vector predicted class labels type = \"prob\": numeric matrix class probabilities, columns named class levels.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/prepare_kindling_args.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare arguments for kindling models — prepare_kindling_args","title":"Prepare arguments for kindling models — prepare_kindling_args","text":"Prepare arguments kindling models","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/prepare_kindling_args.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare arguments for kindling models — prepare_kindling_args","text":"","code":"prepare_kindling_args(args)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for ffnn_fit objects — print.ffnn_fit","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"Print method ffnn_fit objects","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"","code":"# S3 method for class 'ffnn_fit' print(x, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"x object class \"ffnn_fit\" ... Additional arguments (unused)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"return value, called side effects (printing model summary)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for rnn_fit objects — print.rnn_fit","title":"Print method for rnn_fit objects — print.rnn_fit","text":"Print method rnn_fit objects","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for rnn_fit objects — print.rnn_fit","text":"","code":"# S3 method for class 'rnn_fit' print(x, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for rnn_fit objects — print.rnn_fit","text":"x object class \"rnn_fit\" ... Additional arguments (unused)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for rnn_fit objects — print.rnn_fit","text":"return value, called side effects (printing model summary)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. NeuralNetTools garson, olden vip vi_model","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"RNN Implementation — rnn_impl","title":"RNN Implementation — rnn_impl","text":"RNN Implementation","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"RNN Implementation — rnn_impl","text":"","code":"rnn_impl(   x,   y,   hidden_neurons,   rnn_type = \"lstm\",   activations = NULL,   output_activation = NULL,   bias = TRUE,   bidirectional = TRUE,   dropout = 0,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE )"},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Recurrent Neural Network via kindling — rnn_kindling","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"rnn_kindling() defines recurrent neural network model can used classification regression sequential data. integrates tidymodels ecosystem uses torch backend via kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"","code":"rnn_kindling(   mode = \"unknown\",   engine = \"kindling\",   hidden_neurons = NULL,   rnn_type = NULL,   activations = NULL,   output_activation = NULL,   bias = NULL,   bidirectional = NULL,   dropout = NULL,   epochs = NULL,   batch_size = NULL,   penalty = NULL,   mixture = NULL,   learn_rate = NULL,   optimizer = NULL,   optimizer_args = NULL,   loss = NULL,   validation_split = NULL,   device = NULL,   verbose = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"mode single character string type model. Possible values \"unknown\", \"regression\", \"classification\". engine single character string specifying computational engine use fitting. Currently \"kindling\" supported. hidden_neurons integer vector number units hidden layer. Can tuned. rnn_type character string type RNN cell (\"rnn\", \"lstm\", \"gru\"). tuned. activations character vector activation function names hidden layer (e.g., \"relu\", \"tanh\", \"sigmoid\"). Can tuned. output_activation character string output activation function. Can tuned. bias Logical whether include bias terms. Can tuned. bidirectional logical indicating whether use bidirectional RNN. Can tuned. dropout number 0 1 dropout rate layers. Can tuned. epochs integer number training iterations. Can tuned. batch_size integer batch size training. Can tuned. penalty number regularization penalty (lambda). Default 0 (regularization). Higher values increase regularization strength. Can tuned. mixture number 0 1 elastic net mixing parameter. Default 0 (pure L2/Ridge regularization). 0: Pure L2 regularization (Ridge) 1: Pure L1 regularization (Lasso) 0 < mixture < 1: Elastic net (combination L1 L2) relevant penalty > 0. Can tuned. learn_rate number learning rate. Can tuned. optimizer character string optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Can tuned. optimizer_args named list additional arguments passed optimizer. tuned. loss character string loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). tuned. validation_split number 0 1 proportion data used validation. Can tuned. device character string device use (\"cpu\", \"cuda\", \"mps\"). NULL, auto-detects available GPU. tuned. verbose Logical whether print training progress. Default FALSE. tuned.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"model specification object class rnn_kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"function creates model specification recurrent neural network can used within tidymodels workflows. model supports: Multiple RNN types: basic RNN, LSTM, GRU Bidirectional processing Dropout regularization GPU acceleration (CUDA, MPS, CPU) Hyperparameter tuning integration regression classification tasks device parameter controls computation occurs: NULL (default): Auto-detect best available device (CUDA > MPS > CPU) \"cuda\": Use NVIDIA GPU \"mps\": Use Apple Silicon GPU \"cpu\": Use CPU ","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     box::use(         recipes[recipe],         workflows[workflow, add_recipe, add_model],         parsnip[fit]     )      # Model specs     rnn_spec = rnn_kindling(         mode = \"classification\",         hidden_neurons = c(64, 32),         rnn_type = \"lstm\",         activation = c(\"relu\", \"elu\"),         epochs = 100,         bidirectional = TRUE     )      wf = workflow() |>         add_recipe(recipe(Species ~ ., data = iris)) |>         add_model(rnn_spec)      fit_wf = fit(wf, data = iris)     fit_wf } else {     message(\"Torch not fully installed — skipping example\") } #> ══ Workflow [trained] ══════════════════════════════════════════════════════════ #> Preprocessor: Recipe #> Model: rnn_kindling() #>  #> ── Preprocessor ──────────────────────────────────────────────────────────────── #> 0 Recipe Steps #>  #> ── Model ─────────────────────────────────────────────────────────────────────── #> Warning: running command 'tput cols' had status 2 #> Warning: running command 'tput cols' had status 2 #>  #> ========================= Long Short-Term Memory (RNN) ========================= #>  #>  #> -- RNN Model Summary ----------------------------------------------------------- #>  #>  #> Warning: running command 'tput cols' had status 2 #> ----------------------------------------------------------------------- #>   NN Model Type           :              RNN    n_predictors :      4 #>   RNN Type                :             LSTM    n_response   :      3 #>   Bidirectional           :              Yes    reg.         :   None #>   Number of Epochs        :              100    Device       :    cpu #>   Hidden Layer Units      :           64, 32                 :        #>   Number of Hidden Layers :                2                 :        #>   Pred. Type              :   classification                 :        #> ----------------------------------------------------------------------- #>  #>  #>  #> -- Activation function --------------------------------------------------------- #>  #>  #> Warning: running command 'tput cols' had status 2 #> ------------------------------------------------- #>   1st Layer {64}    :                      relu #>   2nd Layer {32}    :                       elu #>   Output Activation :   No act function applied #> ------------------------------------------------- #>  # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/safe_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Safe sampling function — safe_sample","title":"Safe sampling function — safe_sample","text":"R's sample() quirky behavior: sample(5, 1) samples 1:5, c(5). function ensures sample actual vector provided.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/safe_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Safe sampling function — safe_sample","text":"","code":"safe_sample(x, size, replace = FALSE)"},{"path":"https://kindling.joshuamarie.com/dev/reference/safe_sample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Safe sampling function — safe_sample","text":"x Vector sample size Number samples replace Sample replacement?","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"function takes two-column data frame formats summary-like table. table can optionally split two parts, centered, given title. useful displaying summary information clean, tabular format. function also supports styling ANSI colors text formatting {cli} package column alignment options.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"","code":"table_summary(   data,   title = NULL,   l = NULL,   header = FALSE,   center_table = FALSE,   border_char = \"-\",   style = list(),   align = NULL,   ... )"},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"data data frame exactly two columns. data summarized displayed. title character string. optional title displayed table. l integer. number rows include left part split table. NULL, table split. header logical value. TRUE, column names data displayed header. center_table logical value. TRUE, table centered terminal. border_char Character used borders. Default \"\\u2500\". style list controlling visual styling table elements using ANSI formatting. Can include following components: left_col: Styling left column values. right_col: Styling right column values. border_text: Styling border. title: Styling title. sep: Separator character left right column. style component can either predefined style string (e.g., \"blue\", \"red_italic\", \"bold\") function takes context list /without value element returns styled text. align Controls alignment column values. Can specified three ways: single string: affects left column (e.g., \"left\", \"center\", \"right\"). vector two strings: affects columns order (e.g., c(\"left\", \"right\")). list named components: explicitly specifies alignment column ... Additional arguments (currently unused).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"function return value. prints formatted table console.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"","code":"# Create a sample data frame df = data.frame(     Category = c(\"A\", \"B\", \"C\", \"D\", \"E\"),     Value = c(10, 20, 30, 40, 50) )  # Display the table with a title and header table_summary(df, title = \"Sample Table\", header = TRUE) #>  #>     Sample Table      #> --------------------- #>   Category    Value #> --------------------- #>   A              10 #>   B              20 #>   C              30 #>   D              40 #>   E              50 #> ---------------------  # Split the table after the second row and center it table_summary(df, l = 2, center_table = TRUE) #> Warning: running command 'tput cols' had status 2 #> ------------------------------------------ #>   A              10    C              30 #>   B              20    D              40 #>                        E              50 #> ------------------------------------------  # Use styling and alignment table_summary(     df, header = TRUE,     style = list(         left_col = \"blue_bold\",         right_col = \"red\",         title = \"green\",         border_text = \"yellow\"     ),     align = c(\"center\", \"right\") ) #> --------------------- #>   Category    Value #> --------------------- #>      A           10 #>      B           20 #>      C           30 #>      D           40 #>      E           50 #> ---------------------  # Use custom styling with lambda functions table_summary(     df, header = TRUE,     style = list(         left_col = \\(ctx) cli::col_red(ctx), # ctx$value is another option         right_col = \\(ctx) cli::col_blue(ctx)     ),     align = list(left_col = \"left\", right_col = \"right\") ) #> --------------------- #>   Category    Value #> --------------------- #>   A              10 #>   B              20 #>   C              30 #>   D              40 #>   E              50 #> ---------------------"},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate device and get default device — validate_device","title":"Validate device and get default device — validate_device","text":"Check requested device available. auto-detect available GPU device fallback CPU.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate device and get default device — validate_device","text":"","code":"validate_device(device)"},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate device and get default device — validate_device","text":"device Character. Requested device.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate device and get default device — validate_device","text":"Character string validated device.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"new-features-development-version","dir":"Changelog","previous_headings":"","what":"New features","title":"kindling (development version)","text":"Added regularization support neural network models L1 regularization (Lasso) feature selection via mixture = 1 L2 regularization (Ridge) weight decay via mixture = 0 Elastic Net combining L1 L2 penalties via 0 < mixture < 1 Controlled via penalty (regularization strength) mixture (L1/L2 balance) parameters Follows tidymodels conventions consistency glmnet packages Tuning methods grid_depth() now fixed Parameter space number hidden layers now fixed active Initial implementation uses sample() creates bug x > 1 type != “regular” Uses tidyr::expand_grid(), purrr::cross*() Fix randomization parameter space produce NAs outside kindling‘s ’dials’ list columns n_hlayers = 1 Vignette showcase comparison similar packages","code":""},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"kindling-010","dir":"Changelog","previous_headings":"","what":"kindling 0.1.0","title":"kindling 0.1.0","text":"Initial CRAN release Higher-level interface torch package define, train, tune neural networks Support feedforward (multi-layer perceptron) recurrent networks (RNN, LSTM, GRU) Integration tidymodels ecosystem (parsnip, workflows, recipes, tuning) Variable importance plots network visualization tools","code":""}]
