[{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement joshua.marie.k@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to kindling","title":"Contributing to kindling","text":"outlines propose change kindling. detailed discussion contributing tidyverse packages, please see development contributing guide code review principles.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to kindling","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to kindling","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed). See guide create great issue advice.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to kindling","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"joshuamarie/kindling\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to kindling","text":"New code follow tidyverse style guide. can use Air apply style, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://kindling.joshuamarie.com/dev/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to kindling","text":"Please note kindling project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://kindling.joshuamarie.com/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 kindling authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting Started with kindling","text":"kindling bridges gap torch tidymodels, providing streamlined interface building, training, tuning deep learning models. vignette guide basic usage.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Getting Started with kindling","text":"can install kindling CRAN: install development version GitHub:","code":"install.packages('kindling') # install.packages(\"pak\") pak::pak(\"joshuamarie/kindling\") ## devtools::install_github(\"joshuamarie/kindling\") library(kindling) #>  #> Attaching package: 'kindling' #> The following object is masked from 'package:base': #>  #>     args"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"before-using-kindling","dir":"Articles","previous_headings":"","what":"Before using {kindling}","title":"Getting Started with kindling","text":"starting, need install LibTorch, backend PyTorch also backend torch R package:","code":"torch::install_torch()"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"four-levels-of-interaction","dir":"Articles","previous_headings":"","what":"Four Levels of Interaction","title":"Getting Started with kindling","text":"kindling offers flexibility four levels abstraction: Code Generation - Generate raw torch::nn_module code Direct Training - Train models simple function calls tidymodels Integration - Use parsnip, recipes, workflows Hyperparameter Tuning - Optimize models tune dials","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"level-1-code-generation","dir":"Articles","previous_headings":"","what":"Level 1: Code Generation","title":"Getting Started with kindling","text":"Generate PyTorch-style module code:","code":"ffnn_generator(     nn_name = \"MyNetwork\",     hd_neurons = c(64, 32),     no_x = 10,     no_y = 1,     activations = 'relu' )"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"level-2-direct-training","dir":"Articles","previous_headings":"","what":"Level 2: Direct Training","title":"Getting Started with kindling","text":"Train model one function call:","code":"model = ffnn(     Species ~ .,     data = iris,     hidden_neurons = c(10, 15, 7),     activations = act_funs(relu, elu), # c(\"relu\", \"elu\")     loss = \"cross_entropy\",     epochs = 100 )  predictions = predict(model, newdata = iris)"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"level-3-tidymodels-integration","dir":"Articles","previous_headings":"","what":"Level 3: tidymodels Integration","title":"Getting Started with kindling","text":"Work neural networks like parsnip model:","code":"box::use(     parsnip[fit, augment],     yardstick[metrics] )  nn_spec = mlp_kindling(     mode = \"classification\",     hidden_neurons = c(10, 7),     activations = act_funs(relu, softshrink = args(lambd = 0.5)),     epochs = 100 )  nn_fit = fit(nn_spec, Species ~ ., data = iris) augment(nn_fit, new_data = iris) |>      metrics(truth = Species, estimate = .pred_class)"},{"path":"https://kindling.joshuamarie.com/dev/articles/kindling.html","id":"learn-more","dir":"Articles","previous_headings":"","what":"Learn More","title":"Getting Started with kindling","text":"Visit package website: https://kindling.joshuamarie.com Report issues: https://github.com/joshuamarie/kindling/issues","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/similar-packages.html","id":"similar-packages","dir":"Articles","previous_headings":"","what":"Similar packages","title":"Similar packages and comparison","text":"packages discussed built top torch, R’s native implementation PyTorch. torch package provides low-level tensor operations neural network building blocks, requires substantial boilerplate code training. Higher-level packages like kindling, brulee, cito, luz simplify process offering different features design philosophies. kindling distinguishes unique code generation approach, versatile neural architecture support (can expanded future), three-level API design. brulee focuses production-ready statistical models, cito emphasizes explainability statistical inference, luz provides adaptable training loops. kindling different mutually exclusive : offers deep architectural control bridges gap torch code tidymodels workflows.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/articles/similar-packages.html","id":"complementary-use","dir":"Articles","previous_headings":"","what":"Complementary Use","title":"Similar packages and comparison","text":"packages aren’t mutually exclusive. can use kindling brulee production MLPs, except kindling provides RNNs. cito package need oriented model interpretation. , luz package ideal want less verbose training loops. Despite difference philosophies main usage, integrate build upon torch ecosystem, allowing switch modeling needs evolve. instance, prototyping kindling explore different network architectures much easier, well deploying models production, just like brulee, cito stakeholders need detailed explanations model predictions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"whats-so-special-about-kindling","dir":"Articles","previous_headings":"","what":"What’s so special about {kindling}","title":"Special Cases: Linear and Logistic Regression","text":"package planned make compatible machine learning task, even time series image classification cam supported. Yes, can linear regression logistic regression extra steps: heavily customized optimizer loss functions. train_nn() function (available >v0.3.x) supports { optimizer \\leftrightarrow optimizer_args } { loss }. cases, key remove hidden layers rely entirely output layer appropriate loss function recover classical model’s behavior.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Special Cases: Linear and Logistic Regression","text":"","code":"box::use(     kindling[train_nn, act_funs, args],     recipes[         recipe, step_dummy, step_normalize,         all_nominal_predictors, all_numeric_predictors     ],     rsample[initial_split, training, testing],     yardstick[metric_set, rmse, rsq, accuracy, mn_log_loss],     dplyr[mutate, select],     tibble[tibble] )"},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"linear-regression-as-a-special-case","dir":"Articles","previous_headings":"","what":"Linear Regression as a Special Case","title":"Special Cases: Linear and Logistic Regression","text":"standard linear regression model predicts continuous outcome weighted sum inputs — nonlinearity, hidden layers. neural network recovers exactly : hidden layers (hidden_neurons = integer(0) simply omit ), output activation identity (.e., activation), common loss function MSE, can choose different loss function: (loss = \"mse\"). conditions, gradient descent minimizes objective ordinary least squares, learned weights converge OLS solution given sufficient epochs small learning rate.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"data","dir":"Articles","previous_headings":"Linear Regression as a Special Case","what":"Data","title":"Special Cases: Linear and Logistic Regression","text":"use mtcars predict fuel efficiency (mpg) variables.","code":"set.seed(42) split = initial_split(mtcars, prop = 0.8) train = training(split) test = testing(split)  rec = recipe(mpg ~ ., data = train) |>     step_normalize(all_numeric_predictors())"},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"fitting-the-model","dir":"Articles","previous_headings":"Linear Regression as a Special Case","what":"Fitting the model","title":"Special Cases: Linear and Logistic Regression","text":"create hidden units, hidden_neuron parameter train_nn() considers following achieve: NULL Empty c() arguments example, empty vector c() used collapse network single linear layer inputs output. optimizer = \"rmsprop\" small learn_rate mirrors classical gradient descent OLS.","code":"lm_nn = train_nn(     mpg ~ .,     data = train,     hidden_neurons = c(),     loss = torch::nnf_l1_loss,     optimizer = \"rmsprop\",      learn_rate = 0.01,     epochs = 200,     verbose = FALSE )  lm_nn ##  ## ========================== Generalized Neural Network ========================== ##  ##  ## -- Model Summary --------------------------------------------------------------- ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ## ------------------------------------------------------------------- ##   NN Model Type           :         FFNN    n_predictors :     10 ##   Number of Epochs        :          200    n_response   :      1 ##   Hidden Layer Units      :                 reg.         :   None ##   Number of Hidden Layers :            0    Device       :    cpu ##   Pred. Type              :   regression                 :        ## ------------------------------------------------------------------- ##  ##  ##  ## -- Activation Functions -------------------------------------------------------- ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ## ------------------------------------------------- ##    Layer {}         :   No act function applied ##   Output Activation :   No act function applied ## ------------------------------------------------- ##  ##  ##  ## -- Architecture Spec ----------------------------------------------------------- ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ## -------------------------------------------------------------- ##   nn_layer        :   N/A    before_output_transform :   N/A ##   out_nn_layer    :   N/A    after_output_transform  :   N/A ##   nn_layer_args   :   N/A    last_layer_args         :   N/A ##   layer_arg_fn    :   N/A    input_transform         :   N/A ##   forward_extract :   N/A                            :       ## --------------------------------------------------------------"},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"evaluation","dir":"Articles","previous_headings":"Linear Regression as a Special Case","what":"Evaluation","title":"Special Cases: Linear and Logistic Regression","text":"","code":"preds = predict(lm_nn, newdata = test)  tibble(     truth = test$mpg,     estimate = preds ) |>     metric_set(rmse, rsq)(truth = truth, estimate = estimate) ## # A tibble: 2 × 3 ##   .metric .estimator .estimate ##   <chr>   <chr>          <dbl> ## 1 rmse    standard       4.43  ## 2 rsq     standard       0.944"},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"comparison-with-lm","dir":"Articles","previous_headings":"Linear Regression as a Special Case","what":"Comparison with lm()","title":"Special Cases: Linear and Logistic Regression","text":"two models produce similar RMSE R^2 values. small gap reflects gradient descent iterative approximation, lm() solves exact OLS coefficients directly. Increasing epochs switching optimizer = \"lbfgs\" (supported) close gap .","code":"lm_fit = lm(mpg ~ ., data = train)  tibble(     truth = test$mpg,     estimate = predict(lm_fit, newdata = test) ) |>     metric_set(rmse, rsq)(truth = truth, estimate = estimate) ## # A tibble: 2 × 3 ##   .metric .estimator .estimate ##   <chr>   <chr>          <dbl> ## 1 rmse    standard       4.88  ## 2 rsq     standard       0.493"},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"logistic-regression-as-a-special-case","dir":"Articles","previous_headings":"","what":"Logistic Regression as a Special Case","title":"Special Cases: Linear and Logistic Regression","text":"Logistic regression models binary multiclass outcome passing linear combination inputs sigmoid softmax activation. neural network : hidden layers, sigmoid output binary classification (softmax multiclass), Cross-entropy (loss = \"cross_entropy\") loss function mathematically equivalent logistic regression.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"binary-logistic-regression","dir":"Articles","previous_headings":"Logistic Regression as a Special Case","what":"Binary Logistic Regression","title":"Special Cases: Linear and Logistic Regression","text":"use Sonar dataset {mlbench} distinguish rocks mines (binary outcome).","code":"data(\"Sonar\", package = \"mlbench\")  sonar = Sonar set.seed(42) split_s = initial_split(sonar, prop = 0.8, strata = Class) train_s = training(split_s) test_s = testing(split_s)  rec_s = recipe(Class ~ ., data = train_s) |>     step_normalize(all_numeric_predictors()) logit_nn = train_nn(     Class ~ .,     data = train_s,     hidden_neurons = c(),     loss = \"cross_entropy\",     optimizer = \"adam\",     learn_rate = 0.01,     epochs = 200,     verbose = FALSE )  logit_nn ##  ## ========================== Generalized Neural Network ========================== ##  ##  ## -- Model Summary --------------------------------------------------------------- ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ## ----------------------------------------------------------------------- ##   NN Model Type           :             FFNN    n_predictors :     60 ##   Number of Epochs        :              200    n_response   :      2 ##   Hidden Layer Units      :                     reg.         :   None ##   Number of Hidden Layers :                0    Device       :    cpu ##   Pred. Type              :   classification                 :        ## ----------------------------------------------------------------------- ##  ##  ##  ## -- Activation Functions -------------------------------------------------------- ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ## ------------------------------------------------- ##    Layer {}         :   No act function applied ##   Output Activation :   No act function applied ## ------------------------------------------------- ##  ##  ##  ## -- Architecture Spec ----------------------------------------------------------- ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ## -------------------------------------------------------------- ##   nn_layer        :   N/A    before_output_transform :   N/A ##   out_nn_layer    :   N/A    after_output_transform  :   N/A ##   nn_layer_args   :   N/A    last_layer_args         :   N/A ##   layer_arg_fn    :   N/A    input_transform         :   N/A ##   forward_extract :   N/A                            :       ## -------------------------------------------------------------- preds_s = predict(logit_nn, newdata = test_s, type = \"response\")  tibble(     truth = test_s$Class,     estimate = preds_s ) |>     accuracy(truth = truth, estimate = estimate) ## # A tibble: 1 × 3 ##   .metric  .estimator .estimate ##   <chr>    <chr>          <dbl> ## 1 accuracy binary         0.767"},{"path":"https://kindling.joshuamarie.com/dev/articles/special-cases.html","id":"comparison-with-glm-nnetmultinom","dir":"Articles","previous_headings":"Logistic Regression as a Special Case","what":"Comparison with glm() / nnet::multinom()","title":"Special Cases: Linear and Logistic Regression","text":", accuracy comparable two approaches. neural network version converges iteratively, match guaranteed exact, optimizing cross-entropy objective linear model.","code":"box::use(nnet[multinom])  glm_fit = glm(Class ~ ., data = train_s, family = binomial()) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred tibble(     truth = test_s$Class,     estimate = {         as.factor({             preds = predict(glm_fit, newdata = test_s, type = \"response\")             ifelse(preds < 0.5, \"M\", \"R\")         })     } ) |>     accuracy(truth = truth, estimate = estimate) ## # A tibble: 1 × 3 ##   .metric  .estimator .estimate ##   <chr>    <chr>          <dbl> ## 1 accuracy binary         0.698"},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"rationale","dir":"Articles","previous_headings":"","what":"Rationale","title":"Tuning Capabilities","text":"capable package tuning neural networks? One package’s capabilities ability fine-tune whole architecture, includes depth architecture — limited number hidden neurons, also includes number layers. Neural networks torch natively supports different activation functions different layers, thus kindling supports: number hidden layers (depth) number neurons per layer (width) activation function per layer, including parametric variants (e.g. softshrink(lambd = 0.2))","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"custom-grid-creation","dir":"Articles","previous_headings":"","what":"Custom grid creation","title":"Tuning Capabilities","text":"kindling function define grid includes depth architecture: grid_depth(), analogue function dials::grid_space_filling(), except creates \"regular\" grid. can tweak n_hlayer parameter, can define grid depth. parameter can scalar (e.g. 2), integer vector (e.g. 1:2), /using dials function called n_hlayer(). n_hlayer greater 2, certain parameters hidden_neurons activations creates list-column, contains vectors parameter grid, depending n_hlayer defined.","code":""},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Tuning Capabilities","text":"won’t stop using library() function, strongly recommend using box::use() explicitly import names namespaces want attach. ’ll use penguins dataset modeldata predict body mass (kilograms) physical measurements — straightforward regression task lets us focus tuning workflow.","code":"# library(kindling) # library(tidymodels) # library(modeldata)  box::use(     kindling[mlp_kindling, act_funs, args, hidden_neurons, activations, grid_depth],     dplyr[select, ends_with, mutate, slice_sample],     tidyr[drop_na],     rsample[initial_split, training, testing, vfold_cv],     recipes[         recipe, step_dummy, step_normalize,         all_nominal_predictors, all_numeric_predictors     ],     modeldata[penguins],     parsnip[tune, set_mode, fit, augment],     workflows[workflow, add_recipe, add_model],     dials[learn_rate],     tune[tune_grid, show_best, collect_metrics, select_best, finalize_workflow, last_fit],     yardstick[metric_set, rmse, rsq],     ggplot2[autoplot] )"},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"usage","dir":"Articles","previous_headings":"","what":"Usage","title":"Tuning Capabilities","text":"kindling provides mlp_kindling() model spec. Parameters want search marked tune(). Note n_hlayer listed — handled inside grid_depth() rather model spec directly.","code":"spec = mlp_kindling(     hidden_neurons = tune(),     activations = tune(),     epochs = 50,     learn_rate = tune() ) |>     set_mode(\"regression\")"},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"data-preparation","dir":"Articles","previous_headings":"Usage","what":"Data Preparation","title":"Tuning Capabilities","text":"sample 30 rows per species keep example lightweight, stratify splits species preserve class balance. target variable body_mass_kg, derived original body_mass_g column.","code":"penguins_clean = penguins |>     drop_na() |>     select(body_mass_g, ends_with(\"_mm\"), sex, species) |>     mutate(body_mass_kg = body_mass_g / 1000) |>     slice_sample(n = 30, by = species)  set.seed(123) split = initial_split(penguins_clean, prop = 0.8, strata = species) train = training(split) test = testing(split) folds = vfold_cv(train, v = 5, strata = body_mass_kg) ## Warning: The number of observations in each quantile is below the recommended threshold ## of 20. ## • Stratification will use 3 breaks instead. rec = recipe(body_mass_kg ~ ., data = train) |>     step_dummy(all_nominal_predictors()) |>     step_normalize(all_numeric_predictors())"},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"using-grid_depth","dir":"Articles","previous_headings":"Usage","what":"Using grid_depth()","title":"Tuning Capabilities","text":"still can use standard dials grids limitation don’t know network depth, kindling provides grid_depth(). n_hlayer argument controls depths search . Remember, accepts: scalar: n_hlayer = 2 integer vector: n_hlayer = 1:3 dials range object: n_hlayer = n_hlayer(c(1, 3)) n_hlayer > 1, hidden_neurons activations columns become list-columns, row holds vector per-layer values. constrain hidden_neurons range [16, 32] limit activations three candidates — including parametric softshrink. Latin hypercube sampling spreads 10 candidates evenly across search space compared random grid.","code":"set.seed(42) depth_grid = grid_depth(     hidden_neurons(c(16, 32)),     activations(c(\"relu\", \"elu\", \"softshrink(lambd = 0.2)\")),     learn_rate(),     n_hlayer = 1:3,     size = 10,     type = \"latin_hypercube\" )  depth_grid ## # A tibble: 10 × 3 ##    hidden_neurons activations learn_rate ##    <list>         <list>           <dbl> ##  1 <int [1]>      <chr [1]>     2.99e- 6 ##  2 <int [2]>      <chr [2]>     9.46e- 5 ##  3 <int [1]>      <chr [1]>     4.09e- 4 ##  4 <int [1]>      <chr [1]>     2.98e- 8 ##  5 <int [1]>      <chr [1]>     3.66e- 2 ##  6 <int [3]>      <chr [3]>     1.62e- 7 ##  7 <int [3]>      <chr [3]>     5.56e-10 ##  8 <int [1]>      <chr [1]>     1.06e- 9 ##  9 <int [1]>      <chr [1]>     1.40e- 5 ## 10 <int [2]>      <chr [2]>     1.59e- 3"},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"tuning","dir":"Articles","previous_headings":"Usage","what":"Tuning","title":"Tuning Capabilities","text":"happens tuning part? solution easy: parameters induced list-columns becomes something like list(c(1, 2)), internally configured argument unlisted list(c(1, 2))[[1]] (always produces 1 element).","code":"wflow = workflow() |>     add_recipe(rec) |>     add_model(spec)  tune_res = tune_grid(     wflow,     resamples = folds,     grid = depth_grid,     metrics = metric_set(rmse, rsq) )"},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"inspect","dir":"Articles","previous_headings":"Usage","what":"Inspect","title":"Tuning Capabilities","text":"Even list-columns, still normally produces output want produce. Use functions extract metrics output grid search, e.g. collect_metrics() show_best().","code":"collect_metrics(tune_res) ## # A tibble: 20 × 9 ##    hidden_neurons activations learn_rate .metric .estimator   mean     n std_err ##    <list>         <list>           <dbl> <chr>   <chr>       <dbl> <int>   <dbl> ##  1 <int [1]>      <chr [1]>     2.99e- 6 rmse    standard   4.10       5 0.0289  ##  2 <int [1]>      <chr [1]>     2.99e- 6 rsq     standard   0.203      5 0.132   ##  3 <int [2]>      <chr [2]>     9.46e- 5 rmse    standard   4.19       5 0.0501  ##  4 <int [2]>      <chr [2]>     9.46e- 5 rsq     standard   0.428      5 0.138   ##  5 <int [1]>      <chr [1]>     4.09e- 4 rmse    standard   3.86       5 0.0846  ##  6 <int [1]>      <chr [1]>     4.09e- 4 rsq     standard   0.216      5 0.0572  ##  7 <int [1]>      <chr [1]>     2.98e- 8 rmse    standard   4.13       5 0.146   ##  8 <int [1]>      <chr [1]>     2.98e- 8 rsq     standard   0.433      5 0.134   ##  9 <int [1]>      <chr [1]>     3.66e- 2 rmse    standard   0.0985     5 0.0139  ## 10 <int [1]>      <chr [1]>     3.66e- 2 rsq     standard   0.984      5 0.00505 ## 11 <int [3]>      <chr [3]>     1.62e- 7 rmse    standard   4.25       5 0.0526  ## 12 <int [3]>      <chr [3]>     1.62e- 7 rsq     standard   0.449      5 0.133   ## 13 <int [3]>      <chr [3]>     5.56e-10 rmse    standard   4.17       5 0.0214  ## 14 <int [3]>      <chr [3]>     5.56e-10 rsq     standard   0.190      5 0.101   ## 15 <int [1]>      <chr [1]>     1.06e- 9 rmse    standard   4.15       5 0.138   ## 16 <int [1]>      <chr [1]>     1.06e- 9 rsq     standard   0.520      5 0.179   ## 17 <int [1]>      <chr [1]>     1.40e- 5 rmse    standard   4.21       5 0.0716  ## 18 <int [1]>      <chr [1]>     1.40e- 5 rsq     standard   0.253      5 0.132   ## 19 <int [2]>      <chr [2]>     1.59e- 3 rmse    standard   0.833      5 0.0961  ## 20 <int [2]>      <chr [2]>     1.59e- 3 rsq     standard   0.674      5 0.0545  ## # ℹ 1 more variable: .config <chr> show_best(tune_res, metric = \"rmse\", n = 5) ## # A tibble: 5 × 9 ##   hidden_neurons activations  learn_rate .metric .estimator   mean     n std_err ##   <list>         <list>            <dbl> <chr>   <chr>       <dbl> <int>   <dbl> ## 1 <int [1]>      <chr [1]>       3.66e-2 rmse    standard   0.0985     5  0.0139 ## 2 <int [2]>      <chr [2]>       1.59e-3 rmse    standard   0.833      5  0.0961 ## 3 <int [1]>      <chr [1]>       4.09e-4 rmse    standard   3.86       5  0.0846 ## 4 <int [1]>      <chr [1]>       2.99e-6 rmse    standard   4.10       5  0.0289 ## 5 <int [1]>      <chr [1]>       2.98e-8 rmse    standard   4.13       5  0.146  ## # ℹ 1 more variable: .config <chr>"},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"finalizing-the-model","dir":"Articles","previous_headings":"","what":"Finalizing the Model","title":"Tuning Capabilities","text":"’ve identified best configuration, finalize workflow fit full training set.","code":"best_params = select_best(tune_res, metric = \"rmse\") final_wflow = wflow |>     finalize_workflow(best_params)  final_model = fit(final_wflow, data = train) final_model ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: mlp_kindling() ##  ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 2 Recipe Steps ##  ## • step_dummy() ## • step_normalize() ##  ## ── Model ─────────────────────────────────────────────────────────────────────── ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ##  ## ======================= Feedforward Neural Networks (MLP) ====================== ##  ##  ## -- FFNN Model Summary ---------------------------------------------------------- ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ## ------------------------------------------------------------------- ##   NN Model Type           :         FFNN    n_predictors :      7 ##   Number of Epochs        :           50    n_response   :      1 ##   Hidden Layer Units      :           30    reg.         :   None ##   Number of Hidden Layers :            1    Device       :    cpu ##   Pred. Type              :   regression                 :        ## ------------------------------------------------------------------- ##  ##  ##  ## -- Activation function --------------------------------------------------------- ## Warning in system(\"tput cols\", intern = TRUE): running command 'tput cols' had ## status 2 ## ------------------------------------------------- ##   1st Layer {30}    :                       elu ##   Output Activation :   No act function applied ## -------------------------------------------------"},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"evaluating-on-the-test-set","dir":"Articles","previous_headings":"Finalizing the Model","what":"Evaluating on the test set","title":"Tuning Capabilities","text":"","code":"final_model |>     augment(new_data = test) |>     metric_set(rmse, rsq)(         truth = body_mass_kg,         estimate = .pred     ) ## # A tibble: 2 × 3 ##   .metric .estimator .estimate ##   <chr>   <chr>          <dbl> ## 1 rmse    standard      0.0566 ## 2 rsq     standard      0.995"},{"path":"https://kindling.joshuamarie.com/dev/articles/tuning-capabilities.html","id":"a-note-on-parametric-activations","dir":"Articles","previous_headings":"","what":"A Note on Parametric Activations","title":"Tuning Capabilities","text":"kindling supports parametric activation functions, meaning layer’s activation can carry tunable parameter. passed string \"softshrink(lambd = 0.2)\", kindling parses constructs activation automatically. means can include directly activations() candidate list inside grid_depth() without extra setup, shown . manual (non-tuned) use, can also specify activations per layer explicitly:","code":"spec_manual = mlp_kindling(     hidden_neurons = c(50, 15),     activations = act_funs(         softshrink[lambd = 0.5],         relu     ),     epochs = 150,     learn_rate = 0.01 ) |>     set_mode(\"regression\")"},{"path":"https://kindling.joshuamarie.com/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Joshua Marie. Author, maintainer. Antoine Soetewey. Author.","code":""},{"path":"https://kindling.joshuamarie.com/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Marie, Joshua (2026). kindling: Higher-Level Interface 'torch' Package Auto-Train Neural Networks https://kindling.joshuamarie.com","code":"@Manual{,   title = {{kindling}: Higher-Level Interface of 'torch' Package to Auto-Train Neural Networks},   author = {Joshua Marie},   year = {2026},   note = {R package version 0.2.0.9000}, }"},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"package-overview","dir":"","previous_headings":"","what":"Package overview","title":"kindling • Higher-Level Interface for torch","text":"Title: Higher-Level Interface ‘torch’ Package Auto-Train Neural Networks Whether ’re generating neural network architectures expressions direct fitting/training actual models, kindling minimizes boilerplate code preserving torch. since package uses torch backend, GPU/TPU devices supported. kindling also bridges gap torch tidymodels. works seamlessly parsnip, recipes, workflows bring deep learning existing tidymodels modeling pipeline. enables streamlined interface building, training, tuning deep learning models within familiar tidymodels ecosystem.","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"main-features","dir":"","previous_headings":"Package overview","what":"Main Features","title":"kindling • Higher-Level Interface for torch","text":"Code generation torch expression Multiple architectures available Base models interface: feedforward networks (MLP/DNN/FFNN) recurrent variants (RNN, LSTM, GRU) Generalized neural network trainer sequence base models Native support titanic ML frameworks (currently supports tidymodels, mlr3 later) workflows pipelines Fine-grained control network depth, layer sizes, activation functions GPU acceleration supports via torch tensors","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"kindling • Higher-Level Interface for torch","text":"can install kindling CRAN: install development version GitHub:","code":"install.packages('kindling') # install.packages(\"pak\") pak::pak(\"joshuamarie/kindling\") ## devtools::install_github(\"joshuamarie/kindling\")"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"usage-three-levels-of-interaction","dir":"","previous_headings":"","what":"Usage: Three Levels of Interaction","title":"kindling • Higher-Level Interface for torch","text":"kindling powered R’s metaprogramming capabilities code generation. Generated torch::nn_module() expressions power training functions, turn serve engines tidymodels integration. architecture gives flexibility work whatever abstraction level suits task. starting, need install LibTorch, backend PyTorch also backend torch R package:","code":"library(kindling) #>  #> Attaching package: 'kindling' #> The following object is masked from 'package:base': #>  #>     args torch::install_torch()"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-1-code-generation-for-torchnn_module","dir":"","previous_headings":"Usage: Three Levels of Interaction","what":"Level 1: Code Generation for torch::nn_module","title":"kindling • Higher-Level Interface for torch","text":"lowest level, can generate raw torch::nn_module code maximum customization. Functions ending _generator return unevaluated expressions can inspect, modify, execute. ’s generate feedforward network specification: creates three-hidden-layer network (64 - 32 - 16 neurons) takes 10 inputs produces 1 output. hidden layer uses ReLU activation, output layer remains “untransformed”.","code":"ffnn_generator(     nn_name = \"MyFFNN\",     hd_neurons = c(64, 32, 16),     no_x = 10,     no_y = 1,     activations = 'relu' ) #> torch::nn_module(\"MyFFNN\", initialize = function ()  #> { #>     self$fc1 = torch::nn_linear(10, 64, bias = TRUE) #>     self$fc2 = torch::nn_linear(64, 32, bias = TRUE) #>     self$fc3 = torch::nn_linear(32, 16, bias = TRUE) #>     self$out = torch::nn_linear(16, 1, bias = TRUE) #> }, forward = function (x)  #> { #>     x = self$fc1(x) #>     x = torch::nnf_relu(x) #>     x = self$fc2(x) #>     x = torch::nnf_relu(x) #>     x = self$fc3(x) #>     x = torch::nnf_relu(x) #>     x = self$out(x) #>     x #> })"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-2-direct-training-interface","dir":"","previous_headings":"Usage: Three Levels of Interaction","what":"Level 2: Direct Training Interface","title":"kindling • Higher-Level Interface for torch","text":"Skip code generation train models directly data. approach handles torch boilerplate training models internally. Let’s classify iris species: Evaluate prediction predict(). predict() method extended fitted models newdata argument. Two kinds predict() usage: Without newdata predictions default parent data frame. newdata simply pass new data frame new reference.","code":"model = ffnn(     Species ~ .,     data = iris,     hidden_neurons = c(10, 15, 7),     activations = act_funs(relu, \"softshrink(lambd = 0.5)\", elu),      loss = \"cross_entropy\",     epochs = 100 )  model ======================= Feedforward Neural Networks (MLP) ======================   -- FFNN Model Summary ----------------------------------------------------------      -----------------------------------------------------------------------       NN Model Type           :             FFNN    n_predictors :      4       Number of Epochs        :              100    n_response   :      3       Hidden Layer Units      :        10, 15, 7    reg.         :   None       Number of Hidden Layers :                3    Device       :    cpu       Pred. Type              :   classification                 :            -----------------------------------------------------------------------    -- Activation function ---------------------------------------------------------                 -------------------------------------------------                  1st Layer {10}    :                      relu                  2nd Layer {15}    :   softshrink(lambd = 0.5)                  3rd Layer {7}     :                       elu                  Output Activation :   No act function applied                ------------------------------------------------- predict(model) |>     (\\(x) table(actual = iris$Species, predicted = x))() #>             predicted #> actual       setosa versicolor virginica #>   setosa         50          0         0 #>   versicolor      0         48         2 #>   virginica       0          2        48 sample_iris = dplyr::slice_sample(iris, n = 10, by = Species)  predict(model, newdata = sample_iris) |>     (\\(x) table(actual = sample_iris$Species, predicted = x))() #>             predicted #> actual       setosa versicolor virginica #>   setosa         10          0         0 #>   versicolor      0         10         0 #>   virginica       0          0        10"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"level-3-conventional-tidymodels-integration","dir":"","previous_headings":"Usage: Three Levels of Interaction","what":"Level 3: Conventional tidymodels Integration","title":"kindling • Higher-Level Interface for torch","text":"Work neural networks just like parsnip model. unlocks entire tidymodels toolkit preprocessing, cross-validation, model evaluation.","code":"# library(kindling) # library(parsnip) # library(yardstick) box::use(     kindling[mlp_kindling, rnn_kindling, act_funs, args],     parsnip[fit, augment],     yardstick[metrics],     mlbench[Ionosphere] # data(Ionosphere, package = \"mlbench\") )  ionosphere_data = Ionosphere[, -2]  # Train a feedforward network with parsnip mlp_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, softshrink = args(lambd = 0.5)),     epochs = 100 ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> # A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.989 #> 2 kap      binary         0.975  # Or try a recurrent architecture (demonstrative example with tabular data) rnn_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, elu),     epochs = 100,     rnn_type = \"gru\" ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> # A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.641 #> 2 kap      binary         0"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"hyperparameter-tuning--resampling","dir":"","previous_headings":"","what":"Hyperparameter Tuning & Resampling","title":"kindling • Higher-Level Interface for torch","text":"package integration tidymodels, supports hyperparameter tuning via tune searchable parameters. current searchable parameters kindling: Layer widths (neurons per layer) Network depth (number hidden layers) Activation function combinations Output activation Optimizer (Type optimization algorithm) Bias (choose presence absence bias term) Validation Split Proportion Bidirectional (boolean; RNN) searchable parameters outside kindling, .e. dials package learn_rate() also supported. ’s example: Resampling strategies rsample enable robust cross-validation workflows, orchestrated tune dials APIs.","code":"# library(tidymodels) box::use(     kindling[         mlp_kindling, hidden_neurons, activations, output_activation, grid_depth     ],     parsnip[fit, augment],     recipes[recipe],     workflows[workflow, add_recipe, add_model],     rsample[vfold_cv],     tune[tune_grid, tune, select_best, finalize_workflow],     dials[grid_random],     yardstick[accuracy, roc_auc, metric_set, metrics] )  mlp_tune_spec = mlp_kindling(     mode = \"classification\",     hidden_neurons = tune(),     activations = tune(),     output_activation = tune() )  iris_folds = vfold_cv(iris, v = 3) nn_wf = workflow() |>     add_recipe(recipe(Species ~ ., data = iris)) |>     add_model(mlp_tune_spec)  nn_grid_depth = grid_depth(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\")),     output_activation(c(\"sigmoid\", \"linear\")),     n_hlayer = 2,     size = 10,     type = \"latin_hypercube\" )  # This is supported but limited to 1 hidden layer only ## nn_grid = grid_random( ##     hidden_neurons(c(32L, 128L)), ##     activations(c(\"relu\", \"elu\")), ##     output_activation(c(\"sigmoid\", \"linear\")), ##     size = 10 ## )  nn_tunes = tune::tune_grid(     nn_wf,     iris_folds,     grid = nn_grid_depth     # metrics = metric_set(accuracy, roc_auc) )  best_nn = select_best(nn_tunes) final_nn = finalize_workflow(nn_wf, best_nn) # Last run: 4 - 91 (relu) - 3 (sigmoid) units final_nn_model = fit(final_nn, data = iris)  final_nn_model |>     augment(new_data = iris) |>     metrics(truth = Species, estimate = .pred_class) #> # A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy multiclass     0.667 #> 2 kap      multiclass     0.5"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"variable-importance","dir":"","previous_headings":"","what":"Variable Importance","title":"kindling • Higher-Level Interface for torch","text":"kindling integrates established variable importance methods {NeuralNetTools} vip interpret trained neural networks. Two primary algorithms available: Garson’s Algorithm Olden’s Algorithm","code":"garson(model, bar_plot = FALSE) #>        x_names y_names  rel_imp #> 1  Petal.Width       y 30.28746 #> 2 Sepal.Length       y 25.89413 #> 3 Petal.Length       y 24.97980 #> 4  Sepal.Width       y 18.83861 olden(model, bar_plot = FALSE) #>        x_names y_names     rel_imp #> 1  Petal.Width       y -0.22785314 #> 2 Sepal.Length       y  0.19276435 #> 3 Petal.Length       y -0.11285313 #> 4  Sepal.Width       y  0.05431408"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"integration-with-vip","dir":"","previous_headings":"Variable Importance","what":"Integration with {vip}","title":"kindling • Higher-Level Interface for torch","text":"users working within tidymodels ecosystem, kindling models work seamlessly vip package:  Note: Weight caching increases memory usage proportional network size. enable plan compute variable importance multiple times model.","code":"box::use(     vip[vi, vip] )  vi(model) |>      vip()"},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"kindling • Higher-Level Interface for torch","text":"Falbel D, Luraschi J (2023). torch: Tensors Neural Networks ‘GPU’ Acceleration. R package version 0.13.0, https://torch.mlverse.org, https://github.com/mlverse/torch. Wickham H (2019). Advanced R, 2nd edition. Chapman Hall/CRC. ISBN 978-0815384571, https://adv-r.hadley.nz/. Goodfellow , Bengio Y, Courville (2016). Deep Learning. MIT Press. https://www.deeplearningbook.org/.","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"kindling • Higher-Level Interface for torch","text":"MIT + file LICENSE","code":""},{"path":"https://kindling.joshuamarie.com/dev/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"kindling • Higher-Level Interface for torch","text":"Please note kindling project released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":null,"dir":"Reference","previous_headings":"","what":"Activation Functions Specification Helper — act_funs","title":"Activation Functions Specification Helper — act_funs","text":"function DSL function, kind like ggplot2::aes(), helps specify activation functions neural network layers. validates activation functions exist torch parameters match function's formal arguments.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Activation Functions Specification Helper — act_funs","text":"","code":"act_funs(...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Activation Functions Specification Helper — act_funs","text":"... Activation function specifications. Can : Bare symbols: relu, tanh Character strings (simple): \"relu\", \"tanh\" Character strings (params): \"softshrink(lambda = 0.1)\", \"rrelu(lower = 1/5, upper = 1/4)\" Named parameters: softmax = args(dim = 2L) Indexed syntax (named): softshrink[lambd = 0.2], rrelu[lower = 1/5, upper = 1/4] Indexed syntax (unnamed): softshrink[0.5], elu[0.5]","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/act_funs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Activation Functions Specification Helper — act_funs","text":"vctrs vector class \"activation_spec\" containing validated activation specifications.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":null,"dir":"Reference","previous_headings":"","what":"Activation Function Arguments Helper — args","title":"Activation Function Arguments Helper — args","text":"superseded v0.3.0 favour indexed syntax, e.g. <act_fn[param = 0]> type. Type-safe helper specify parameters activation functions. parameters must named match formal arguments corresponding {torch} activation function.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Activation Function Arguments Helper — args","text":"","code":"args(...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Activation Function Arguments Helper — args","text":"... Named arguments activation function.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/args.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Activation Function Arguments Helper — args","text":"list class \"activation_args\" containing parameters.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Tunable hyperparameters for kindling models — dials-kindling","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"parameters extend dials framework support hyperparameter tuning neural networks built {kindling} package. control network architecture, activation functions, optimization, training behavior.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"","code":"n_hlayers(range = c(1L, 2L), trans = NULL)  hidden_neurons(range = c(8L, 512L), disc_values = NULL, trans = NULL)  activations(   values = c(\"relu\", \"relu6\", \"elu\", \"selu\", \"celu\", \"leaky_relu\", \"gelu\", \"softplus\",     \"softshrink\", \"softsign\", \"tanhshrink\", \"hardtanh\", \"hardshrink\", \"hardswish\",     \"hardsigmoid\", \"silu\", \"mish\", \"logsigmoid\") )  output_activation(   values = c(\"relu\", \"elu\", \"selu\", \"softplus\", \"softmax\", \"log_softmax\", \"logsigmoid\",     \"hardtanh\", \"hardsigmoid\", \"silu\") )  optimizer(values = c(\"adam\", \"sgd\", \"rmsprop\", \"adamw\"))  bias(values = c(TRUE, FALSE))  validation_split(range = c(0, 0.5), trans = NULL)  bidirectional(values = c(TRUE, FALSE))"},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"range two-element numeric vector default lower upper bounds. trans optional transformation; NULL none. disc_values NULL (default) integer vector specific possible disc_values (e.g., c(32L, 64L, 128L, 256L)). provided, tuning restricted discrete values. range automatically derived values explicitly given. trans parameter still ignored parameter supplied. values Logical vector possible values.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"function returns dials parameter object: n_hlayers() quantitative parameter number hidden layers hidden_neurons() quantitative parameter hidden units per layer activations() qualitative parameter activation function names output_activation() qualitative parameter output activation optimizer() qualitative parameter optimizer type bias() qualitative parameter bias inclusion validation_split() quantitative parameter validation proportion bidirectional() qualitative parameter bidirectional RNN","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"architecture-strategy","dir":"Reference","previous_headings":"","what":"Architecture Strategy","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Since tidymodels tuning works independent parameters, use simplified approach : hidden_neurons specifies single value used layers activations specifies single activation used layers n_hlayers controls depth complex architectures different neurons/activations per layer, users manually specify tuning use custom tuning logic.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"n_hlayers Number hidden layers network. hidden_neurons Number units per hidden layer (applied layers). activation Single activation function applied hidden layers. output_activation Activation function output layer. optimizer Optimizer algorithm. bias Whether include bias terms layers. validation_split Proportion training data held validation. bidirectional Whether RNN layers bidirectional.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"number-of-hidden-layers","dir":"Reference","previous_headings":"","what":"Number of Hidden Layers","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Controls depth network. tuning, determine many layers created, hidden_neurons units activations function.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"hidden-units-per-layer","dir":"Reference","previous_headings":"","what":"Hidden Units per Layer","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Specifies number units per hidden layer.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"activation-function-hidden-layers-","dir":"Reference","previous_headings":"","what":"Activation Function (Hidden Layers)","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Activation functions hidden layers.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"output-activation-function","dir":"Reference","previous_headings":"","what":"Output Activation Function","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Activation function applied output layer. Values must correspond torch::nnf_* functions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"optimizer-type","dir":"Reference","previous_headings":"","what":"Optimizer Type","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"optimization algorithm used training.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"include-bias-terms","dir":"Reference","previous_headings":"","what":"Include Bias Terms","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Whether layers include bias parameters.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"validation-split-proportion","dir":"Reference","previous_headings":"","what":"Validation Split Proportion","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Fraction training data use validation set training.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"bidirectional-rnn","dir":"Reference","previous_headings":"","what":"Bidirectional RNN","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"Whether recurrent layers process sequences directions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dials-kindling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tunable hyperparameters for kindling models — dials-kindling","text":"","code":"# \\donttest{ library(dials) #> Loading required package: scales library(tune)  # Create a tuning grid grid = grid_regular(     n_hlayers(range = c(1L, 4L)),     hidden_neurons(range = c(32L, 256L)),     activations(c('relu', 'elu', 'selu')),     levels = c(4, 5, 3) )  # Use in a model spec mlp_spec = mlp_kindling(     mode = \"classification\",     hidden_neurons = tune(),     activations = tune(),     epochs = tune(),     learn_rate = tune() ) # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/dot-train_nn_tab_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocessing bridge for data.frame and formula methods — .train_nn_tab_impl","title":"Preprocessing bridge for data.frame and formula methods — .train_nn_tab_impl","text":"Preprocessing bridge data.frame formula methods","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/dot-train_nn_tab_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocessing bridge for data.frame and formula methods — .train_nn_tab_impl","text":"","code":".train_nn_tab_impl(   processed,   formula,   hidden_neurons,   activations,   output_activation,   bias,   arch,   early_stopping,   epochs,   batch_size,   penalty,   mixture,   learn_rate,   optimizer,   optimizer_args,   loss,   validation_split,   device,   verbose,   cache_weights )"},{"path":"https://kindling.joshuamarie.com/dev/reference/early_stop.html","id":null,"dir":"Reference","previous_headings":"","what":"Early Stopping Specification — early_stop","title":"Early Stopping Specification — early_stop","text":"early_stop() helper function supplied early_stopping arguments.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/early_stop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Early Stopping Specification — early_stop","text":"","code":"early_stop(   patience = 5L,   min_delta = 1e-04,   restore_best_weights = TRUE,   monitor = \"val_loss\" )"},{"path":"https://kindling.joshuamarie.com/dev/reference/early_stop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Early Stopping Specification — early_stop","text":"patience Integer. Epochs wait last improvement. Default 5. min_delta Numeric. Minimum improvement qualify better. Default 1e-4. restore_best_weights Logical. Restore weights best epoch. Default TRUE. monitor Character. Metric monitor. One \"val_loss\" (default) \"train_loss\".","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/early_stop.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Early Stopping Specification — early_stop","text":"object class \"early_stop_spec\".","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/eval_act_funs.html","id":null,"dir":"Reference","previous_headings":"","what":"Activation Function Specs Evaluation — eval_act_funs","title":"Activation Function Specs Evaluation — eval_act_funs","text":"Helper function act_funcs() argument.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/eval_act_funs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Activation Function Specs Evaluation — eval_act_funs","text":"","code":"eval_act_funs(activations, output_activation)"},{"path":"https://kindling.joshuamarie.com/dev/reference/eval_act_funs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Activation Function Specs Evaluation — eval_act_funs","text":"activations Quosure containing activations expression output_activation Quosure containing output_activation expression","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/eval_act_funs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Activation Function Specs Evaluation — eval_act_funs","text":"list two elements: activations output_activation","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/extract_depth_param.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract depth parameter values from n_hlayer argument — extract_depth_param","title":"Extract depth parameter values from n_hlayer argument — extract_depth_param","text":"Extract depth parameter values n_hlayer argument","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/extract_depth_param.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract depth parameter values from n_hlayer argument — extract_depth_param","text":"","code":"extract_depth_param(n_hlayer, param_list = list(), levels = 3L)"},{"path":"https://kindling.joshuamarie.com/dev/reference/extract_depth_param.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract depth parameter values from n_hlayer argument — extract_depth_param","text":"n_hlayer Either integer vector param object param_list List parameters (extracting n_hlayers present) levels Number levels regular grids","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/extract_depth_param.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract depth parameter values from n_hlayer argument — extract_depth_param","text":"List values component containing integer vector depths","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ffnn_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"FFNN Implementation — ffnn_impl","title":"FFNN Implementation — ffnn_impl","text":"FFNN Implementation","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ffnn_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"FFNN Implementation — ffnn_impl","text":"","code":"ffnn_impl(   x,   y,   hidden_neurons,   activations = NULL,   output_activation = NULL,   bias = TRUE,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE )"},{"path":"https://kindling.joshuamarie.com/dev/reference/formula_to_expr_transformer.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Formula to Expression Transformer — formula_to_expr_transformer","title":"Convert Formula to Expression Transformer — formula_to_expr_transformer","text":"Convert Formula Expression Transformer","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/formula_to_expr_transformer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Formula to Expression Transformer — formula_to_expr_transformer","text":"","code":"formula_to_expr_transformer(formula_or_fn)"},{"path":"https://kindling.joshuamarie.com/dev/reference/formula_to_expr_transformer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Formula to Expression Transformer — formula_to_expr_transformer","text":"formula_or_fn formula like ~ .[[1]] function transforms expressions","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/formula_to_expr_transformer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Formula to Expression Transformer — formula_to_expr_transformer","text":"function takes expression returns transformed expression, NULL","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/formula_to_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Formula to Function with Named Arguments — formula_to_function","title":"Formula to Function with Named Arguments — formula_to_function","text":"Formula Function Named Arguments","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/formula_to_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Formula to Function with Named Arguments — formula_to_function","text":"","code":"formula_to_function(   formula_or_fn,   default_fn = NULL,   arg_names = NULL,   alias_map = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/formula_to_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Formula to Function with Named Arguments — formula_to_function","text":"formula_or_fn formula function default_fn Default function formula_or_fn NULL arg_names Character vector formal argument names alias_map Named list mapping arg_names formula aliases (e.g., list(in_dim = \".\"))","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/formula_to_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Formula to Function with Named Arguments — formula_to_function","text":"function","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict from a trained neural network — gen-nn-predict","title":"Predict from a trained neural network — gen-nn-predict","text":"Generate predictions \"nn_fit\" object produced train_nn(). Three S3 methods registered: predict.nn_fit() — base method matrix-trained models. predict.nn_fit_tab() — extends base method tabular fits; runs new data hardhat::forge() predicting. predict.nn_fit_ds() — extends base method torch dataset fits.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict from a trained neural network — gen-nn-predict","text":"","code":"# S3 method for class 'nn_fit' predict(object, newdata = NULL, new_data = NULL, type = \"response\", ...)  # S3 method for class 'nn_fit_tab' predict(object, newdata = NULL, new_data = NULL, type = \"response\", ...)  # S3 method for class 'nn_fit_ds' predict(object, newdata = NULL, new_data = NULL, type = \"response\", ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict from a trained neural network — gen-nn-predict","text":"object fitted model object returned train_nn(). newdata New predictor data. Accepted forms depend method: predict.nn_fit(): numeric matrix coercible object. predict.nn_fit_tab(): data.frame columns used training; preprocessing applied automatically via hardhat::forge(). predict.nn_fit_ds(): torch dataset, numeric array, matrix, data.frame. NULL, cached fitted values training returned (available type = \"prob\"). new_data Legacy alias newdata. Retained compatibility. type Character. Output type: \"response\" (default): predicted class labels (factor) classification, numeric vector / matrix regression. \"prob\": numeric matrix class probabilities (classification ). ... Currently unused; reserved future extensions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict from a trained neural network — gen-nn-predict","text":"Regression: numeric vector (single output) matrix (multiple outputs). Classification, type = \"response\": factor levels matching seen training. Classification, type = \"prob\": numeric matrix one column per class, columns named class label.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":null,"dir":"Reference","previous_headings":"","what":"Generalized Neural Network Trainer — gen-nn-train","title":"Generalized Neural Network Trainer — gen-nn-train","text":"train_nn() generic function training neural networks user-defined architecture via nn_arch(). Dispatch based class x. Recommended workflow: Define architecture nn_arch() (optional). Train train_nn(). Predict predict.nn_fit(). methods delegate shared implementation core preprocessing. architecture = NULL, model falls back plain feed-forward neural network (nn_linear) architecture.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generalized Neural Network Trainer — gen-nn-train","text":"","code":"train_nn(x, ...)  # S3 method for class 'matrix' train_nn(   x,   y,   hidden_neurons = NULL,   activations = NULL,   output_activation = NULL,   bias = TRUE,   arch = NULL,   architecture = NULL,   early_stopping = NULL,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   ... )  # S3 method for class 'data.frame' train_nn(   x,   y,   hidden_neurons = NULL,   activations = NULL,   output_activation = NULL,   bias = TRUE,   arch = NULL,   architecture = NULL,   early_stopping = NULL,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   ... )  # S3 method for class 'formula' train_nn(   x,   data,   hidden_neurons = NULL,   activations = NULL,   output_activation = NULL,   bias = TRUE,   arch = NULL,   architecture = NULL,   early_stopping = NULL,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   ... )  # Default S3 method train_nn(x, ...)  # S3 method for class 'dataset' train_nn(   x,   y = NULL,   hidden_neurons = NULL,   activations = NULL,   output_activation = NULL,   bias = TRUE,   arch = NULL,   architecture = NULL,   flatten_input = NULL,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   n_classes = NULL,   ... )"},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generalized Neural Network Trainer — gen-nn-train","text":"x Dispatch based current class: matrix: used directly, preprocessing applied. data.frame: preprocessed via hardhat::mold(). y may vector / factor / matrix outcomes, formula describing outcome–predictor relationship within x. formula: combined data preprocessed via hardhat::mold(). dataset: torch dataset object; batched via torch::dataloader(). recommended interface sequence/time-series image data. ... Additional arguments passed specific methods. y Outcome data. Interpretation depends method: matrix data.frame methods: numeric vector, factor, matrix outcomes. data.frame method : alternatively formula form outcome ~ predictors, evaluated x. Ignored x formula (outcome taken formula) dataset (labels come dataset ). hidden_neurons Integer vector specifying number neurons hidden layer, e.g. c(128, 64) two hidden layers. NULL missing, hidden layers used model reduces single linear mapping inputs outputs. activations Activation function specification(s) hidden layers. See act_funs() supported values. Recycled single value given. output_activation Optional activation function output layer. Defaults NULL (activation / linear output). bias Logical. Whether include bias terms layer. Default TRUE. arch Backward-compatible alias architecture. supplied, must identical. architecture nn_arch() object specifying custom architecture. Default NULL, falls back standard feed-forward network. early_stopping early_stop() object specifying early stopping behaviour, NULL (default) disable early stopping. supplied, training halts monitored metric improve least min_delta patience consecutive epochs. Example: early_stopping = early_stop(patience = 10). epochs Positive integer. Number full passes training data. Default 100. batch_size Positive integer. Number samples per mini-batch. Default 32. penalty Non-negative numeric. L1/L2 regularization strength (lambda). Default 0 (regularization). mixture Numeric [0, 1]. Elastic net mixing parameter: 0 = pure ridge (L2), 1 = pure lasso (L1). Default 0. learn_rate Positive numeric. Step size optimizer. Default 0.001. optimizer Character. Optimizer algorithm. One \"adam\" (default), \"sgd\", \"rmsprop\". optimizer_args Named list additional arguments forwarded optimizer constructor (e.g. list(momentum = 0.9) SGD). Default list(). loss Character function. Loss function used training. Built-options: \"mse\" (default), \"mae\", \"cross_entropy\", \"bce\". classification tasks scalar label, \"cross_entropy\" set automatically. Alternatively, supply custom function formula signature function(input, target) returning scalar torch_tensor. validation_split Numeric [0, 1). Proportion training data held validation. Default 0 (validation set). device Character. Compute device: \"cpu\", \"cuda\", \"mps\". Default NULL, auto-detects best available device. verbose Logical. TRUE, prints loss (validation loss) regular intervals training. Default FALSE. cache_weights Logical. TRUE, stores copy trained weight matrices returned object $cached_weights. Default FALSE. data data frame. Required x formula. flatten_input Logical NULL (dataset method ). Controls whether batch/sample flattened 2D entering model. NULL (default) auto-selects: TRUE architecture = NULL, otherwise FALSE. n_classes Positive integer. Number output classes. Required x dataset scalar (classification) labels; ignored otherwise.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generalized Neural Network Trainer — gen-nn-train","text":"object class \"nn_fit\", one subclasses: c(\"nn_fit_tab\", \"nn_fit\") — returned data.frame formula methods c(\"nn_fit_ds\", \"nn_fit\") — returned dataset method subclasses share common structure. See Details list components.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generalized Neural Network Trainer — gen-nn-train","text":"returned \"nn_fit\" object named list following components: model — trained torch::nn_module object fitted — fitted values training data (NULL dataset fits) loss_history — numeric vector per-epoch training loss, trimmed actual epochs run (relevant early stopping active) val_loss_history — per-epoch validation loss, NULL validation_split = 0 n_epochs — number epochs actually trained stopped_epoch — epoch early stopping triggered, NA training ran completion hidden_neurons, activations, output_activation — architecture spec penalty, mixture — regularization settings feature_names, response_name — variable names (tabular methods ) no_x, no_y — number input features output nodes is_classification — logical flag y_levels, n_classes — class labels count (classification ) device — device model cached_weights — list weight matrices, NULL arch — nn_arch object used, NULL","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"supported-tasks-and-input-formats","dir":"Reference","previous_headings":"","what":"Supported tasks and input formats","title":"Generalized Neural Network Trainer — gen-nn-train","text":"train_nn() task-agnostic design (explicit task argument). Task behavior determined input interface architecture: Tabular data: use matrix, data.frame, formula methods. Time series: use dataset method per-item tensors shaped [time, features] (preferred convention) recurrent architecture via nn_arch(). Image classification: use dataset method per-item tensors shaped first layer (commonly [channels, height, width] torch::nn_conv2d). source arrays channel-last, reorder dataset via input_transform.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"matrix-method","dir":"Reference","previous_headings":"","what":"Matrix method","title":"Generalized Neural Network Trainer — gen-nn-train","text":"x supplied raw numeric matrix, preprocessing applied. Data passed directly shared train_nn_impl core.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"data-frame-method","dir":"Reference","previous_headings":"","what":"Data frame method","title":"Generalized Neural Network Trainer — gen-nn-train","text":"x data frame, y can either vector / factor / matrix outcomes, formula form outcome ~ predictors evaluated x. Preprocessing handled hardhat::mold().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"formula-method","dir":"Reference","previous_headings":"","what":"Formula method","title":"Generalized Neural Network Trainer — gen-nn-train","text":"x formula, data must supplied data frame formula evaluated. Preprocessing handled hardhat::mold().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"dataset-method-train-nn-dataset-","dir":"Reference","previous_headings":"","what":"Dataset method (train_nn.dataset())","title":"Generalized Neural Network Trainer — gen-nn-train","text":"Trains neural network directly torch dataset object. Batching lazy loading handled torch::dataloader(), making method well-suited large datasets fit entirely memory. Architecture configuration follows contract train_nn() methods via architecture = nn_arch(...) (legacy arch = ...). non-tabular inputs (time series, images), set flatten_input = FALSE preserve tensor dimensions expected recurrent convolutional layers. Labels taken second element dataset item (.e. dataset[[]][[2]]), y ignored. label scalar tensor, classification task assumed n_classes must supplied. loss automatically switched \"cross_entropy\" case. Fitted values cached returned object. Use predict.nn_fit_ds() newdata obtain predictions training.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/reference/gen-nn-train.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generalized Neural Network Trainer — gen-nn-train","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     # Matrix method — no preprocessing     model = train_nn(         x = as.matrix(iris[, 2:4]),         y = iris$Sepal.Length,         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50     )      # Data frame method — y as a vector     model = train_nn(         x = iris[, 2:4],         y = iris$Sepal.Length,         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50     )      # Data frame method — y as a formula evaluated against x     model = train_nn(         x = iris,         y = Sepal.Length ~ . - Species,         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50     )      # Formula method — outcome derived from formula     model = train_nn(         x = Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50     )      # No hidden layers — linear model     model = train_nn(         x = Sepal.Length ~ .,         data = iris[, 1:4],         epochs = 50     )      # Architecture object (nn_arch -> train_nn)     mlp_arch = nn_arch(nn_name = \"mlp_model\")     model = train_nn(         x = Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         activations = \"relu\",         architecture = mlp_arch,         epochs = 50     )      # Custom layer architecture     custom_linear = torch::nn_module(         \"CustomLinear\",         initialize = function(in_features, out_features, bias = TRUE) {             self$layer = torch::nn_linear(in_features, out_features, bias = bias)         },         forward = function(x) self$layer(x)     )     custom_arch = nn_arch(         nn_name = \"custom_linear_mlp\",         nn_layer = ~ custom_linear     )     model = train_nn(         x = Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(16, 8),         activations = \"relu\",         architecture = custom_arch,         epochs = 50     )      # With early stopping     model = train_nn(         x = Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 200,         validation_split = 0.2,         early_stopping = early_stop(patience = 10)     ) } # }  # \\donttest{ if (torch::torch_is_installed()) {     # torch dataset method — labels come from the dataset itself     iris_cls_dataset = torch::dataset(         name = \"iris_cls_dataset\",                  initialize = function(data = iris) {             self$x = torch::torch_tensor(                 as.matrix(data[, 1:4]),                 dtype = torch::torch_float32()             )             # Species is a factor; convert to integer (1-indexed -> keep as-is for cross_entropy)             self$y = torch::torch_tensor(                 as.integer(data$Species),                 dtype = torch::torch_long()             )         },                  .getitem = function(i) {             list(self$x[i, ], self$y[i])         },                  .length = function() {             self$x$size(1)         }     )()          model_nn_ds = train_nn(         x = iris_cls_dataset,         hidden_neurons = c(32, 10),         activations = \"relu\",         epochs = 80,         batch_size = 16,         learn_rate = 0.01,         n_classes = 3, # Iris dataset has only 3 species         validation_split = 0.2,         verbose = TRUE     )          pred_nn = predict(model_nn_ds, iris_cls_dataset)     class_preds = c(\"Setosa\", \"Versicolor\", \"Virginica\")[predict(model_nn_ds, iris_cls_dataset)]          # Confusion Matrix     table(actual = iris$Species, pred = class_preds) } #> → Auto-detected classification task. Using cross_entropy loss. #> ℹ Using device: cpu #> Epoch 8/80 - Loss: 0.2430 - Val Loss: 0.2618 #> Epoch 16/80 - Loss: 0.0743 - Val Loss: 0.1010 #> Epoch 24/80 - Loss: 0.0567 - Val Loss: 0.1291 #> Epoch 32/80 - Loss: 0.0785 - Val Loss: 0.0974 #> Epoch 40/80 - Loss: 0.0611 - Val Loss: 0.0987 #> Epoch 48/80 - Loss: 0.0618 - Val Loss: 0.1038 #> Epoch 56/80 - Loss: 0.0599 - Val Loss: 0.0965 #> Epoch 64/80 - Loss: 0.0570 - Val Loss: 0.0934 #> Epoch 72/80 - Loss: 0.1491 - Val Loss: 0.3945 #> Epoch 80/80 - Loss: 0.0787 - Val Loss: 0.0801 #>             pred #> actual       Setosa Versicolor Virginica #>   setosa         50          0         0 #>   versicolor      0         48         2 #>   virginica       0          2        48 # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":null,"dir":"Reference","previous_headings":"","what":"Depth-Aware Grid Generation for Neural Networks — grid_depth","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"grid_depth() extends standard grid generation support multi-layer neural network architectures. creates heterogeneous layer configurations generating list columns hidden_neurons activations.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"","code":"grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'parameters' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'list' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'workflow' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'model_spec' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # S3 method for class 'param' grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )  # Default S3 method grid_depth(   x,   ...,   n_hlayer = 2L,   size = 5L,   type = c(\"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\"),   original = TRUE,   levels = 3L,   variogram_range = 0.5,   iter = 1000L )"},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"x parameters object, list, workflow, model spec. Can also single param object ... contains additional param objects. ... One param objects (e.g., hidden_neurons(), epochs()). x parameters object, ... ignored. None objects can unknown() values. n_hlayer Integer vector specifying number hidden layers generate (e.g., 2:4 2, 3, 4 layers), param object created n_hlayers(). Default 2. size Integer. Number parameter combinations generate. type Character. Type grid: \"regular\", \"random\", \"latin_hypercube\", \"max_entropy\", \"audze_eglais\". original Logical. original parameter ranges used? levels Integer. Levels per parameter regular grids. variogram_range Numeric. Range audze_eglais design. iter Integer. Iterations max_entropy optimization.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"tibble list columns hidden_neurons activations, element vector length n_hlayer.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"function specifically {kindling} models. n_hlayer parameter determines network depth creates list columns hidden_neurons activations, element vector length matching sampled depth. n_hlayer parameter object (created n_hlayers()), treated tunable parameter sampled according defined range.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/grid_depth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Depth-Aware Grid Generation for Neural Networks — grid_depth","text":"","code":"# \\donttest{ if (FALSE) { # \\dontrun{ library(dials) library(workflows) library(tune)  # Method 1: Fixed depth grid = grid_depth(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\")),     epochs(c(50L, 200L)),     n_hlayer = 2:3,     type = \"random\",     size = 20 )  # Method 2: Tunable depth using parameter object grid = grid_depth(     hidden_neurons(c(32L, 128L)),     activations(c(\"relu\", \"elu\")),     epochs(c(50L, 200L)),     n_hlayer = n_hlayers(range = c(2L, 4L)),     type = \"random\",     size = 20 )  # Method 3: From workflow wf = workflow() |>     add_model(mlp_kindling(hidden_neurons = tune(), activations = tune())) |>     add_formula(y ~ .) grid = grid_depth(wf, n_hlayer = 2:4, type = \"latin_hypercube\", size = 15) } # } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":null,"dir":"Reference","previous_headings":"","what":"Base models for Neural Network Training in kindling — kindling-basemodels","title":"Base models for Neural Network Training in kindling — kindling-basemodels","text":"Base models Neural Network Training kindling","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base models for Neural Network Training in kindling — kindling-basemodels","text":"","code":"ffnn(   formula = NULL,   data = NULL,   hidden_neurons,   activations = NULL,   output_activation = NULL,   bias = TRUE,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   ...,   x = NULL,   y = NULL )  rnn(   formula = NULL,   data = NULL,   hidden_neurons,   rnn_type = \"lstm\",   activations = NULL,   output_activation = NULL,   bias = TRUE,   bidirectional = TRUE,   dropout = 0,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   ...,   x = NULL,   y = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base models for Neural Network Training in kindling — kindling-basemodels","text":"formula Formula. Model formula (e.g., y ~ x1 + x2). data Data frame. Training data. hidden_neurons Integer vector. Number neurons hidden layer. activations Activation function specifications. See act_funs(). output_activation Optional. Activation output layer. bias Logical. Use bias weights. Default TRUE. epochs Integer. Number training epochs. Default 100. batch_size Integer. Batch size training. Default 32. penalty Numeric. Regularization penalty (lambda). Default 0 (regularization). mixture Numeric. Elastic net mixing parameter (0-1). Default 0. learn_rate Numeric. Learning rate optimizer. Default 0.001. optimizer Character. Optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Default \"adam\". optimizer_args Named list. Additional arguments passed optimizer. Default list(). loss Character. Loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). Default \"mse\". validation_split Numeric. Proportion data validation (0-1). Default 0. device Character. Device use (\"cpu\", \"cuda\", \"mps\"). Default NULL (auto-detect). verbose Logical. Print training progress. Default FALSE. cache_weights Logical. Cache weight matrices faster variable importance. Default FALSE. ... Additional arguments. Can used pass x y direct interface. x using formula: predictor data (data.frame matrix). y using formula: outcome data (vector, factor, matrix). rnn_type Character. Type RNN (\"rnn\", \"lstm\", \"gru\"). Default \"lstm\". bidirectional Logical. Use bidirectional RNN. Default TRUE. dropout Numeric. Dropout rate layers. Default 0.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Base models for Neural Network Training in kindling — kindling-basemodels","text":"object class \"ffnn_fit\" containing trained model metadata.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"ffnn","dir":"Reference","previous_headings":"","what":"FFNN","title":"Base models for Neural Network Training in kindling — kindling-basemodels","text":"Train feed-forward neural network using torch package.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"rnn","dir":"Reference","previous_headings":"","what":"RNN","title":"Base models for Neural Network Training in kindling — kindling-basemodels","text":"Train recurrent neural network using torch package.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-basemodels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base models for Neural Network Training in kindling — kindling-basemodels","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     # Formula interface (original)     model_reg = ffnn(         Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50     )      # XY interface (new)     model_xy = ffnn(         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50,         x = iris[, 2:4],         y = iris$Sepal.Length     ) } # }  # \\donttest{ if (torch::torch_is_installed()) {     # Formula interface (original)     model_rnn = rnn(         Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         rnn_type = \"lstm\",         activations = \"relu\",         epochs = 50     )      # XY interface (new)     model_xy = rnn(         hidden_neurons = c(64, 32),         rnn_type = \"gru\",         epochs = 50,         x = iris[, 2:4],         y = iris$Sepal.Length     ) } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":null,"dir":"Reference","previous_headings":"","what":"kindling-tidymodels wrapper — train_nn_wrapper","title":"kindling-tidymodels wrapper — train_nn_wrapper","text":"kindling-tidymodels wrapper Basemodels-tidymodels wrappers","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"kindling-tidymodels wrapper — train_nn_wrapper","text":"","code":"train_nn_wrapper(formula, data, ...)  ffnn_wrapper(formula, data, ...)  rnn_wrapper(formula, data, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"kindling-tidymodels wrapper — train_nn_wrapper","text":"formula formula specifying model (e.g., y ~ x1 + x2) data data frame containing training data ... Additional arguments passed underlying training function","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"kindling-tidymodels wrapper — train_nn_wrapper","text":"train_nn_wrapper() returns \"nn_fit_tab\" object. See train_nn() details. ffnn_wrapper() returns object class \"ffnn_fit\" containing trained feedforward neural network model metadata. See ffnn() details. rnn_wrapper() returns object class \"rnn_fit\" containing trained recurrent neural network model metadata. See rnn() details.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"kindling-tidymodels wrapper — train_nn_wrapper","text":"wrapper function designed interface {tidymodels} ecosystem, particularly use tune::tune_grid() workflows. handles conversion tuning parameters (especially list-column parameters grid_depth()) format expected train_nn(). wrapper functions designed interface {tidymodels} ecosystem, particularly use tune::tune_grid() workflows. handle conversion tuning parameters (especially list-column parameters grid_depth()) format expected ffnn() rnn().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"mlp-wrapper-for-tidymodels-interface","dir":"Reference","previous_headings":"","what":"MLP Wrapper for {tidymodels} interface","title":"kindling-tidymodels wrapper — train_nn_wrapper","text":"Internal wrapper — use mlp_kindling() + fit() instead.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"ffnn-mlp-wrapper-for-tidymodels-interface","dir":"Reference","previous_headings":"","what":"FFNN (MLP) Wrapper for {tidymodels} interface","title":"kindling-tidymodels wrapper — train_nn_wrapper","text":"function interface {tidymodels} (use , use kindling::ffnn() instead).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-nn-wrappers.html","id":"rnn-wrapper-for-tidymodels-interface","dir":"Reference","previous_headings":"","what":"RNN Wrapper for {tidymodels} interface","title":"kindling-tidymodels wrapper — train_nn_wrapper","text":"Internal wrapper — use rnn_kindling() + fit() instead.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable Importance Methods for kindling Models — kindling-varimp","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"file implements methods variable importance generics NeuralNetTools vip packages.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"","code":"# S3 method for class 'ffnn_fit' garson(mod_in, bar_plot = FALSE, ...)  # S3 method for class 'ffnn_fit' olden(mod_in, bar_plot = TRUE, ...)  # S3 method for class 'ffnn_fit' vi_model(object, type = c(\"olden\", \"garson\"), ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"mod_in fitted model object class \"ffnn_fit\". bar_plot Logical. Whether plot variable importance (default TRUE). ... Additional arguments passed methods. object fitted model object class \"ffnn_fit\". type Type algorithm extract variable importance. must one strings: 'olden' 'garson'","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"data frame \"garson\" \"olden\" classes columns: x_names Character vector predictor variable names y_names Character string response variable name rel_imp Numeric vector relative importance scores (percentage) data frame sorted importance descending order. tibble columns \"Variable\" \"Importance\" (via vip::vi() / vip::vi_model() ).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"garson-s-algorithm-for-ffnn-models","dir":"Reference","previous_headings":"","what":"Garson's Algorithm for FFNN Models","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"{kindling} inherits NeuralNetTools::garson extract variable importance fitted ffnn() model.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"olden-s-algorithm-for-ffnn-models","dir":"Reference","previous_headings":"","what":"Olden's Algorithm for FFNN Models","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"{kindling} inherits NeuralNetTools::olden extract variable importance fitted ffnn() model.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"variable-importance-via-vip-package","dir":"Reference","previous_headings":"","what":"Variable Importance via {vip} Package","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"can directly use vip::vi() vip::vi_model() extract variable importance fitted ffnn() model.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"Beck, M.W. 2018. NeuralNetTools: Visualization Analysis Tools Neural Networks. Journal Statistical Software. 85(11):1-20. Garson, G.D. 1991. Interpreting neural network connection weights. Artificial Intelligence Expert. 6(4):46-51. Goh, .T.C. 1995. Back-propagation neural networks modeling complex systems. Artificial Intelligence Engineering. 9(3):143-151. Olden, J.D., Jackson, D.. 2002. Illuminating 'black-box': randomization approach understanding variable contributions artificial neural networks. Ecological Modelling. 154:135-150. Olden, J.D., Joy, M.K., Death, R.G. 2004. accurate comparison methods quantifying variable importance artificial neural networks using simulated data. Ecological Modelling. 178:389-397.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling-varimp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable Importance Methods for kindling Models — kindling-varimp","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     model_mlp = ffnn(         Species ~ .,         data = iris,         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 100,         verbose = FALSE,         cache_weights = TRUE     )          # Directly use `NeuralNetTools::garson`     model_mlp |>         garson()          # Directly use `NeuralNetTools::olden`         model_mlp |>         olden() } else {     message(\"Torch not fully installed — skipping example\") }  #>        x_names y_names     rel_imp #> 1 Sepal.Length       y -0.22932633 #> 2  Petal.Width       y  0.14513147 #> 3  Sepal.Width       y -0.11486761 #> 4 Petal.Length       y  0.05591473 # }  # \\donttest{ # kindling also supports `vip::vi()` / `vip::vi_model()` if (torch::torch_is_installed()) {     model_mlp = ffnn(         Species ~ .,         data = iris,         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 100,         verbose = FALSE,         cache_weights = TRUE     )      model_mlp |>         vip::vi(type = 'garson') |>         vip::vip() } else {     message(\"Torch not fully installed — skipping example\") }  # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"{kindling} enables R users build train deep neural networks : Deep Neural Networks / (Deep) Feedforward Neural Networks (DNN / FFNN) Recurrent Neural Networks (RNN) mainly designed generate code expressions current architectures happens reduce boilerplate {torch} code said current architectures. also integrate seamlessly titanic ML frameworks - currently {tidymodels}, enables components like {parsnip}, {recipes}, {workflows}, allowing ergonomic interface model specification, training, evaluation. Thus, package supports hyperparameter tuning : Number hidden layers Number units per layer Choice activation functions Clarification: hyperparameter tuning support supported version 0.1.0, n_hlayer() dial parameter supported. changes 0.2.0 release.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"{kindling} package provides unified, high-level interface bridges {torch} {tidymodels} ecosystems, making easy define, train, tune deep learning models using familiar tidymodels workflow.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"how-to-use","dir":"Reference","previous_headings":"","what":"How to use","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"following uses package 3 levels: Level 1: Code generation   Level 2: Direct Execution   Level 3: Conventional tidymodels interface","code":"ffnn_generator(     nn_name = \"MyFFNN\",     hd_neurons = c(64, 32, 16),     no_x = 10,     no_y = 1,     activations = 'relu' ) ffnn(     Species ~ .,     data = iris,     hidden_neurons = c(128, 64, 32),     activations = 'relu',     loss = \"cross_entropy\",     epochs = 100 ) # library(parsnip) # library(kindling) box::use(    kindling[mlp_kindling, rnn_kindling, act_funs, args],    parsnip[fit, augment],    yardstick[metrics],    mlbench[Ionosphere] # data(Ionosphere, package = \"mlbench\") )  # Remove V2 as it's all zeros ionosphere_data = Ionosphere[, -2]  # MLP example mlp_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, softshrink = args(lambd = 0.5)),     epochs = 100 ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.989 #> 2 kap      binary         0.975  # RNN example (toy usage on non-sequential data) rnn_kindling(     mode = \"classification\",     hidden_neurons = c(128, 64),     activations = act_funs(relu, elu),     epochs = 100,     rnn_type = \"gru\" ) |>     fit(Class ~ ., data = ionosphere_data) |>     augment(new_data = ionosphere_data) |>     metrics(truth = Class, estimate = .pred_class) #> A tibble: 2 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy binary         0.641 #> 2 kap      binary         0"},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"main-features","dir":"Reference","previous_headings":"","what":"Main Features","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"Code generation {torch} expression Multiple architectures available: feedforward networks (MLP/DNN/FFNN) recurrent variants (RNN, LSTM, GRU) Native support {tidymodels} workflows pipelines Fine-grained control network depth, layer sizes, activation functions GPU acceleration supports via {torch} tensors","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"license","dir":"Reference","previous_headings":"","what":"License","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"MIT + file LICENSE","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"Falbel D, Luraschi J (2023). torch: Tensors Neural Networks 'GPU' Acceleration. R package version 0.13.0, https://torch.mlverse.org, https://github.com/mlverse/torch. Wickham H (2019). Advanced R, 2nd edition. Chapman Hall/CRC. ISBN 978-0815384571, https://adv-r.hadley.nz/. Goodfellow , Bengio Y, Courville (2016). Deep Learning. MIT Press. https://www.deeplearningbook.org/.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/reference/kindling.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"{kindling}: Higher-level interface of torch package to auto-train neural networks — kindling","text":"Maintainer: Joshua Marie joshua.marie.k@gmail.com Authors: Antoine Soetewey ant.soetewey@gmail.com (ORCID)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/layer-attributes.html","id":null,"dir":"Reference","previous_headings":"","what":"","title":"","text":"\"Layer\" attributes","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/layer-attributes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"","text":"","code":"# S3 method for class 'layer_pr' x$name"},{"path":"https://kindling.joshuamarie.com/dev/reference/layer-attributes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"","text":"x .layer name following: : Layer index (1-based integer) ind: Input dimension layer : Output dimension layer is_output: Logical indicating output layer","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/layer-attributes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"","text":"pronoun, returns nothing","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/layer_prs.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer argument pronouns for formula-based specifications — layer_prs","title":"Layer argument pronouns for formula-based specifications — layer_prs","text":"pronouns provide cleaner, readable way reference layer parameters formula-based specifications nn_module_generator() related functions. work similarly rlang::.data rlang::.env.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/layer_prs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer argument pronouns for formula-based specifications — layer_prs","text":"","code":".layer  .i  .in  .out  .is_output"},{"path":"https://kindling.joshuamarie.com/dev/reference/layer_prs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Layer argument pronouns for formula-based specifications — layer_prs","text":"object class layer_pr (inherits list) length 0. object class layer_index_pr (inherits layer_pr, list) length 0. object class layer_input_pr (inherits layer_pr, list) length 0. object class layer_output_pr (inherits layer_pr, list) length 0. object class layer_is_output_pr (inherits layer_pr, list) length 0.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/layer_prs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Layer argument pronouns for formula-based specifications — layer_prs","text":"Available pronouns: .layer: Access layer parameters list-like object .: Layer index (1-based integer) .: Input dimension layer .: Output dimension layer .is_output: Logical indicating output layer pronouns can used formulas passed : layer_arg_fn parameter Custom layer configuration functions","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/layer_prs.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer argument pronouns for formula-based specifications — layer_prs","text":"","code":"# Using individual pronouns layer_arg_fn = ~ list(     input_size = .in,     hidden_size = .out,     num_layers = if (.i == 1) 2L else 1L )  # Using .layer pronoun (alternative syntax) layer_arg_fn = ~ list(     input_size = .layer$ind,     hidden_size = .layer$out,     is_first = .layer$i == 1 )"},{"path":"https://kindling.joshuamarie.com/dev/reference/make_kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Register kindling engines with parsnip — make_kindling","title":"Register kindling engines with parsnip — make_kindling","text":"function registers kindling engine MLP, RNN, train_nn models parsnip. called package loaded.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/make_kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register kindling engines with parsnip — make_kindling","text":"","code":"make_kindling()"},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"mlp_kindling() defines feedforward neural network model can used classification regression. integrates tidymodels ecosystem uses torch backend via kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"","code":"mlp_kindling(   mode = \"unknown\",   engine = \"kindling\",   hidden_neurons = NULL,   activations = NULL,   output_activation = NULL,   bias = NULL,   epochs = NULL,   batch_size = NULL,   penalty = NULL,   mixture = NULL,   learn_rate = NULL,   optimizer = NULL,   validation_split = NULL,   optimizer_args = NULL,   loss = NULL,   architecture = NULL,   flatten_input = NULL,   early_stopping = NULL,   device = NULL,   verbose = NULL,   cache_weights = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"mode single character string type model. Possible values \"unknown\", \"regression\", \"classification\". engine single character string specifying computational engine use fitting. Currently \"kindling\" supported. hidden_neurons integer vector number units hidden layer. Can tuned. activations character vector activation function names hidden layer (e.g., \"relu\", \"tanh\", \"sigmoid\"). Can tuned. output_activation character string output activation function. Can tuned. bias Logical whether include bias terms. Can tuned. epochs integer number training iterations. Can tuned. batch_size integer batch size training. Can tuned. penalty number regularization penalty (lambda). Default 0 (regularization). Higher values increase regularization strength. Can tuned. mixture number 0 1 elastic net mixing parameter. Default 0 (pure L2/Ridge regularization). 0: Pure L2 regularization (Ridge) 1: Pure L1 regularization (Lasso) 0 < mixture < 1: Elastic net (combination L1 L2) relevant penalty > 0. Can tuned. learn_rate number learning rate. Can tuned. optimizer character string optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Can tuned. validation_split number 0 1 proportion data used validation. Can tuned. optimizer_args named list additional arguments passed optimizer. tuned — pass via set_engine(). loss character string loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). tuned — pass via set_engine(). architecture nn_arch() object custom architecture. tuned — pass via set_engine(). flatten_input Logical NULL. Controls input flattening. tuned — pass via set_engine(). early_stopping early_stop() object NULL. tuned — pass via set_engine(). device character string device (\"cpu\", \"cuda\", \"mps\"). tuned — pass via set_engine(). verbose Logical whether print training progress. tuned — pass via set_engine(). cache_weights Logical. TRUE, stores trained weight matrices returned object. tuned — pass via set_engine().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"model specification object class mlp_kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"function creates model specification feedforward neural network can used within tidymodels workflows. model supports: Multiple hidden layers configurable units Various activation functions per layer GPU acceleration (CUDA, MPS, CPU) Hyperparameter tuning integration regression classification tasks Parameters tuned (architecture, flatten_input, early_stopping, device, verbose, cache_weights, optimizer_args, loss) must set via set_engine(), arguments mlp_kindling().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/mlp_kindling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-Layer Perceptron (Feedforward Neural Network) via kindling — mlp_kindling","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     box::use(         recipes[recipe],         workflows[workflow, add_recipe, add_model],         tune[tune],         parsnip[fit]     )      # library(recipes)     # library(workflows)     # library(parsnip)     # library(tune)      # Model specs     mlp_spec = mlp_kindling(         mode = \"classification\",         hidden_neurons = c(128, 64, 32),         activation = c(\"relu\", \"relu\", \"relu\"),         epochs = 100     )      # If you want to tune     mlp_tune_spec = mlp_kindling(         mode = \"classification\",         hidden_neurons = tune(),         activation = tune(),         epochs = tune(),         learn_rate = tune()     )      wf = workflow() |>         add_recipe(recipe(Species ~ ., data = iris)) |>         add_model(mlp_spec)       fit_wf = fit(wf, data = iris) } else {     message(\"Torch not fully installed — skipping example\") } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/new_act_fn.html","id":null,"dir":"Reference","previous_headings":"","what":"Custom Activation Function Constructor — new_act_fn","title":"Custom Activation Function Constructor — new_act_fn","text":"Wraps user-supplied function validated custom activation, ensuring accepts returns torch_tensor. Performs eager dry-run probe definition time errors surface early, wraps function call-time type guard safety.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/new_act_fn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Custom Activation Function Constructor — new_act_fn","text":"","code":"new_act_fn(fn, probe = TRUE, .name = \"<custom>\")"},{"path":"https://kindling.joshuamarie.com/dev/reference/new_act_fn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Custom Activation Function Constructor — new_act_fn","text":"fn function taking single tensor argument returning tensor. E.g. \\(x) torch::torch_tanh(x). probe Logical. TRUE (default), runs dry-run small dummy tensor definition time catch obvious errors early. .name string. Default \"<custom>\".","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/new_act_fn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Custom Activation Function Constructor — new_act_fn","text":"object class c(\"custom_activation\", \"parameterized_activation\"), compatible act_funs().","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_arch.html","id":null,"dir":"Reference","previous_headings":"","what":"Architecture specification for train_nn() — nn_arch","title":"Architecture specification for train_nn() — nn_arch","text":"nn_arch() defines architecture specification object consumed train_nn() via architecture (legacy arch). Conceptual workflow: Define architecture nn_arch(). Train train_nn(..., architecture = <nn_arch>). Predict predict(). Architecture fields mirror nn_module_generator() passed without additional branching logic.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_arch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Architecture specification for train_nn() — nn_arch","text":"","code":"nn_arch(   nn_name = \"nnModule\",   nn_layer = NULL,   out_nn_layer = NULL,   nn_layer_args = list(),   layer_arg_fn = NULL,   forward_extract = NULL,   before_output_transform = NULL,   after_output_transform = NULL,   last_layer_args = list(),   input_transform = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_arch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Architecture specification for train_nn() — nn_arch","text":"nn_name Character. Name generated module class. Default \"nnModule\". nn_layer Layer type. See nn_module_generator(). Default NULL (nn_linear). out_nn_layer Optional. Layer type forced last layer. Default NULL. nn_layer_args Named list. Additional arguments passed every layer constructor. Default list(). layer_arg_fn Formula function. Generates per-layer constructor arguments. Default NULL (FFNN-style: list(in_dim, out_dim, bias = bias)). forward_extract Formula function. Processes layer output forward pass. Default NULL. before_output_transform Formula function. Transforms input output layer. Default NULL. after_output_transform Formula function. Transforms output output layer. Default NULL. last_layer_args Named list formula. Extra arguments output layer . Default list(). input_transform Formula function. Transforms entire input tensor training begins. Applied full dataset tensor, per-batch. Transforms must therefore independent batch size. Safe examples: ~ .$unsqueeze(2) (RNN sequence dim), ~ .$unsqueeze(1) (CNN channel dim). Avoid transforms reshape based .$size(1) reflect full dataset size, mini-batch size.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_arch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Architecture specification for train_nn() — nn_arch","text":"object class c(\"nn_arch\", \"kindling_arch\"), implemented named list nn_module_generator() arguments \"env\" attribute capturing calling environment custom layer resolution.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_arch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Architecture specification for train_nn() — nn_arch","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     # Standard architecture object for train_nn()     std_arch = nn_arch(nn_name = \"mlp_model\")      # GRU architecture spec     gru_arch = nn_arch(         nn_name = \"GRU\",         nn_layer = \"torch::nn_gru\",         layer_arg_fn = ~ if (.is_output) {             list(.in, .out)         } else {             list(input_size = .in, hidden_size = .out, batch_first = TRUE)         },         out_nn_layer = \"torch::nn_linear\",         forward_extract = ~ .[[1]],         before_output_transform = ~ .[, .$size(2), ],         input_transform = ~ .$unsqueeze(2)     )      # Custom layer architecture (resolved from calling environment)     custom_linear = torch::nn_module(         \"CustomLinear\",         initialize = function(in_features, out_features, bias = TRUE) {             self$layer = torch::nn_linear(in_features, out_features, bias = bias)         },         forward = function(x) self$layer(x)     )     custom_arch = nn_arch(         nn_name = \"CustomMLP\",         nn_layer = ~ custom_linear     )      model = train_nn(         Sepal.Length ~ .,         data = iris[, 1:4],         hidden_neurons = c(64, 32),         activations = \"relu\",         epochs = 50,         architecture = gru_arch     ) } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":null,"dir":"Reference","previous_headings":"","what":"Functions to generate nn_module (language) expression — nn_gens","title":"Functions to generate nn_module (language) expression — nn_gens","text":"Functions generate nn_module (language) expression","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Functions to generate nn_module (language) expression — nn_gens","text":"","code":"ffnn_generator(   nn_name = \"DeepFFN\",   hd_neurons,   no_x,   no_y,   activations = NULL,   output_activation = NULL,   bias = TRUE )  rnn_generator(   nn_name = \"DeepRNN\",   hd_neurons,   no_x,   no_y,   rnn_type = \"lstm\",   bias = TRUE,   activations = NULL,   output_activation = NULL,   bidirectional = TRUE,   dropout = 0,   ... )"},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Functions to generate nn_module (language) expression — nn_gens","text":"nn_name Character. Name generated RNN module class. Default \"DeepRNN\". hd_neurons Integer vector. Number neurons hidden RNN layer. no_x Integer. Number input features. no_y Integer. Number output features. activations Activation function specifications hidden layer. Can : NULL: activation functions. Character vector: e.g., c(\"relu\", \"sigmoid\"). List: e.g., act_funs(relu, elu, softshrink = args(lambd = 0.5)). activation_spec object act_funs(). length activations 1L, activation throughout architecture. output_activation Optional. Activation function output layer. format activations single activation. bias Logical. Whether use bias weights. Default TRUE rnn_type Character. Type RNN use. Must one \"rnn\", \"lstm\", \"gru\". Default \"lstm\". bidirectional Logical. Whether use bidirectional RNN layers. Default TRUE. dropout Numeric. Dropout rate RNN layers. Default 0. ... Additional arguments (currently unused).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Functions to generate nn_module (language) expression — nn_gens","text":"torch module expression representing FFNN. torch module expression representing RNN.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Functions to generate nn_module (language) expression — nn_gens","text":"generated FFNN module specified number hidden layers, layer containing specified number neurons. Activation functions can applied hidden layer specified. can used classification regression tasks. generated module properly namespaces torch functions avoid polluting global namespace. generated RNN module specified number recurrent layers, layer containing specified number hidden units. Activation functions can applied RNN layer specified. final output taken last time step passed linear layer. generated module properly namespaces torch functions avoid polluting global namespace.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"feed-forward-neural-network-module-generator","dir":"Reference","previous_headings":"","what":"Feed-Forward Neural Network Module Generator","title":"Functions to generate nn_module (language) expression — nn_gens","text":"ffnn_generator() function generates feed-forward neural network (FFNN) module expression torch package R. allows customization FFNN architecture, including number hidden layers, neurons, activation functions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"recurrent-neural-network-module-generator","dir":"Reference","previous_headings":"","what":"Recurrent Neural Network Module Generator","title":"Functions to generate nn_module (language) expression — nn_gens","text":"rnn_generator() function generates recurrent neural network (RNN) module expression torch package R. allows customization RNN architecture, including number hidden layers, neurons, RNN type, activation functions, parameters.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_gens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Functions to generate nn_module (language) expression — nn_gens","text":"","code":"# \\donttest{ # FFNN if (torch::torch_is_installed()) {     # Generate an MLP module with 3 hidden layers     ffnn_mod = ffnn_generator(         nn_name = \"MyFFNN\",         hd_neurons = c(64, 32, 16),         no_x = 10,         no_y = 1,         activations = 'relu'     )      # Evaluate and instantiate     model = eval(ffnn_mod)()      # More complex: With different activations     ffnn_mod2 = ffnn_generator(         nn_name = \"MyFFNN2\",         hd_neurons = c(128, 64, 32),         no_x = 20,         no_y = 5,         activations = act_funs(             relu,             selu,             sigmoid         )     )      # Even more complex: Different activations and customized argument     # for the specific activation function     ffnn_mod2 = ffnn_generator(         nn_name = \"MyFFNN2\",         hd_neurons = c(128, 64, 32),         no_x = 20,         no_y = 5,         activations = act_funs(             relu,             selu,             softshrink = args(lambd = 0.5)         )     )      # Customize output activation (softmax is useful for classification tasks)     ffnn_mod3 = ffnn_generator(         hd_neurons = c(64, 32),         no_x = 10,         no_y = 3,         activations = 'relu',         output_activation = act_funs(softmax = args(dim = 2L))     ) } else {     message(\"Torch not fully installed — skipping example\") } #> Warning: `args()` was deprecated in kindling 0.3.0. #> ℹ Use indexed syntax for parametric activation functions, e.g. `<softplus[beta #>   = 0.5]>`. # }  # \\donttest{ ## RNN if (torch::torch_is_installed()) {     # Basic LSTM with 2 layers     rnn_mod = rnn_generator(         nn_name = \"MyLSTM\",         hd_neurons = c(64, 32),         no_x = 10,         no_y = 1,         rnn_type = \"lstm\",         activations = 'relu'     )      # Evaluate and instantiate     model = eval(rnn_mod)()      # GRU with different activations     rnn_mod2 = rnn_generator(         nn_name = \"MyGRU\",         hd_neurons = c(128, 64, 32),         no_x = 20,         no_y = 5,         rnn_type = \"gru\",         activations = act_funs(relu, elu, relu),         bidirectional = FALSE     )  } else {     message(\"Torch not fully installed — skipping example\") } # }  if (FALSE) { # \\dontrun{ ## Parameterized activation and dropout # (Will throw an error due to `nnf_tanh()` not being available in `{torch}`) # rnn_mod3 = rnn_generator( #     hd_neurons = c(100, 50, 25), #     no_x = 15, #     no_y = 3, #     rnn_type = \"lstm\", #     activations = act_funs( #         relu, #         leaky_relu = args(negative_slope = 0.01), #         tanh #     ), #     bidirectional = TRUE, #     dropout = 0.3 # ) } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_module_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"Generalized Neural Network Module Expression Generator — nn_module_generator","title":"Generalized Neural Network Module Expression Generator — nn_module_generator","text":"nn_module_generator() generalized function generates neural network module expressions various architectures. provides flexible framework creating custom neural network modules parameterizing layer types, construction arguments, forward pass behavior. designed primarily {torch} modules, can work custom layer implementations current environment, including user-defined layers like RBF networks, custom attention mechanisms, novel architectures. function serves foundation specialized generators like ffnn_generator() rnn_generator(), can used directly create custom architectures.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_module_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generalized Neural Network Module Expression Generator — nn_module_generator","text":"","code":"nn_module_generator(   nn_name = \"nnModule\",   nn_layer = NULL,   out_nn_layer = NULL,   nn_layer_args = list(),   layer_arg_fn = NULL,   forward_extract = NULL,   before_output_transform = NULL,   after_output_transform = NULL,   last_layer_args = list(),   hd_neurons,   no_x,   no_y,   activations = NULL,   output_activation = NULL,   bias = TRUE,   eval = FALSE,   .env = parent.frame(),   ... )"},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_module_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generalized Neural Network Module Expression Generator — nn_module_generator","text":"nn_name Character string specifying name generated neural network module class. Default \"nnModule\". nn_layer type neural network layer use. Can specified : NULL (default): Uses nn_linear() {torch} Character string: e.g., \"nn_linear\", \"nn_gru\", \"nn_lstm\", \"some_custom_layer\" Named function: function object constructs layer Anonymous function: e.g., \\() nn_linear() function() nn_linear() layer constructor first searched current environment, parent environments, finally falls back {torch} namespace. allows use custom layer implementations alongside standard torch layers. out_nn_layer Default NULL. supplied, forces neural network layer used last layer. Can specified : Character string, e.g. \"nn_linear\", \"nn_gru\", \"nn_lstm\", \"some_custom_layer\" Named function: function object constructs layer Formula interface, e.g. ~torch::nn_linear, ~some_custom_layer Internally, almost works nn_layer parameter. nn_layer_args Named list additional arguments passed layer constructor specified nn_layer. arguments applied layers. layer-specific arguments, use layer_arg_fn. Default empty list. layer_arg_fn Optional function formula generates layer-specific construction arguments. Can specified : Formula: ~ list(input_size = ., hidden_size = .) ., ., ., .is_output available Function: function(, in_dim, out_dim, is_output) signature formula/function return named list arguments pass layer constructor. Available variables formula context: .: Integer, layer index (1-based) .in_dim: Integer, input dimension layer .out_dim: Integer, output dimension layer .is_output is_output: Logical, whether final output layer NULL, defaults FFNN-style arguments: list(in_dim, out_dim, bias = bias). forward_extract Optional formula function processes layer outputs forward pass. Useful layers return complex structures (e.g., RNNs return list(output, hidden)). Can specified : Formula: ~ .[[1]] ~ .$output . represents layer output Function: function(expr) accepts/returns language object Common patterns: Extract first element: ~ .[[1]] Extract named element: ~ .$output Extract method: ~ .$get_output() NULL, layer outputs used directly. before_output_transform Optional formula function transforms input output layer. applied last hidden layer (activation) output layer. Can specified : Formula: ~ .[, .$size(2), ] . represents current tensor Function: function(expr) accepts/returns language object Common patterns: Extract last timestep: ~ .[, .$size(2), ] Flatten: ~ .$flatten(start_dim = 1) Global pooling: ~ .$mean(dim = 2) Extract token: ~ .[, 1, ] NULL, transformation applied. after_output_transform Optional formula function transforms output output layer. applied self$(x) (final layer) returning result. Can specified : Formula: ~ .$mean(dim = 2) . represents output tensor Function: function(expr) accepts/returns language object Common patterns: Global average pooling: ~ .$mean(dim = 2) Squeeze dimensions: ~ .$squeeze() Reshape output: ~ .$view(c(-1, 10)) Extract specific outputs: ~ .[, , 1:5] NULL, transformation applied. last_layer_args Optional named list formula specifying additional arguments output layer . arguments appended output layer constructor arguments layer_arg_fn. Can specified : Formula: ~ list(kernel_size = 2L, bias = FALSE) Named list: list(kernel_size = 2L, bias = FALSE) useful need override add specific parameters final layer without affecting hidden layers. example, CNNs might want different kernel size output layer, RNNs might want disable bias final linear projection. Arguments last_layer_args override conflicting arguments layer_arg_fn .is_output = TRUE. Default empty list. hd_neurons Integer vector specifying number neurons (hidden units) hidden layer. length determines number hidden layers network. Must contain least one element. no_x Integer specifying number input features (input dimension). no_y Integer specifying number output features (output dimension). activations Activation function specifications hidden layers. Can : NULL: activation functions applied Character vector: e.g., c(\"relu\", \"sigmoid\", \"tanh\") activation_spec object: Created using act_funs(), allows specifying custom arguments. See examples. single activation provided, replicated across hidden layers. Otherwise, length match number hidden layers. output_activation Optional activation function output layer. format activations, specify single activation. Common choices include \"softmax\" classification \"sigmoid\" binary outcomes. Default NULL (output activation). bias Logical indicating whether include bias terms layers. Default TRUE. Note passed layer_arg_fn provided, custom layer argument functions handle parameter appropriately. eval Logical indicating whether evaluate generated expression immediately. TRUE, returns instantiated nn_module class can called directly (e.g., model()). FALSE (default), returns unevaluated language expression can inspected evaluated later eval(). Default FALSE. .env Default parent.frame(). environment generated expression evaluated ... Additional arguments passed layer constructors future extensions.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/nn_module_generator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generalized Neural Network Module Expression Generator — nn_module_generator","text":"eval = FALSE (default): language object (unevaluated expression) representing torch::nn_module definition. expression can evaluated eval() create module class, can instantiated eval(result)() create model instance. eval = TRUE: instantiated nn_module class constructor can called directly create model instances (e.g., result()).","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":null,"dir":"Reference","previous_headings":"","what":"Ordinal Suffixes Generator — ordinal_gen","title":"Ordinal Suffixes Generator — ordinal_gen","text":"function originally numform::f_ordinal().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ordinal Suffixes Generator — ordinal_gen","text":"","code":"ordinal_gen(x)"},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ordinal Suffixes Generator — ordinal_gen","text":"x Vector numbers. string equivalent","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ordinal Suffixes Generator — ordinal_gen","text":"Returns string vector ordinal suffixes.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"this-is-how-you-use-it","dir":"Reference","previous_headings":"","what":"This is how you use it","title":"Ordinal Suffixes Generator — ordinal_gen","text":"Note: exported public namespace. please, refer numform::f_ordinal() instead.","code":"kindling:::ordinal_gen(1:10)"},{"path":"https://kindling.joshuamarie.com/dev/reference/ordinal_gen.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Ordinal Suffixes Generator — ordinal_gen","text":"Rinker, T. W. (2021). numform: publication style number plot formatter version 0.7.0. https://github.com/trinker/numform","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict-basemodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict method for kindling basemodel fits — predict-basemodel","title":"Predict method for kindling basemodel fits — predict-basemodel","text":"Predict method kindling basemodel fits","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict-basemodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict method for kindling basemodel fits — predict-basemodel","text":"","code":"# S3 method for class 'ffnn_fit' predict(object, newdata = NULL, new_data = NULL, type = \"response\", ...)  # S3 method for class 'rnn_fit' predict(object, newdata = NULL, new_data = NULL, type = \"response\", ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/predict-basemodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict method for kindling basemodel fits — predict-basemodel","text":"object object class \"ffnn_fit\" \"rnn_fit\". newdata Data frame. New data predictions. NULL, uses original training data (available). new_data Alternative newdata (consistency hardhat). type Character. Type prediction: \"response\" (default) – predicted values predicted classes \"prob\" – class probabilities (classification models) ... Currently unused.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/predict-basemodel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict method for kindling basemodel fits — predict-basemodel","text":"regression models: numeric vector (single output) matrix (multiple outputs) predicted values. classification models: type = \"response\": factor vector predicted class labels type = \"prob\": numeric matrix class probabilities, columns named class levels.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/prepare_kindling_args.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare arguments for kindling models — prepare_kindling_args","title":"Prepare arguments for kindling models — prepare_kindling_args","text":"Prepare arguments kindling models","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/prepare_kindling_args.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare arguments for kindling models — prepare_kindling_args","text":"","code":"prepare_kindling_args(args)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print-layer_pronoun.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for the pronouns — print.layer_pr","title":"Print method for the pronouns — print.layer_pr","text":"Print method pronouns","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print-layer_pronoun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for the pronouns — print.layer_pr","text":"","code":"# S3 method for class 'layer_pr' print(x, ...)  # S3 method for class 'layer_index_pr' print(x, ...)  # S3 method for class 'layer_input_pr' print(x, ...)  # S3 method for class 'layer_output_pr' print(x, ...)  # S3 method for class 'layer_is_output_pr' print(x, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print-layer_pronoun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for the pronouns — print.layer_pr","text":"x object class \"ffnn_fit\" ... Additional arguments (unused)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print-layer_pronoun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for the pronouns — print.layer_pr","text":"return value, prints type pronoun used","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print-layer_pronoun.html","id":"for-layer","dir":"Reference","previous_headings":"","what":"For .layer","title":"Print method for the pronouns — print.layer_pr","text":"displays fields accessed $.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for ffnn_fit objects — print.ffnn_fit","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"Print method ffnn_fit objects","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"","code":"# S3 method for class 'ffnn_fit' print(x, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"x object class \"ffnn_fit\" ... Additional arguments (unused)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.ffnn_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for ffnn_fit objects — print.ffnn_fit","text":"return value, called side effects (printing model summary)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.nn_arch.html","id":null,"dir":"Reference","previous_headings":"","what":"Display nn_arch() configuration — print.nn_arch","title":"Display nn_arch() configuration — print.nn_arch","text":"Display nn_arch() configuration","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.nn_arch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Display nn_arch() configuration — print.nn_arch","text":"","code":"# S3 method for class 'nn_arch' print(x, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.nn_arch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Display nn_arch() configuration — print.nn_arch","text":"x object class \"nn_arch\" ... Additional arguments (unused)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.nn_arch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Display nn_arch() configuration — print.nn_arch","text":"return value, just information","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.nn_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for nn_fit objects — print.nn_fit","title":"Print method for nn_fit objects — print.nn_fit","text":"Print method nn_fit objects","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.nn_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for nn_fit objects — print.nn_fit","text":"","code":"# S3 method for class 'nn_fit' print(x, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.nn_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for nn_fit objects — print.nn_fit","text":"x object class \"nn_fit\" ... Additional arguments (unused)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.nn_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for nn_fit objects — print.nn_fit","text":"return value, called side effects (printing model summary)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for rnn_fit objects — print.rnn_fit","title":"Print method for rnn_fit objects — print.rnn_fit","text":"Print method rnn_fit objects","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for rnn_fit objects — print.rnn_fit","text":"","code":"# S3 method for class 'rnn_fit' print(x, ...)"},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for rnn_fit objects — print.rnn_fit","text":"x object class \"rnn_fit\" ... Additional arguments (unused)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/print.rnn_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for rnn_fit objects — print.rnn_fit","text":"return value, called side effects (printing model summary)","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. NeuralNetTools garson, olden vip vi_model","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"RNN Implementation — rnn_impl","title":"RNN Implementation — rnn_impl","text":"RNN Implementation","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"RNN Implementation — rnn_impl","text":"","code":"rnn_impl(   x,   y,   hidden_neurons,   rnn_type = \"lstm\",   activations = NULL,   output_activation = NULL,   bias = TRUE,   bidirectional = TRUE,   dropout = 0,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE )"},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":null,"dir":"Reference","previous_headings":"","what":"Recurrent Neural Network via kindling — rnn_kindling","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"rnn_kindling() defines recurrent neural network model can used classification regression sequential data. integrates tidymodels ecosystem uses torch backend via kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"","code":"rnn_kindling(   mode = \"unknown\",   engine = \"kindling\",   hidden_neurons = NULL,   activations = NULL,   output_activation = NULL,   bias = NULL,   bidirectional = NULL,   dropout = NULL,   epochs = NULL,   batch_size = NULL,   penalty = NULL,   mixture = NULL,   learn_rate = NULL,   optimizer = NULL,   validation_split = NULL,   rnn_type = NULL,   optimizer_args = NULL,   loss = NULL,   early_stopping = NULL,   device = NULL,   verbose = NULL,   cache_weights = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"mode single character string type model. Possible values \"unknown\", \"regression\", \"classification\". engine single character string specifying computational engine use fitting. Currently \"kindling\" supported. hidden_neurons integer vector number units hidden layer. Can tuned. activations character vector activation function names hidden layer (e.g., \"relu\", \"tanh\", \"sigmoid\"). Can tuned. output_activation character string output activation function. Can tuned. bias Logical whether include bias terms. Can tuned. bidirectional logical indicating whether use bidirectional RNN. Can tuned. dropout number 0 1 dropout rate layers. Can tuned. epochs integer number training iterations. Can tuned. batch_size integer batch size training. Can tuned. penalty number regularization penalty (lambda). Default 0 (regularization). Higher values increase regularization strength. Can tuned. mixture number 0 1 elastic net mixing parameter. Default 0 (pure L2/Ridge regularization). 0: Pure L2 regularization (Ridge) 1: Pure L1 regularization (Lasso) 0 < mixture < 1: Elastic net (combination L1 L2) relevant penalty > 0. Can tuned. learn_rate number learning rate. Can tuned. optimizer character string optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Can tuned. validation_split number 0 1 proportion data used validation. Can tuned. rnn_type character string type RNN cell (\"rnn\", \"lstm\", \"gru\"). tuned — pass via set_engine(). optimizer_args named list additional arguments passed optimizer. tuned — pass via set_engine(). loss character string loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). tuned — pass via set_engine(). early_stopping early_stop() object NULL. tuned — pass via set_engine(). device character string device (\"cpu\", \"cuda\", \"mps\"). tuned — pass via set_engine(). verbose Logical whether print training progress. tuned — pass via set_engine(). cache_weights Logical. TRUE, stores trained weight matrices returned object. tuned — pass via set_engine().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"model specification object class rnn_kindling.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"function creates model specification recurrent neural network can used within tidymodels workflows. model supports: Multiple RNN types: basic RNN, LSTM, GRU Bidirectional processing Dropout regularization GPU acceleration (CUDA, MPS, CPU) Hyperparameter tuning integration regression classification tasks device parameter controls computation occurs: NULL (default): Auto-detect best available device (CUDA > MPS > CPU) \"cuda\": Use NVIDIA GPU \"mps\": Use Apple Silicon GPU \"cpu\": Use CPU ","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/rnn_kindling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recurrent Neural Network via kindling — rnn_kindling","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     box::use(         recipes[recipe],         workflows[workflow, add_recipe, add_model],         parsnip[fit]     )      # Model specs     rnn_spec = rnn_kindling(         mode = \"classification\",         hidden_neurons = c(64, 32),         rnn_type = \"lstm\",         activation = c(\"relu\", \"elu\"),         epochs = 100,         bidirectional = TRUE     )      wf = workflow() |>         add_recipe(recipe(Species ~ ., data = iris)) |>         add_model(rnn_spec)      fit_wf = fit(wf, data = iris)     fit_wf } else {     message(\"Torch not fully installed — skipping example\") } #> ══ Workflow [trained] ══════════════════════════════════════════════════════════ #> Preprocessor: Recipe #> Model: rnn_kindling() #>  #> ── Preprocessor ──────────────────────────────────────────────────────────────── #> 0 Recipe Steps #>  #> ── Model ─────────────────────────────────────────────────────────────────────── #> Warning: running command 'tput cols' had status 2 #> Warning: running command 'tput cols' had status 2 #>  #> ========================= Long Short-Term Memory (RNN) ========================= #>  #>  #> -- RNN Model Summary ----------------------------------------------------------- #>  #>  #> Warning: running command 'tput cols' had status 2 #> ----------------------------------------------------------------------- #>   NN Model Type           :              RNN    n_predictors :      4 #>   RNN Type                :             LSTM    n_response   :      3 #>   Bidirectional           :              Yes    reg.         :   None #>   Number of Epochs        :              100    Device       :    cpu #>   Hidden Layer Units      :           64, 32                 :        #>   Number of Hidden Layers :                2                 :        #>   Pred. Type              :   classification                 :        #> ----------------------------------------------------------------------- #>  #>  #>  #> -- Activation function --------------------------------------------------------- #>  #>  #> Warning: running command 'tput cols' had status 2 #> ------------------------------------------------- #>   1st Layer {64}    :                      relu #>   2nd Layer {32}    :                       elu #>   Output Activation :   No act function applied #> ------------------------------------------------- #>  # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/safe_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Safe sampling function — safe_sample","title":"Safe sampling function — safe_sample","text":"R's sample() quirky behavior: sample(5, 1) samples 1:5, c(5). function ensures sample actual vector provided.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/safe_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Safe sampling function — safe_sample","text":"","code":"safe_sample(x, size, replace = FALSE)"},{"path":"https://kindling.joshuamarie.com/dev/reference/safe_sample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Safe sampling function — safe_sample","text":"x Vector sample size Number samples replace Sample replacement?","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/substitute_dot.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursively Substitute . with Expression — substitute_dot","title":"Recursively Substitute . with Expression — substitute_dot","text":"Recursively Substitute . Expression","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/substitute_dot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursively Substitute . with Expression — substitute_dot","text":"","code":"substitute_dot(expr, replacement)"},{"path":"https://kindling.joshuamarie.com/dev/reference/substitute_dot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recursively Substitute . with Expression — substitute_dot","text":"expr Expression containing . placeholders replacement Expression substitute .","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/substitute_dot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recursively Substitute . with Expression — substitute_dot","text":"Modified expression","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"function takes two-column data frame formats summary-like table. table can optionally split two parts, centered, given title. useful displaying summary information clean, tabular format. function also supports styling ANSI colors text formatting {cli} package column alignment options.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"","code":"table_summary(   data,   title = NULL,   l = NULL,   header = FALSE,   center_table = FALSE,   border_char = \"-\",   style = list(),   align = NULL,   ... )"},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"data data frame exactly two columns. data summarized displayed. title character string. optional title displayed table. l integer. number rows include left part split table. NULL, table split. header logical value. TRUE, column names data displayed header. center_table logical value. TRUE, table centered terminal. border_char Character used borders. Default \"\\u2500\". style list controlling visual styling table elements using ANSI formatting. Can include following components: left_col: Styling left column values. right_col: Styling right column values. border_text: Styling border. title: Styling title. sep: Separator character left right column. style component can either predefined style string (e.g., \"blue\", \"red_italic\", \"bold\") function takes context list /without value element returns styled text. align Controls alignment column values. Can specified three ways: single string: affects left column (e.g., \"left\", \"center\", \"right\"). vector two strings: affects columns order (e.g., c(\"left\", \"right\")). list named components: explicitly specifies alignment column ... Additional arguments (currently unused).","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"function return value. prints formatted table console.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/table_summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize and Display a Two-Column Data Frame as a Formatted Table — table_summary","text":"","code":"# Create a sample data frame df = data.frame(     Category = c(\"A\", \"B\", \"C\", \"D\", \"E\"),     Value = c(10, 20, 30, 40, 50) )  # Display the table with a title and header table_summary(df, title = \"Sample Table\", header = TRUE) #>  #>     Sample Table      #> --------------------- #>   Category    Value #> --------------------- #>   A              10 #>   B              20 #>   C              30 #>   D              40 #>   E              50 #> ---------------------  # Split the table after the second row and center it table_summary(df, l = 2, center_table = TRUE) #> Warning: running command 'tput cols' had status 2 #> ------------------------------------------ #>   A              10    C              30 #>   B              20    D              40 #>                        E              50 #> ------------------------------------------  # Use styling and alignment table_summary(     df, header = TRUE,     style = list(         left_col = \"blue_bold\",         right_col = \"red\",         title = \"green\",         border_text = \"yellow\"     ),     align = c(\"center\", \"right\") ) #> --------------------- #>   Category    Value #> --------------------- #>      A           10 #>      B           20 #>      C           30 #>      D           40 #>      E           50 #> ---------------------  # Use custom styling with lambda functions table_summary(     df, header = TRUE,     style = list(         left_col = \\(ctx) cli::col_red(ctx), # ctx$value is another option         right_col = \\(ctx) cli::col_blue(ctx)     ),     align = list(left_col = \"left\", right_col = \"right\") ) #> --------------------- #>   Category    Value #> --------------------- #>   A              10 #>   B              20 #>   C              30 #>   D              40 #>   E              50 #> ---------------------"},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nn_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Shared core implementation — train_nn_impl","title":"Shared core implementation — train_nn_impl","text":"Shared core implementation","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nn_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shared core implementation — train_nn_impl","text":"","code":"train_nn_impl(   x,   y,   hidden_neurons,   activations = NULL,   output_activation = NULL,   bias = TRUE,   arch = NULL,   early_stopping = NULL,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   fit_class = \"nn_fit\" )"},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nn_impl_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"train_nn implementation for torch datasets — train_nn_impl_dataset","title":"train_nn implementation for torch datasets — train_nn_impl_dataset","text":"train_nn implementation torch datasets","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nn_impl_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"train_nn implementation for torch datasets — train_nn_impl_dataset","text":"","code":"train_nn_impl_dataset(   dataset,   no_x,   no_y,   is_classification,   hidden_neurons,   activations = NULL,   output_activation = NULL,   bias = TRUE,   epochs = 100,   batch_size = 32,   penalty = 0,   mixture = 0,   learn_rate = 0.001,   optimizer = \"adam\",   optimizer_args = list(),   loss = \"mse\",   validation_split = 0,   device = NULL,   verbose = FALSE,   cache_weights = FALSE,   flatten_input = TRUE,   arch = NULL,   fit_class = \"nn_fit_ds\" )"},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nnsnip.html","id":null,"dir":"Reference","previous_headings":"","what":"Parsnip Interface of train_nn() — train_nnsnip","title":"Parsnip Interface of train_nn() — train_nnsnip","text":"train_nnsnip() defines neural network model specification can used classification regression. integrates tidymodels ecosystem uses train_nn() fitting backend, supporting architecture expressible via nn_arch() — feedforward, recurrent, convolutional, beyond.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nnsnip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parsnip Interface of train_nn() — train_nnsnip","text":"","code":"train_nnsnip(   mode = \"unknown\",   engine = \"kindling\",   hidden_neurons = NULL,   activations = NULL,   output_activation = NULL,   bias = NULL,   epochs = NULL,   batch_size = NULL,   penalty = NULL,   mixture = NULL,   learn_rate = NULL,   optimizer = NULL,   validation_split = NULL,   optimizer_args = NULL,   loss = NULL,   architecture = NULL,   flatten_input = NULL,   early_stopping = NULL,   device = NULL,   verbose = NULL,   cache_weights = NULL )"},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nnsnip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parsnip Interface of train_nn() — train_nnsnip","text":"mode single character string type model. Possible values \"unknown\", \"regression\", \"classification\". engine single character string specifying computational engine use fitting. Currently \"kindling\" supported. hidden_neurons integer vector number units hidden layer. Can tuned. activations character vector activation function names hidden layer (e.g., \"relu\", \"tanh\", \"sigmoid\"). Can tuned. output_activation character string output activation function. Can tuned. bias Logical whether include bias terms. Can tuned. epochs integer number training iterations. Can tuned. batch_size integer batch size training. Can tuned. penalty number regularization penalty (lambda). Default 0 (regularization). Higher values increase regularization strength. Can tuned. mixture number 0 1 elastic net mixing parameter. Default 0 (pure L2/Ridge regularization). 0: Pure L2 regularization (Ridge) 1: Pure L1 regularization (Lasso) 0 < mixture < 1: Elastic net (combination L1 L2) relevant penalty > 0. Can tuned. learn_rate number learning rate. Can tuned. optimizer character string optimizer type (\"adam\", \"sgd\", \"rmsprop\"). Can tuned. validation_split number 0 1 proportion data used validation. Can tuned. optimizer_args named list additional arguments passed optimizer. tuned — pass via set_engine(). loss character string loss function (\"mse\", \"mae\", \"cross_entropy\", \"bce\"). tuned — pass via set_engine(). architecture nn_arch() object custom architecture. tuned — pass via set_engine(). flatten_input Logical NULL. Controls input flattening. tuned — pass via set_engine(). early_stopping early_stop() object NULL. tuned — pass via set_engine(). device character string device use (\"cpu\", \"cuda\", \"mps\"). NULL, auto-detects best available device. tuned — pass via set_engine(). verbose Logical whether print training progress. Default FALSE. tuned — pass via set_engine(). cache_weights Logical. TRUE, stores trained weight matrices returned object. tuned — pass via set_engine().","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nnsnip.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parsnip Interface of train_nn() — train_nnsnip","text":"model specification object class train_nnsnip.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nnsnip.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parsnip Interface of train_nn() — train_nnsnip","text":"function creates model specification neural network can used within tidymodels workflows. underlying engine train_nn(), architecture-agnostic: architecture = NULL falls back standard feed-forward network, architecture expressible via nn_arch() can used instead. model supports: Configurable hidden layers activation functions (default MLP path) Custom architectures via nn_arch() (recurrent, convolutional, etc.) GPU acceleration (CUDA, MPS, CPU) Hyperparameter tuning integration regression classification tasks using default MLP path (custom architecture), hidden_neurons accepts integer vector element represents number neurons hidden layer. example, hidden_neurons = c(128, 64, 32) creates network three hidden layers. Pass nn_arch() object via set_engine() use custom architecture instead. device parameter controls computation occurs: NULL (default): Auto-detect best available device (CUDA > MPS > CPU) \"cuda\": Use NVIDIA GPU \"mps\": Use Apple Silicon GPU \"cpu\": Use CPU tuning, can use special tune tokens: hidden_neurons: use tune(\"hidden_neurons\") custom range activation: use tune(\"activation\") values like \"relu\", \"tanh\"","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/train_nnsnip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parsnip Interface of train_nn() — train_nnsnip","text":"","code":"# \\donttest{ if (torch::torch_is_installed()) {     box::use(         recipes[recipe],         workflows[workflow, add_recipe, add_model],         tune[tune],         parsnip[fit]     )      # Model spec     nn_spec = train_nnsnip(         mode = \"classification\",         hidden_neurons = c(30, 5),         activations = c(\"relu\", \"elu\"),         epochs = 100     )      wf = workflow() |>         add_recipe(recipe(Species ~ ., data = iris)) |>         add_model(nn_spec)      fit_wf = fit(wf, data = iris) } else {     message(\"Torch not fully installed — skipping example\") } # }"},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate device and get default device — validate_device","title":"Validate device and get default device — validate_device","text":"Check requested device available. auto-detect available GPU device fallback CPU.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate device and get default device — validate_device","text":"","code":"validate_device(device)"},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate device and get default device — validate_device","text":"device Character. Requested device.","code":""},{"path":"https://kindling.joshuamarie.com/dev/reference/validate_device.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate device and get default device — validate_device","text":"Character string validated device.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"new-experimental-functions-development-version","dir":"Changelog","previous_headings":"","what":"New Experimental functions","title":"kindling (development version)","text":"Generalized nn_module() expression generator generate torch::nn_module() expression sequential NN architectures use generate nn_module() 1D-CNN (Convolutional Neural Networks) 3 hidden layers: train_nn() execute nn_module_generator() nn_arch() must supplied inherit extra arguments nn_module_generator() function. Allows early stopping early_stopping supplied early_stop(). train_nnsnip() now provided make train_nn() interfaced tidymodels","code":"nn_module_generator(     nn_name = \"CNN1DClassifier\",     nn_layer = \"nn_conv1d\",     layer_arg_fn = ~ if (.is_output) {         list(.in, .out)     } else {         list(             in_channels = .in,             out_channels = .out,             kernel_size = 3L,             stride = 1L,             padding = 1L          )     },     after_output_transform = ~ .$mean(dim = 2),     last_layer_args = list(kernel_size = 1, stride = 2),     hd_neurons = c(16, 32, 64),     no_x = 1,     no_y = 10,     activations = \"relu\" )"},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"superset-development-version","dir":"Changelog","previous_headings":"New Experimental functions","what":"Superset","title":"kindling (development version)","text":"act_funs() DSL function now supports index-style parameter specification parametric activation functions Activation functions can now modified using [ syntax (e.g. softplus[beta = 0.2]) current args() (e.g. softplus = args(beta = 0.2)) now superseded .","code":""},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"bug-fixes-development-version","dir":"Changelog","previous_headings":"New Experimental functions","what":"Bug Fixes","title":"kindling (development version)","text":"suffix generated 13 ordinal_gen(). Now fixed.","code":""},{"path":[]},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"fixes-0-2-1","dir":"Changelog","previous_headings":"","what":"Fixes","title":"kindling 0.2.1","text":"hd_neurons ffnn_generator() rnn_generator() accepts empty arguments, implies ’s hidden layers applied.","code":""},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"kindling-020","dir":"Changelog","previous_headings":"","what":"kindling 0.2.0","title":"kindling 0.2.0","text":"CRAN release: 2026-02-04","code":""},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"kindling 0.2.0","text":"Added regularization support neural network models L1 regularization (Lasso) feature selection via mixture = 1 L2 regularization (Ridge) weight decay via mixture = 0 Elastic Net combining L1 L2 penalties via 0 < mixture < 1 Controlled via penalty (regularization strength) mixture (L1/L2 balance) parameters Follows tidymodels conventions consistency glmnet packages n_hlayers() now fully supports tuning number hidden layers hidden_neurons() gains support discrete values via disc_values argument e.g. disc_values = c(32L, 64L, 128L, 256L)) now allowed allows tuning specific common hidden unit sizes instead (addition ) continuous range","code":""},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"implementation-fixes-0-2-0","dir":"Changelog","previous_headings":"","what":"Implementation fixes","title":"kindling 0.2.0","text":"Tuning methods grid_depth() now fixed Parameter space number hidden layers now fixed active Corrected parameter space handling n_hlayers (invalid sampling x > 1) Uses tidyr::expand_grid(), purrr::cross*() Fix randomization parameter space produce NAs outside kindling‘s ’dials’ list columns n_hlayers = 1 supported models now use hardhat::mold(), instead model.frame() model.matrix().","code":""},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"documentation-0-2-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"kindling 0.2.0","text":"Add vignette showcase comparison similar packages package description got clarifications Vignette showcase comparison similar packages hidden_neurons parameter now supports discrete values specification Users can specify exact neuron counts via values parameter (e.g., hidden_neurons(values = c(32, 64, 128))) Maintains backward compatibility range-based parameters (e.g., hidden_neurons(range = c(8L, 512L)) / hidden_neurons(c(8L, 512L))) Added \\value documentation kindling-nn-wrappers CRAN compliance Documented argument handling list-column unwrapping tidymodels wrapper functions Clarified relationship grid_depth() wrapper functions","code":""},{"path":"https://kindling.joshuamarie.com/dev/news/index.html","id":"kindling-010","dir":"Changelog","previous_headings":"","what":"kindling 0.1.0","title":"kindling 0.1.0","text":"CRAN release: 2026-01-31 Initial CRAN release Higher-level interface torch package define, train, tune neural networks Support feedforward (multi-layer perceptron) recurrent networks (RNN, LSTM, GRU) Integration tidymodels ecosystem (parsnip, workflows, recipes, tuning) Variable importance plots network visualization tools","code":""}]
