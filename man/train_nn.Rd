% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/generalized-nn-fit.R
\name{train_nn}
\alias{train_nn}
\title{Generalized Neural Network Trainer}
\usage{
train_nn(
  formula = NULL,
  data = NULL,
  hidden_neurons,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE,
  arch = NULL,
  epochs = 100,
  batch_size = 32,
  penalty = 0,
  mixture = 0,
  learn_rate = 0.001,
  optimizer = "adam",
  optimizer_args = list(),
  loss = "mse",
  validation_split = 0,
  device = NULL,
  verbose = FALSE,
  cache_weights = FALSE,
  ...,
  x = NULL,
  y = NULL
)
}
\arguments{
\item{formula}{Formula. Model formula (e.g., \code{y ~ x1 + x2}).}

\item{data}{Data frame. Training data.}

\item{hidden_neurons}{Integer vector. Number of neurons in each hidden layer.}

\item{activations}{Activation function specifications. See \code{act_funs()}.}

\item{output_activation}{Optional. Activation for the output layer.}

\item{bias}{Logical. Use bias weights. Default \code{TRUE}.}

\item{arch}{An \code{nn_arch()} object specifying the architecture. Default \code{NULL} (FFNN fallback).}

\item{epochs}{Integer. Number of training epochs. Default \code{100}.}

\item{batch_size}{Integer. Batch size for training. Default \code{32}.}

\item{penalty}{Numeric. Regularization penalty (lambda). Default \code{0}.}

\item{mixture}{Numeric. Elastic net mixing parameter (0-1). Default \code{0}.}

\item{learn_rate}{Numeric. Learning rate for optimizer. Default \code{0.001}.}

\item{optimizer}{Character. Optimizer type (\code{"adam"}, \code{"sgd"}, \code{"rmsprop"}). Default \code{"adam"}.}

\item{optimizer_args}{Named list. Additional arguments passed to the optimizer. Default \code{list()}.}

\item{loss}{Character. Loss function (\code{"mse"}, \code{"mae"}, \code{"cross_entropy"}, \code{"bce"}).
Default \code{"mse"}.}

\item{validation_split}{Numeric. Proportion of data for validation (0-1). Default \code{0}.}

\item{device}{Character. Device to use (\code{"cpu"}, \code{"cuda"}, \code{"mps"}). Default \code{NULL} (auto-detect).}

\item{verbose}{Logical. Print training progress. Default \code{FALSE}.}

\item{cache_weights}{Logical. Cache weight matrices. Default \code{FALSE}.}

\item{...}{Additional arguments. It is currently unused.}

\item{x}{When not using formula: predictor data (data.frame or matrix).}

\item{y}{When not using formula: outcome data (vector, factor, or matrix).}
}
\value{
An object of class \code{"nn_fit"}.
}
\description{
\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}}

Train a neural network with a user-defined architecture supplied via \code{nn_arch()}.
\code{train_nn()} handles data preprocessing, the training loop, and prediction,
while the model architecture is fully controlled through the \code{arch} argument.
When \code{arch = NULL}, it falls back to a plain FFNN (\code{nn_linear}) architecture.
}
\examples{
\donttest{
if (torch::torch_is_installed()) {
    # Default FFNN fallback (arch = NULL)
    model_ffnn = train_nn(
        Sepal.Length ~ .,
        data = iris[, 1:4],
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50
    )

    # GRU via nn_arch()
    gru_arch = nn_arch(
        nn_name = "GRU",
        nn_layer = "nn_gru",
        layer_arg_fn = ~ if (.is_output) {
            list(.in, .out)
        } else {
            list(input_size = .in, hidden_size = .out, batch_first = TRUE)
        },
        forward_extract = ~ .[[1]],
        before_output_transform = ~ .[, .$size(2), ]
    )

    model_gru = train_nn(
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50,
        arch = gru_arch,
        x = iris[, 2:4],
        y = iris$Sepal.Length
    )
}
}

}
