% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/generalized-nn-fit.R, R/generalized-nn-fitds.R
\name{gen-nn-train}
\alias{gen-nn-train}
\alias{train_nn}
\alias{train_nn.matrix}
\alias{train_nn.data.frame}
\alias{train_nn.formula}
\alias{train_nn.default}
\alias{train_nn.dataset}
\title{Generalized Neural Network Trainer}
\usage{
train_nn(x, ...)

\method{train_nn}{matrix}(
  x,
  y,
  hidden_neurons = NULL,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE,
  arch = NULL,
  epochs = 100,
  batch_size = 32,
  penalty = 0,
  mixture = 0,
  learn_rate = 0.001,
  optimizer = "adam",
  optimizer_args = list(),
  loss = "mse",
  validation_split = 0,
  device = NULL,
  verbose = FALSE,
  cache_weights = FALSE,
  ...
)

\method{train_nn}{data.frame}(
  x,
  y,
  hidden_neurons = NULL,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE,
  arch = NULL,
  epochs = 100,
  batch_size = 32,
  penalty = 0,
  mixture = 0,
  learn_rate = 0.001,
  optimizer = "adam",
  optimizer_args = list(),
  loss = "mse",
  validation_split = 0,
  device = NULL,
  verbose = FALSE,
  cache_weights = FALSE,
  ...
)

\method{train_nn}{formula}(
  x,
  data,
  hidden_neurons = NULL,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE,
  arch = NULL,
  epochs = 100,
  batch_size = 32,
  penalty = 0,
  mixture = 0,
  learn_rate = 0.001,
  optimizer = "adam",
  optimizer_args = list(),
  loss = "mse",
  validation_split = 0,
  device = NULL,
  verbose = FALSE,
  cache_weights = FALSE,
  ...
)

\method{train_nn}{default}(x, ...)

\method{train_nn}{dataset}(
  x,
  y = NULL,
  hidden_neurons,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE,
  arch = NULL,
  epochs = 100,
  batch_size = 32,
  penalty = 0,
  mixture = 0,
  learn_rate = 0.001,
  optimizer = "adam",
  optimizer_args = list(),
  loss = "mse",
  validation_split = 0,
  device = NULL,
  verbose = FALSE,
  cache_weights = FALSE,
  n_classes = NULL,
  ...
)
}
\arguments{
\item{x}{A torch dataset object.}

\item{...}{Additional arguments passed to specific methods.}

\item{y}{Ignored. Labels come from the dataset itself.}

\item{hidden_neurons}{Integer vector specifying the number of neurons in each
hidden layer, e.g. \code{c(128, 64)} for two hidden layers. When \code{NULL} or missing,
no hidden layers are used and the model reduces to a single linear mapping
from inputs to outputs.}

\item{activations}{Activation function specification(s) for the hidden layers.
See \code{\link[=act_funs]{act_funs()}} for supported values. Recycled if a single value is given.}

\item{output_activation}{Optional activation function for the output layer.
Defaults to \code{NULL} (no activation / linear output).}

\item{bias}{Logical. Whether to include bias terms in each layer. Default \code{TRUE}.}

\item{arch}{An \code{\link[=nn_arch]{nn_arch()}} object specifying a custom architecture. Default
\code{NULL}, which falls back to a standard feed-forward network.}

\item{epochs}{Positive integer. Number of full passes over the training data.
Default \code{100}.}

\item{batch_size}{Positive integer. Number of samples per mini-batch. Default \code{32}.}

\item{penalty}{Non-negative numeric. L1/L2 regularization strength (lambda).
Default \code{0} (no regularization).}

\item{mixture}{Numeric in [0, 1]. Elastic net mixing parameter: \code{0} = pure
ridge (L2), \code{1} = pure lasso (L1). Default \code{0}.}

\item{learn_rate}{Positive numeric. Step size for the optimizer. Default \code{0.001}.}

\item{optimizer}{Character. Optimizer algorithm. One of \code{"adam"} (default),
\code{"sgd"}, or \code{"rmsprop"}.}

\item{optimizer_args}{Named list of additional arguments forwarded to the
optimizer constructor (e.g. \code{list(momentum = 0.9)} for SGD). Default \code{list()}.}

\item{loss}{Character. Loss function used during training. One of \code{"mse"}
(default), \code{"mae"}, \code{"cross_entropy"}, or \code{"bce"}. For classification tasks
with a scalar label, \code{"cross_entropy"} is set automatically.}

\item{validation_split}{Numeric in [0, 1). Proportion of training data held
out for validation. Default \code{0} (no validation set).}

\item{device}{Character. Compute device: \code{"cpu"}, \code{"cuda"}, or \code{"mps"}.
Default \code{NULL}, which auto-detects the best available device.}

\item{verbose}{Logical. If \code{TRUE}, prints loss (and validation loss) at regular
intervals during training. Default \code{FALSE}.}

\item{cache_weights}{Logical. If \code{TRUE}, stores a copy of the trained weight
matrices in the returned object under \verb{$cached_weights}. Default \code{FALSE}.}

\item{data}{A data frame. Required when \code{x} is a formula.}

\item{n_classes}{Positive integer. Number of output classes. Required when
\code{x} is a \code{dataset} with scalar (classification) labels; ignored otherwise.}
}
\value{
An object of class \code{"nn_fit"}, or one of its subclasses:
\itemize{
\item \code{c("nn_fit_tab", "nn_fit")} — returned by the \code{data.frame} and \code{formula} methods
\item \code{c("nn_fit_ds", "nn_fit")} — returned by the \code{dataset} method
}

All subclasses share a common structure. See \strong{Details} for the list of
components.

An object of class \code{c("nn_fit_ds", "nn_fit")}.
}
\description{
\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}}

\code{train_nn()} is a generic function for training neural networks with a
user-defined architecture via \code{\link[=nn_arch]{nn_arch()}}. Dispatch is based on the class
of \code{x}.

All methods delegate to a shared implementation core after preprocessing.
When \code{arch = NULL}, the model falls back to a plain feed-forward neural network
(\code{nn_linear}) architecture.
}
\details{
The returned \code{"nn_fit"} object is a named list with the following components:
\itemize{
\item \code{model} — the trained \code{torch::nn_module} object
\item \code{fitted} — fitted values on the training data (or \code{NULL} for dataset fits)
\item \code{loss_history} — numeric vector of per-epoch training loss
\item \code{val_loss_history} — per-epoch validation loss, or \code{NULL} if \code{validation_split = 0}
\item \code{n_epochs} — number of epochs trained
\item \code{hidden_neurons}, \code{activations}, \code{output_activation} — architecture spec
\item \code{penalty}, \code{mixture} — regularization settings
\item \code{feature_names}, \code{response_name} — variable names (tabular methods only)
\item \code{no_x}, \code{no_y} — number of input features and output nodes
\item \code{is_classification} — logical flag
\item \code{y_levels}, \code{n_classes} — class labels and count (classification only)
\item \code{device} — device the model is on
\item \code{cached_weights} — list of weight matrices, or \code{NULL}
\item \code{arch} — the \code{nn_arch} object used, or \code{NULL}
}
}
\section{Matrix type}{

When \code{x} is supplied as a raw numeric matrix, it directly uses the base implementation
\code{train_nn_impl} directly. In this case, no preprocessing happens.
}

\section{Tabular data (\code{data.frame} method)}{

When \code{x} is a data frame, \code{y} can be either a vector / factor / matrix of
outcomes (standard supervised learning), or a formula of the form
\code{outcome ~ predictors} evaluated against \code{x}. In both cases preprocessing
is handled by \code{hardhat::mold()}.
}

\section{Formula interface}{

When \code{x} is a formula, \code{data} must be supplied as the data frame against
which the formula is evaluated. Preprocessing is handled by \code{hardhat::mold()}.
}

\section{Dataset method (\code{train_nn.dataset()})}{

Trains a neural network directly on a \code{torch} dataset object. Batching and
lazy loading are handled by \code{torch::dataloader()}, making this method
well-suited for large datasets that do not fit entirely in memory.

Labels are taken from the second element of each dataset item (i.e.
\code{dataset[[i]][[2]]}), so \code{y} is ignored. When the label is a scalar tensor,
a classification task is assumed and \code{n_classes} must be supplied. The loss
is automatically switched to \code{"cross_entropy"} in that case.

Fitted values are \strong{not} cached in the returned object. Use
\code{\link[=predict.nn_fit_ds]{predict.nn_fit_ds()}} with \code{newdata} to obtain predictions after training.
}

\examples{
\donttest{
if (torch::torch_is_installed()) {
    # Matrix method — no preprocessing
    model = train_nn(
        x = as.matrix(iris[, 2:4]),
        y = iris$Sepal.Length,
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50
    )

    # Data frame method — y as a vector
    model = train_nn(
        x = iris[, 2:4],
        y = iris$Sepal.Length,
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50
    )

    # Data frame method — y as a formula evaluated against x
    model = train_nn(
        x = iris,
        y = Sepal.Length ~ . - Species,
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50
    )

    # Formula method — outcome derived from formula
    model = train_nn(
        x = Sepal.Length ~ .,
        data = iris[, 1:4],
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50
    )

    # No hidden layers — linear model (hidden_neurons omitted)
    model = train_nn(
        x = Sepal.Length ~ .,
        data = iris[, 1:4],
        epochs = 50
    )
}
}

\donttest{
if (torch::torch_is_installed()) {
    # torch dataset method — labels come from the dataset itself
    iris_cls_dataset = torch::dataset(
        name = "iris_cls_dataset",
        
        initialize = function(data = iris) {
            self$x = torch::torch_tensor(
                as.matrix(data[, 1:4]),
                dtype = torch::torch_float32()
            )
            # Species is a factor; convert to integer (1-indexed -> keep as-is for cross_entropy)
            self$y = torch::torch_tensor(
                as.integer(data$Species),
                dtype = torch::torch_long()
            )
        },
        
        .getitem = function(i) {
            list(self$x[i, ], self$y[i])
        },
        
        .length = function() {
            self$x$size(1)
        }
    )()
    
    model = train_nn(
        x = iris_cls_dataset,
        hidden_neurons = c(32, 10),
        activations = "relu",
        epochs = 80,
        n_classes = 3 # Iris dataset has only 3 species
    )
    
    pred_nn = predict(model_nn_ds, iris_cls_dataset)
    class_preds = c("Setosa", "Versicolor", "Virginica")[predict(model_nn_ds, iris_cls_dataset)]
    
    # Confusion Matrix
    table(actual = iris$Species, pred = class_preds)
}
}

}
\seealso{
\code{\link[=predict.nn_fit]{predict.nn_fit()}}, \code{\link[=nn_arch]{nn_arch()}}, \code{\link[=act_funs]{act_funs()}}
}
