% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/generalized-nn-fit.R
\name{gen-nn-train}
\alias{gen-nn-train}
\alias{train_nn}
\alias{train_nn.matrix}
\alias{train_nn.data.frame}
\alias{train_nn.formula}
\alias{train_nn.default}
\title{Generalized Neural Network Trainer}
\usage{
train_nn(x, ...)

\method{train_nn}{matrix}(
  x,
  y,
  hidden_neurons,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE,
  arch = NULL,
  epochs = 100,
  batch_size = 32,
  penalty = 0,
  mixture = 0,
  learn_rate = 0.001,
  optimizer = "adam",
  optimizer_args = list(),
  loss = "mse",
  validation_split = 0,
  device = NULL,
  verbose = FALSE,
  cache_weights = FALSE,
  ...
)

\method{train_nn}{data.frame}(
  x,
  y,
  activations = NULL,
  output_activation = NULL,
  hidden_neurons,
  bias = TRUE,
  arch = NULL,
  epochs = 100,
  batch_size = 32,
  penalty = 0,
  mixture = 0,
  learn_rate = 0.001,
  optimizer = "adam",
  optimizer_args = list(),
  loss = "mse",
  validation_split = 0,
  device = NULL,
  verbose = FALSE,
  cache_weights = FALSE,
  ...
)

\method{train_nn}{formula}(
  x,
  data,
  hidden_neurons,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE,
  arch = NULL,
  epochs = 100,
  batch_size = 32,
  penalty = 0,
  mixture = 0,
  learn_rate = 0.001,
  optimizer = "adam",
  optimizer_args = list(),
  loss = "mse",
  validation_split = 0,
  device = NULL,
  verbose = FALSE,
  cache_weights = FALSE,
  ...
)

\method{train_nn}{default}(x, ...)
}
\arguments{
\item{x}{Predictor data. Dispatch is based on its class:
\itemize{
\item \code{matrix}: used directly, no preprocessing
\item \code{data.frame}: preprocessed via \code{hardhat::mold()}
\item \code{formula}: combined with \code{data} and preprocessed via \code{hardhat::mold()}
}}

\item{...}{Additional arguments passed to methods.}

\item{y}{Outcome data (vector, factor, or matrix). Not used when \code{x} is a formula.}

\item{hidden_neurons}{Integer vector. Number of neurons in each hidden layer.}

\item{activations}{Activation function specifications. See \code{act_funs()}.}

\item{output_activation}{Optional. Activation for the output layer.}

\item{bias}{Logical. Use bias weights. Default \code{TRUE}.}

\item{arch}{An \code{nn_arch()} object specifying the architecture. Default \code{NULL} (FFNN fallback).}

\item{epochs}{Integer. Number of training epochs. Default \code{100}.}

\item{batch_size}{Integer. Batch size for training. Default \code{32}.}

\item{penalty}{Numeric. Regularization penalty (lambda). Default \code{0}.}

\item{mixture}{Numeric. Elastic net mixing parameter (0-1). Default \code{0}.}

\item{learn_rate}{Numeric. Learning rate for optimizer. Default \code{0.001}.}

\item{optimizer}{Character. Optimizer type (\code{"adam"}, \code{"sgd"}, \code{"rmsprop"}). Default \code{"adam"}.}

\item{optimizer_args}{Named list. Additional arguments passed to the optimizer. Default \code{list()}.}

\item{loss}{Character. Loss function (\code{"mse"}, \code{"mae"}, \code{"cross_entropy"}, \code{"bce"}).
Default \code{"mse"}.}

\item{validation_split}{Numeric. Proportion of data for validation (0-1). Default \code{0}.}

\item{device}{Character. Device to use (\code{"cpu"}, \code{"cuda"}, \code{"mps"}). Default \code{NULL} (auto-detect).}

\item{verbose}{Logical. Print training progress. Default \code{FALSE}.}

\item{cache_weights}{Logical. Cache weight matrices. Default \code{FALSE}.}

\item{data}{Data frame. Required when \code{x} is a formula.}
}
\value{
An object of class \code{"nn_fit"}, or a subclass thereof:
\itemize{
\item \code{c("nn_fit_tab", "nn_fit")} when called via \code{data.frame} or \code{formula} method
}
}
\description{
\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}}

\code{train_nn()} is a generic function for training neural networks with a
user-defined architecture via \code{\link[=nn_arch]{nn_arch()}}. Dispatch is based on the class
of \code{x}, allowing different preprocessing pipelines per data type:
\itemize{
\item \code{train_nn.matrix()} — raw interface, no preprocessing
\item \code{train_nn.data.frame()} — tabular interface via \code{hardhat::mold()}
\item \code{train_nn.formula()} — formula interface via \code{hardhat::mold()}
}

All methods delegate to the shared \code{\link[=train_nn_impl]{train_nn_impl()}} core after preprocessing.
When \code{arch = NULL}, the model falls back to a plain FFNN (\code{nn_linear}) architecture.
}
\examples{
\donttest{
if (torch::torch_is_installed()) {
    # matrix method
    model = train_nn(
        x = as.matrix(iris[, 2:4]),
        y = iris$Sepal.Length,
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50
    )

    # data.frame method
    model = train_nn(
        x = iris[, 2:4],
        y = iris$Sepal.Length,
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50
    )

    # formula method
    model = train_nn(
        x = Sepal.Length ~ .,
        data = iris[, 1:4],
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50
    )
}
}

}
