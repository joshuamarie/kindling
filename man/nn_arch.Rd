% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nn_arch.R
\name{nn_arch}
\alias{nn_arch}
\title{Architecture specification for train_nn()}
\usage{
nn_arch(
  nn_name = "nnModule",
  nn_layer = NULL,
  out_nn_layer = NULL,
  nn_layer_args = list(),
  layer_arg_fn = NULL,
  forward_extract = NULL,
  before_output_transform = NULL,
  after_output_transform = NULL,
  last_layer_args = list(),
  input_transform = NULL
)
}
\arguments{
\item{nn_name}{Character. Name of the generated module class. Default \code{"nnModule"}.}

\item{nn_layer}{Layer type for hidden layers. Accepted forms:
\itemize{
\item \code{NULL} (default): uses \code{nn_linear} from \code{{torch}}
\item Character string: e.g. \code{"torch::nn_gru"}, \code{"my_custom_layer"}
\item Formula: e.g. \code{~ rbf_layer}, \code{~ torch::nn_linear} (the RHS is used as the constructor)
\item Function object: a layer constructor function
}}

\item{out_nn_layer}{Optional. Layer type forced on the output layer. Accepts the same forms
as \code{nn_layer}. Default \code{NULL} (uses the same layer as \code{nn_layer}).}

\item{nn_layer_args}{Named list. Additional arguments passed to every layer constructor.
Default \code{list()}.}

\item{layer_arg_fn}{Formula or function. Generates per-layer constructor arguments.
Default \code{NULL} (FFNN-style: \code{list(in_dim, out_dim, bias = bias)}).}

\item{forward_extract}{Formula or function. Processes layer output in the forward pass.
Default \code{NULL}.}

\item{before_output_transform}{Formula or function. Transforms input before the output
layer. Default \code{NULL}.}

\item{after_output_transform}{Formula or function. Transforms output after the output
layer. Default \code{NULL}.}

\item{last_layer_args}{Named list or formula. Extra arguments for the output layer only.
Default \code{list()}.}

\item{input_transform}{Formula or function. Applied to each input tensor before it is
passed to the model â€” both during training (per-batch) and prediction. Use this for
structural reshaping that must happen every forward pass, such as adding a sequence
dimension for RNNs (\code{~ .$unsqueeze(2)}) or a channel dimension for CNNs
(\code{~ .$unsqueeze(1)}).}
}
\value{
An object of class \code{"nn_arch"}, a named list of \code{nn_module_generator()} arguments.
}
\description{
\code{nn_arch()} is a helper that bundles \code{nn_module_generator()} arguments into a
single object passed to \code{train_nn()} via the \code{arch} parameter. All arguments
mirror those of \code{nn_module_generator()} exactly, including their defaults.
}
\note{
The \code{nn_arch} object captures the caller environment at construction time so that
user-defined layer constructors (e.g. a custom \code{rbf_layer}) remain accessible when the
model is built. This means saving the \code{nn_arch} or the resulting \code{nn_fit} object to disk
with \code{saveRDS()} will embed that environment, which can produce large files and will
\strong{not} restore the layer constructor when reloaded in a fresh session. If you plan to
serialize fitted models, ensure the custom layer is defined in a package or sourced
before calling \code{predict()}.
}
\examples{
\donttest{
if (torch::torch_is_installed()) {
    # GRU architecture spec
    gru_arch = nn_arch(
        nn_name = "GRU",
        nn_layer = "torch::nn_gru",
        layer_arg_fn = ~ if (.is_output) {
            list(.in, .out)
        } else {
            list(input_size = .in, hidden_size = .out, batch_first = TRUE)
        },
        out_nn_layer = "torch::nn_linear",
        forward_extract = ~ .[[1]],
        before_output_transform = ~ .[, .$size(2), ],
        input_transform = ~ .$unsqueeze(2)
    )

    model = train_nn(
        Sepal.Length ~ .,
        data = iris[, 1:4],
        hidden_neurons = c(64, 32),
        activations = "relu",
        epochs = 50,
        arch = gru_arch
    )
}
}

}
