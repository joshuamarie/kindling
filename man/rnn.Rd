% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/torch_nn_exec.R
\name{rnn}
\alias{rnn}
\title{Recurrent Neural Network Training}
\usage{
rnn(
  formula,
  data,
  hidden_neurons,
  rnn_type = "lstm",
  activations = NULL,
  bias = TRUE,
  bidirectional = TRUE,
  dropout = 0,
  epochs = 100,
  batch_size = 32,
  learning_rate = 0.001,
  optimizer = "adam",
  loss = "mse",
  validation_split = 0,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{formula}{Formula. Model formula (e.g., y ~ x1 + x2).}

\item{data}{Data frame. Training data.}

\item{hidden_neurons}{Integer vector. Number of neurons in each hidden layer.}

\item{rnn_type}{Character. Type of RNN ("rnn", "lstm", "gru"). Default \code{"lstm"}.}

\item{activations}{Activation function specifications. See \code{act_funs()}.}

\item{bias}{Logical. Use bias weights. Default \code{TRUE}.}

\item{bidirectional}{Logical. Use bidirectional RNN. Default \code{TRUE}.}

\item{dropout}{Numeric. Dropout rate between layers. Default \code{0}.}

\item{epochs}{Integer. Number of training epochs. Default \code{100}.}

\item{batch_size}{Integer. Batch size for training. Default \code{32}.}

\item{learning_rate}{Numeric. Learning rate for optimizer. Default \code{0.001}.}

\item{optimizer}{Character. Optimizer type ("adam", "sgd", "rmsprop"). Default \code{"adam"}.}

\item{loss}{Character. Loss function ("mse", "mae", "cross_entropy", "bce"). Default \code{"mse"}.}

\item{validation_split}{Numeric. Proportion of data for validation (0-1). Default \code{0}.}

\item{verbose}{Logical. Print training progress. Default \code{FALSE}.}

\item{...}{Additional arguments passed to the optimizer.}
}
\description{
Train a recurrent neural network using the torch package.
}
\examples{
\dontrun{
# Regression with LSTM
model_rnn = rnn(
    Sepal.Length ~ .,
    data = iris[, 1:4],
    hidden_neurons = c(64, 32),
    rnn_type = "lstm",
    activations = "relu",
    epochs = 50
)

# Classification with GRU
model_gru = rnn(
    Species ~ .,
    data = iris,
    hidden_neurons = c(128, 64),
    rnn_type = "gru",
    activations = act_funs(relu, tanh),
    loss = "cross_entropy",
    epochs = 100
)
}

}
