% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/torch_nn_exec.R
\name{ffnn}
\alias{ffnn}
\title{Feed-Forward Neural Network Training}
\usage{
ffnn(
  formula,
  data,
  hidden_neurons,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE,
  epochs = 100,
  batch_size = 32,
  learning_rate = 0.001,
  optimizer = "adam",
  loss = "mse",
  validation_split = 0,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{formula}{Formula. Model formula (e.g., y ~ x1 + x2).}

\item{data}{Data frame. Training data.}

\item{hidden_neurons}{Integer vector. Number of neurons in each hidden layer.}

\item{activations}{Activation function specifications. See \code{act_funs()}.}

\item{output_activation}{Optional. Activation for output layer.}

\item{bias}{Logical. Use bias weights. Default \code{TRUE}.}

\item{epochs}{Integer. Number of training epochs. Default \code{100}.}

\item{batch_size}{Integer. Batch size for training. Default \code{32}.}

\item{learning_rate}{Numeric. Learning rate for optimizer. Default \code{0.001}.}

\item{optimizer}{Character. Optimizer type ("adam", "sgd", "rmsprop"). Default \code{"adam"}.}

\item{loss}{Character. Loss function ("mse", "mae", "cross_entropy", "bce"). Default \code{"mse"}.}

\item{validation_split}{Numeric. Proportion of data for validation (0-1). Default \code{0}.}

\item{verbose}{Logical. Print training progress. Default \code{FALSE}.}

\item{...}{Additional arguments passed to the optimizer.}
}
\value{
An object of class "ffnn_fit" containing:
\item{model}{Trained torch module}
\item{formula}{Model formula}
\item{fitted.values}{Fitted values on training data}
\item{loss_history}{Training loss per epoch}
\item{val_loss_history}{Validation loss per epoch (if validation_split > 0)}
\item{n_epochs}{Number of epochs trained}
\item{feature_names}{Names of predictor variables}
\item{response_name}{Name of response variable}
}
\description{
Train a feed-forward neural network using the torch package.
}
\examples{
\dontrun{
# Regression task
model_reg = ffnn(
    Sepal.Length ~ .,
    data = iris[, 1:4],
    hidden_neurons = c(64, 32),
    activations = "relu",
    epochs = 50,
    verbose = FALSE
)

# Classification task
model_clf = ffnn(
    Species ~ .,
    data = iris,
    hidden_neurons = c(128, 64, 32),
    activations = act_funs(relu, selu, softshrink = args(lambd = 0.5)),
    loss = "cross_entropy",
    epochs = 100
)

# Predictions work for both tasks
pred_reg = predict(model_reg, iris[1:5, 1:4])
pred_clf = predict(model_clf, iris[1:5, ])
}

}
