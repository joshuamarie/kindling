% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/torch_nn_generator.R
\name{rnn_generator}
\alias{rnn_generator}
\title{Recurrent Neural Network Module Generator}
\usage{
rnn_generator(
  nn_name = "DeepRNN",
  hd_neurons,
  no_x,
  no_y,
  rnn_type = "lstm",
  bias = TRUE,
  activations = NULL,
  bidirectional = TRUE,
  dropout = 0,
  ...
)
}
\arguments{
\item{nn_name}{Character. Name of the generated RNN module class. Default is \code{"DeepRNN"}.}

\item{hd_neurons}{Integer vector. Number of neurons in each hidden RNN layer.}

\item{no_x}{Integer. Number of input features.}

\item{no_y}{Integer. Number of output features.}

\item{rnn_type}{Character. Type of RNN to use. Must be one of \code{"rnn"}, \code{"lstm"}, or \code{"gru"}. Default is \code{"lstm"}.}

\item{bias}{Logical. Whether to use bias weights. Default is \code{TRUE}}

\item{activations}{Activation function specifications for each hidden layer.
Can be:
\itemize{
\item \code{NULL}: No activation functions.
\item Character vector: e.g., \code{c("relu", "sigmoid")}.
\item List: e.g., \code{act_funs(relu, elu, softshrink = args(lambd = 0.5))}.
\item \code{activation_spec} object from \code{act_funs()}.
}

If the length of \code{activations} is \code{1L}, this will be the activation throughout the architecture.}

\item{bidirectional}{Logical. Whether to use bidirectional RNN layers. Default is \code{TRUE}.}

\item{dropout}{Numeric. Dropout rate between RNN layers. Default is \code{0}.}

\item{...}{Additional arguments (currently unused).}
}
\value{
A \code{torch} module expression representing the RNN.
}
\description{
This function generates a recurrent neural network (RNN) module expression
from the \code{torch} package in R. It allows customization of the RNN architecture,
including the number of hidden layers, neurons, RNN type, activation functions,
and other parameters.
}
\details{
The generated RNN module will have the specified number of recurrent layers,
with each layer containing the specified number of hidden units. Activation functions
can be applied after each RNN layer as specified. The final output is taken from the
last time step and passed through a linear layer.

The generated module properly namespaces all torch functions to avoid
polluting the global namespace.
}
\examples{
# Basic LSTM with 2 layers
rnn_mod = rnn_generator(
    nn_name = "MyLSTM",
    hd_neurons = c(64, 32),
    no_x = 10,
    no_y = 1,
    rnn_type = "lstm",
    activations = 'relu'
)

# Evaluate and instantiate
model = eval(rnn_mod)()

# GRU with different activations
rnn_mod2 = rnn_generator(
    nn_name = "MyGRU",
    hd_neurons = c(128, 64, 32),
    no_x = 20,
    no_y = 5,
    rnn_type = "gru",
    activations = act_funs(relu, elu, relu),
    bidirectional = FALSE
)

# Parameterized activation and dropout
rnn_mod3 = rnn_generator(
    hd_neurons = c(100, 50, 25),
    no_x = 15,
    no_y = 3,
    rnn_type = "lstm",
    activations = act_funs(
        relu,
        leaky_relu = args(negative_slope = 0.01),
        tanh
    ),
    bidirectional = TRUE,
    dropout = 0.3
)

}
