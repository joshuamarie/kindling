% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mlp_kindling.R
\name{mlp_kindling}
\alias{mlp_kindling}
\title{Multi-Layer Perceptron (Feedforward Neural Network) via kindling}
\usage{
mlp_kindling(
  mode = "unknown",
  engine = "kindling",
  hidden_neurons = NULL,
  activations = NULL,
  output_activation = NULL,
  bias = NULL,
  epochs = NULL,
  batch_size = NULL,
  penalty = NULL,
  mixture = NULL,
  learn_rate = NULL,
  optimizer = NULL,
  validation_split = NULL,
  optimizer_args = NULL,
  loss = NULL,
  architecture = NULL,
  flatten_input = NULL,
  early_stopping = NULL,
  device = NULL,
  verbose = NULL,
  cache_weights = NULL
)
}
\arguments{
\item{mode}{A single character string for the type of model. Possible values
are "unknown", "regression", or "classification".}

\item{engine}{A single character string specifying what computational engine
to use for fitting. Currently only "kindling" is supported.}

\item{hidden_neurons}{An integer vector for the number of units in each hidden
layer. Can be tuned.}

\item{activations}{A character vector of activation function names for each
hidden layer (e.g., "relu", "tanh", "sigmoid"). Can be tuned.}

\item{output_activation}{A character string for the output activation function.
Can be tuned.}

\item{bias}{Logical for whether to include bias terms. Can be tuned.}

\item{epochs}{An integer for the number of training iterations. Can be tuned.}

\item{batch_size}{An integer for the batch size during training. Can be tuned.}

\item{penalty}{A number for the regularization penalty (lambda). Default \code{0}
(no regularization). Higher values increase regularization strength. Can be tuned.}

\item{mixture}{A number between 0 and 1 for the elastic net mixing parameter.
Default \code{0} (pure L2/Ridge regularization).
\itemize{
\item \code{0}: Pure L2 regularization (Ridge)
\item \code{1}: Pure L1 regularization (Lasso)
\item \verb{0 < mixture < 1}: Elastic net (combination of L1 and L2)
Only relevant when \code{penalty > 0}. Can be tuned.
}}

\item{learn_rate}{A number for the learning rate. Can be tuned.}

\item{optimizer}{A character string for the optimizer type ("adam", "sgd",
"rmsprop"). Can be tuned.}

\item{validation_split}{A number between 0 and 1 for the proportion of data
used for validation. Can be tuned.}

\item{optimizer_args}{A named list of additional arguments passed to the
optimizer. Cannot be tuned — pass via \code{set_engine()}.}

\item{loss}{A character string for the loss function ("mse", "mae",
"cross_entropy", "bce"). Cannot be tuned — pass via \code{set_engine()}.}

\item{architecture}{An \code{\link[=nn_arch]{nn_arch()}} object for a custom architecture. Cannot
be tuned — pass via \code{set_engine()}.}

\item{flatten_input}{Logical or \code{NULL}. Controls input flattening. Cannot be
tuned — pass via \code{set_engine()}.}

\item{early_stopping}{An \code{\link[=early_stop]{early_stop()}} object or \code{NULL}. Cannot be tuned —
pass via \code{set_engine()}.}

\item{device}{A character string for the device ("cpu", "cuda", "mps").
Cannot be tuned — pass via \code{set_engine()}.}

\item{verbose}{Logical for whether to print training progress. Cannot be
tuned — pass via \code{set_engine()}.}

\item{cache_weights}{Logical. If \code{TRUE}, stores trained weight matrices in
the returned object. Cannot be tuned — pass via \code{set_engine()}.}
}
\value{
A model specification object with class \code{mlp_kindling}.
}
\description{
\code{mlp_kindling()} defines a feedforward neural network model that can be used
for classification or regression. It integrates with the tidymodels ecosystem
and uses the torch backend via kindling.
}
\details{
This function creates a model specification for a feedforward neural network
that can be used within tidymodels workflows. The model supports:
\itemize{
\item Multiple hidden layers with configurable units
\item Various activation functions per layer
\item GPU acceleration (CUDA, MPS, or CPU)
\item Hyperparameter tuning integration
\item Both regression and classification tasks
}

Parameters that cannot be tuned (\code{architecture}, \code{flatten_input},
\code{early_stopping}, \code{device}, \code{verbose}, \code{cache_weights}, \code{optimizer_args},
\code{loss}) must be set via \code{set_engine()}, not as arguments to \code{mlp_kindling()}.
}
\examples{
\donttest{
if (torch::torch_is_installed()) {
    box::use(
        recipes[recipe],
        workflows[workflow, add_recipe, add_model],
        tune[tune],
        parsnip[fit]
    )

    # library(recipes)
    # library(workflows)
    # library(parsnip)
    # library(tune)

    # Model specs
    mlp_spec = mlp_kindling(
        mode = "classification",
        hidden_neurons = c(128, 64, 32),
        activation = c("relu", "relu", "relu"),
        epochs = 100
    )

    # If you want to tune
    mlp_tune_spec = mlp_kindling(
        mode = "classification",
        hidden_neurons = tune(),
        activation = tune(),
        epochs = tune(),
        learn_rate = tune()
    )
     wf = workflow() |>
        add_recipe(recipe(Species ~ ., data = iris)) |>
        add_model(mlp_spec)

     fit_wf = fit(wf, data = iris)
} else {
    message("Torch not fully installed — skipping example")
}
}

}
