% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/torch_nn_generator.R
\name{ffnn_generator}
\alias{ffnn_generator}
\title{Feed-Forward Neural Network Module Generator}
\usage{
ffnn_generator(
  nn_name = "DeepFFN",
  hd_neurons,
  no_x,
  no_y,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE
)
}
\arguments{
\item{nn_name}{Character. Name of the generated FFNN module class. Default is \code{"DeepFFN"}.}

\item{hd_neurons}{Integer vector. Number of neurons in each hidden layer.}

\item{no_x}{Integer. Number of input features.}

\item{no_y}{Integer. Number of output features.}

\item{activations}{Activation function specifications for each hidden layer.
Can be:
\itemize{
\item \code{NULL}: No activation functions.
\item Character vector: e.g., \code{c("relu", "tanh")}.
\item List: e.g., \code{act_funs(relu, tanh, softshrink = args(lambd = 0.5))}.
\item \code{activation_spec} object from \code{act_funs()}.
}}

\item{output_activation}{Optional. Activation function for the output layer.
Same format as \code{activations} but should be a single activation.}

\item{bias}{Logical. Whether to use bias weights. Default is \code{TRUE}.}
}
\value{
A \code{torch} module expression representing the FFNN.
}
\description{
This function generates a feed-forward neural network (FFNN) module expression
from the \code{torch} package in R. It allows customization of the FFNN architecture,
including the number of hidden layers, neurons, and activation functions.
}
\details{
The generated FFNN module will have the specified number of hidden layers,
with each layer containing the specified number of neurons. Activation functions
can be applied after each hidden layer as specified.
This can be used for both classification and regression tasks.
}
\examples{

# Generate a FFNN module with 3 hidden layers
ffnn_mod = ffnn_generator(
    nn_name = "MyFFNN",
    hd_neurons = c(64, 32, 16),
    no_x = 10,
    no_y = 1,
    activations = 'relu'
)

# More complex example with different activations
ffnn_mod2 = ffnn_generator(
    nn_name = "MyFFNN2",
    hd_neurons = c(128, 64, 32),
    no_x = 20,
    no_y = 5,
    activations = act_funs(
        relu,
        tanh,
        softshrink = args(lambd = 0.5)
    )
)

# With output activation for classification
ffnn_mod3 = ffnn_generator(
    hd_neurons = c(64, 32),
    no_x = 10,
    no_y = 3,
    activations = 'relu',
    output_activation = act_funs(softmax = args(dim = 1L))
)

}
