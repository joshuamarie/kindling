% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/torch_nn_generator.R
\name{ffnn_generator}
\alias{ffnn_generator}
\title{Feed-Forward Neural Network Module Generator}
\usage{
ffnn_generator(
  nn_name = "DeepFFN",
  hd_neurons,
  no_x,
  no_y,
  activations = NULL,
  output_activation = NULL,
  bias = TRUE
)
}
\arguments{
\item{nn_name}{Character. Name of the generated FFNN module class. Default is \code{"DeepFFN"}.}

\item{hd_neurons}{Integer vector. Number of neurons in each hidden layer.}

\item{no_x}{Integer. Number of input features.}

\item{no_y}{Integer. Number of output features.}

\item{activations}{Activation function specifications for each hidden layer.
Can be:
\itemize{
\item \code{NULL}: No activation functions.
\item Character vector: e.g., \code{c("relu", "sigmoid")}.
\item List: e.g., \code{act_funs(relu, elu, softshrink = args(lambd = 0.5))}.
\item \code{activation_spec} object from \code{act_funs()}.
}

If the length of \code{activations} is \code{1L}, this will be the activation throughout the architecture.}

\item{output_activation}{Optional. Activation function for the output layer.
Same format as \code{activations} but should be a single activation.}

\item{bias}{Logical. Whether to use bias weights. Default is \code{TRUE}.}
}
\value{
A \code{torch} module expression representing the FFNN.
}
\description{
This function generates a feed-forward neural network (FFNN) module expression
from the \code{torch} package in R. It allows customization of the FFNN architecture,
including the number of hidden layers, neurons, and activation functions.
}
\details{
The generated FFNN module will have the specified number of hidden layers,
with each layer containing the specified number of neurons. Activation functions
can be applied after each hidden layer as specified.
This can be used for both classification and regression tasks.

The generated module properly namespaces all torch functions to avoid
polluting the global namespace.
}
\examples{
# Generate an MLP module with 3 hidden layers
ffnn_mod = ffnn_generator(
    nn_name = "MyFFNN",
    hd_neurons = c(64, 32, 16),
    no_x = 10,
    no_y = 1,
    activations = 'relu'
)

# Evaluate and instantiate
model = eval(rnn_mod)()

# More complex: With different activations
ffnn_mod2 = ffnn_generator(
    nn_name = "MyFFNN2",
    hd_neurons = c(128, 64, 32),
    no_x = 20,
    no_y = 5,
    activations = act_funs(
        relu,
        selu,
        sigmoid
    )
)

# Even more complex: Different activations and customized argument
# for the specific activation function
ffnn_mod2 = ffnn_generator(
    nn_name = "MyFFNN2",
    hd_neurons = c(128, 64, 32),
    no_x = 20,
    no_y = 5,
    activations = act_funs(
        relu,
        selu,
        softshrink = args(lambd = 0.5)
    )
)

# Customize output activation (softmax is useful for classification tasks)
ffnn_mod3 = ffnn_generator(
    hd_neurons = c(64, 32),
    no_x = 10,
    no_y = 3,
    activations = 'relu',
    output_activation = act_funs(softmax = args(dim = 2L))
)

}
