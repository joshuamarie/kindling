% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rnn_kindling.R
\name{rnn_kindling}
\alias{rnn_kindling}
\title{Recurrent Neural Network via kindling}
\usage{
rnn_kindling(
  mode = "unknown",
  engine = "kindling",
  hidden_neurons = NULL,
  rnn_type = NULL,
  activations = NULL,
  output_activation = NULL,
  bias = NULL,
  bidirectional = NULL,
  dropout = NULL,
  epochs = NULL,
  batch_size = NULL,
  learn_rate = NULL,
  optimizer = NULL,
  loss = NULL,
  validation_split = NULL,
  device = NULL,
  verbose = NULL
)
}
\arguments{
\item{mode}{A single character string for the type of model. Possible values
are "unknown", "regression", or "classification".}

\item{engine}{A single character string specifying what computational engine
to use for fitting. Currently only "kindling" is supported.}

\item{hidden_neurons}{An integer vector for the number of units in each hidden
layer. Can be tuned.}

\item{rnn_type}{A character string for the type of RNN cell ("rnn", "lstm",
"gru"). Can be tuned.}

\item{activations}{A character vector of activation function names for each
hidden layer (e.g., "relu", "tanh", "sigmoid"). Can be tuned.}

\item{output_activation}{A character string for the output activation function.
Can be tuned.}

\item{bias}{Logical for whether to include bias terms. Can be tuned.}

\item{bidirectional}{A logical indicating whether to use bidirectional RNN.
Can be tuned.}

\item{dropout}{A number between 0 and 1 for dropout rate between layers.
Can be tuned.}

\item{epochs}{An integer for the number of training iterations. Can be tuned.}

\item{batch_size}{An integer for the batch size during training. Can be tuned.}

\item{learn_rate}{A number for the learning rate. Can be tuned.}

\item{optimizer}{A character string for the optimizer type ("adam", "sgd",
"rmsprop"). Can be tuned.}

\item{loss}{A character string for the loss function ("mse", "mae",
"cross_entropy", "bce"). Can be tuned.}

\item{validation_split}{A number between 0 and 1 for the proportion of data
used for validation. Can be tuned.}

\item{device}{A character string for the device to use ("cpu", "cuda", "mps").
If NULL, auto-detects available GPU. Can be tuned.}

\item{verbose}{Logical for whether to print training progress. Default FALSE.}
}
\value{
A model specification object with class \code{rnn_kindling}.
}
\description{
\code{rnn_kindling()} defines a recurrent neural network model that can be used
for classification or regression on sequential data. It integrates with the
tidymodels ecosystem and uses the torch backend via kindling.
}
\details{
This function creates a model specification for a recurrent neural network
that can be used within tidymodels workflows. The model supports:
\itemize{
\item Multiple RNN types: basic RNN, LSTM, and GRU
\item Bidirectional processing
\item Dropout regularization
\item GPU acceleration (CUDA, MPS, or CPU)
\item Hyperparameter tuning integration
\item Both regression and classification tasks
}

The \code{device} parameter controls where computation occurs:
\itemize{
\item \code{NULL} (default): Auto-detect best available device (CUDA > MPS > CPU)
\item \code{"cuda"}: Use NVIDIA GPU
\item \code{"mps"}: Use Apple Silicon GPU
\item \code{"cpu"}: Use CPU only
}
}
\examples{
\dontrun{
box::use(tune[tune])

# Model Specs for LSTM
rnn_spec = rnn_kindling(
    mode = "classification",
    hidden_neurons = c(64, 32),
    rnn_type = "lstm",
    activation = c("relu", "elu"),
    epochs = 100,
    bidirectional = TRUE
)
}

}
